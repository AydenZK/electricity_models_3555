,run_dir,metric_val_loss,metric_loss,loss_diff,metric_mape,metric_val_mape,metric_log_cosh,metric_val_log_cosh,flag_dense_units1,flag_dense_units2,flag_batch_size,flag_learning_rate,flag_dense_units3,flag_dense_units4,flag_dropout1,flag_dropout2,flag_dropout3,flag_dropout4,flag_decay_steps,epochs,epochs_completed,metrics,model,loss_function,optimizer,learning_rate,script,start,end,completed,output,error_message,error_traceback,source_code,context,type
102,runs/2022-10-13T11-07-16Z,2038.6361,2038.5977,0.0384,7.6383,7.2315,NA,NA,64,64,128,1.00E-04,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-07-16Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B84107BA60>,1.00E-04,nn_exp.R,13/10/2022 11:07,13/10/2022 11:07,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-07-16Z/tfruns.d/source.tar.gz,local,training
19,runs/2022-10-13T14-46-46Z,823.0812,823.0228,0.0584,2.9636,2.9736,NA,NA,128,128,32,0.005,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T14-46-46Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC32D8E0>,0.005,nn_exp.R,13/10/2022 14:46,13/10/2022 14:48,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T14-46-46Z/tfruns.d/source.tar.gz,local,training
379,runs/2022-10-13T04-09-22Z,1040.0607,1039.9666,0.0941,3.7608,3.8133,NA,NA,64,128,16,0.1,64,32,NA,NA,NA,NA,NA,35,24,runs/2022-10-13T04-09-22Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814978EB0>,0.100000001,nn_exp.R,13/10/2022 4:09,13/10/2022 4:11,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-09-22Z/tfruns.d/source.tar.gz,local,training
388,runs/2022-10-13T03-49-59Z,1027.8561,1028.3923,0.5362,3.713,3.4588,NA,NA,64,128,16,0.1,32,32,NA,NA,NA,NA,NA,35,33,runs/2022-10-13T03-49-59Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FA45FD0>,0.100000001,nn_exp.R,13/10/2022 3:49,13/10/2022 3:51,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T03-49-59Z/tfruns.d/source.tar.gz,local,training
244,runs/2022-10-13T08-01-01Z,996.2013,996.7958,0.5945,3.6046,3.2807,NA,NA,64,128,32,0.1,64,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-01-01Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E0E7D60>,0.100000001,nn_exp.R,13/10/2022 8:01,13/10/2022 8:02,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-01-01Z/tfruns.d/source.tar.gz,local,training
79,runs/2022-10-13T11-32-24Z,866.9179,869.5721,2.6542,3.1343,2.9284,NA,NA,128,128,64,0.001,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-32-24Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC09A1F0>,0.001,nn_exp.R,13/10/2022 11:32,13/10/2022 11:33,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-32-24Z/tfruns.d/source.tar.gz,local,training
75,runs/2022-10-13T11-38-07Z,1885.4133,1882.2229,3.1904,7.0346,6.6108,NA,NA,128,128,128,1.00E-04,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-38-07Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC0A4580>,1.00E-04,nn_exp.R,13/10/2022 11:38,13/10/2022 11:39,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-38-07Z/tfruns.d/source.tar.gz,local,training
92,runs/2022-10-13T11-16-37Z,826.6285,833.851,7.2225,3.004,2.9921,NA,NA,64,128,16,0.01,32,32,NA,NA,NA,NA,NA,35,28,runs/2022-10-13T11-16-37Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829A59640>,0.01,nn_exp.R,13/10/2022 11:16,13/10/2022 11:18,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-16-37Z/tfruns.d/source.tar.gz,local,training
86,runs/2022-10-13T11-22-55Z,1215.6887,1223.155,7.4663,4.4684,4.1805,NA,NA,64,128,16,1.00E-04,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-22-55Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FAE90A0>,1.00E-04,nn_exp.R,13/10/2022 11:22,13/10/2022 11:24,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-22-55Z/tfruns.d/source.tar.gz,local,training
225,runs/2022-10-13T08-28-56Z,934.5729,926.5427,8.0302,3.3405,3.0858,NA,NA,128,128,64,0.1,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-28-56Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8134AFEE0>,0.100000001,nn_exp.R,13/10/2022 8:28,13/10/2022 8:30,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-28-56Z/tfruns.d/source.tar.gz,local,training
267,runs/2022-10-13T07-25-11Z,934.5162,925.3537,9.1625,3.3534,3.3969,NA,NA,128,32,32,0.1,128,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-25-11Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81484C5B0>,0.100000001,nn_exp.R,13/10/2022 7:25,13/10/2022 7:26,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-25-11Z/tfruns.d/source.tar.gz,local,training
13,runs/2022-10-13T15-40-29Z,853.5437,842.9056,10.6381,3.0415,2.8629,NA,NA,128,32,32,0.005,NA,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T15-40-29Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B83BF2ECD0>,0.005,nn_exp.R,13/10/2022 15:40,13/10/2022 15:41,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   #flag_integer(""d ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T15-40-29Z/tfruns.d/source.tar.gz,local,training
387,runs/2022-10-13T03-51-51Z,1005.0295,994.0494,10.9801,3.5929,3.6356,NA,NA,128,128,16,0.1,32,32,NA,NA,NA,NA,NA,35,34,runs/2022-10-13T03-51-51Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B03AB7C0>,0.100000001,nn_exp.R,13/10/2022 3:51,13/10/2022 3:53,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T03-51-51Z/tfruns.d/source.tar.gz,local,training
71,runs/2022-10-13T12-06-45Z,793.2476,780.9239,12.3237,2.8168,2.6907,NA,NA,64,64,8,0.001,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T12-06-45Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCBD3880>,0.001,nn_exp.R,13/10/2022 12:06,13/10/2022 12:09,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-06-45Z/tfruns.d/source.tar.gz,local,training
281,runs/2022-10-13T07-10-11Z,984.0909,997.1893,13.0984,3.6164,3.3375,NA,NA,32,128,32,0.1,32,64,NA,NA,NA,NA,NA,35,29,runs/2022-10-13T07-10-11Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B03552E0>,0.100000001,nn_exp.R,13/10/2022 7:10,13/10/2022 7:10,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-10-11Z/tfruns.d/source.tar.gz,local,training
117,runs/2022-10-13T10-45-46Z,1026.6754,1040.2941,13.6187,3.7961,3.5277,NA,NA,128,64,32,1.00E-04,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-45-46Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC10EEE0>,1.00E-04,nn_exp.R,13/10/2022 10:45,13/10/2022 10:46,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-45-46Z/tfruns.d/source.tar.gz,local,training
115,runs/2022-10-13T10-47-55Z,815.3818,829.6217,14.2399,2.9946,2.7556,NA,NA,64,128,32,0.001,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-47-55Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8147AA4C0>,0.001,nn_exp.R,13/10/2022 10:47,13/10/2022 10:49,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-47-55Z/tfruns.d/source.tar.gz,local,training
42,runs/2022-10-13T13-26-08Z,811.6265,797.2229,14.4036,2.8705,2.7373,NA,NA,128,64,32,0.01,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T13-26-08Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BE05580>,0.01,nn_exp.R,13/10/2022 13:26,13/10/2022 13:27,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-26-08Z/tfruns.d/source.tar.gz,local,training
94,runs/2022-10-13T11-15-15Z,1175.1935,1189.9252,14.7317,4.3512,4.038,NA,NA,128,64,64,1.00E-04,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-15-15Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BCB7730>,1.00E-04,nn_exp.R,13/10/2022 11:15,13/10/2022 11:15,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-15-15Z/tfruns.d/source.tar.gz,local,training
363,runs/2022-10-13T04-47-24Z,1005.4011,989.9404,15.4607,3.5707,3.6516,NA,NA,128,64,16,0.1,32,64,NA,NA,NA,NA,NA,35,34,runs/2022-10-13T04-47-24Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8203892E0>,0.100000001,nn_exp.R,13/10/2022 4:47,13/10/2022 4:49,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-47-24Z/tfruns.d/source.tar.gz,local,training
103,runs/2022-10-13T11-06-40Z,1426.2615,1410.3085,15.953,5.1906,4.8863,NA,NA,64,64,64,1.00E-04,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-06-40Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E0ECF10>,1.00E-04,nn_exp.R,13/10/2022 11:06,13/10/2022 11:07,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-06-40Z/tfruns.d/source.tar.gz,local,training
84,runs/2022-10-13T11-25-11Z,1915.7271,1932.2267,16.4996,7.2188,6.8517,NA,NA,64,128,128,1.00E-04,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-25-11Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829AD9CA0>,1.00E-04,nn_exp.R,13/10/2022 11:25,13/10/2022 11:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-25-11Z/tfruns.d/source.tar.gz,local,training
353,runs/2022-10-13T05-07-14Z,1025.459,1007.7409,17.7181,3.6412,3.4327,NA,NA,32,128,16,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T05-07-14Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E2D7400>,0.100000001,nn_exp.R,13/10/2022 5:07,13/10/2022 5:08,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-07-14Z/tfruns.d/source.tar.gz,local,training
201,runs/2022-10-13T08-52-09Z,894.9492,877.1719,17.7773,3.1551,2.9819,NA,NA,128,64,64,0.1,32,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-52-09Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCA903A0>,0.100000001,nn_exp.R,13/10/2022 8:52,13/10/2022 8:52,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-52-09Z/tfruns.d/source.tar.gz,local,training
93,runs/2022-10-13T11-15-57Z,1958.0205,1976.7799,18.7594,7.413,6.9702,NA,NA,128,64,128,1.00E-04,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-15-57Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B84107D970>,1.00E-04,nn_exp.R,13/10/2022 11:15,13/10/2022 11:16,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-15-57Z/tfruns.d/source.tar.gz,local,training
290,runs/2022-10-13T07-01-53Z,4720.7988,4701.2544,19.5444,17.4242,17.1106,NA,NA,32,128,32,0.1,128,32,NA,NA,NA,NA,NA,35,19,runs/2022-10-13T07-01-53Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B84107BD60>,0.100000001,nn_exp.R,13/10/2022 7:01,13/10/2022 7:02,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-01-53Z/tfruns.d/source.tar.gz,local,training
126,runs/2022-10-13T10-29-13Z,739.2665,719.2766,19.9899,2.5889,2.5008,NA,NA,128,128,32,0.01,32,16,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-29-13Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E1F2AC0>,0.01,nn_exp.R,13/10/2022 10:29,13/10/2022 10:30,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-29-13Z/tfruns.d/source.tar.gz,local,training
293,runs/2022-10-13T06-59-12Z,974.9586,954.5758,20.3828,3.4561,3.5513,NA,NA,32,64,32,0.1,128,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-59-12Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E233700>,0.100000001,nn_exp.R,13/10/2022 6:59,13/10/2022 7:00,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-59-12Z/tfruns.d/source.tar.gz,local,training
107,runs/2022-10-13T11-02-16Z,766.7484,788.7385,21.9901,2.8365,2.7897,NA,NA,64,64,16,0.001,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-02-16Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829B21B50>,0.001,nn_exp.R,13/10/2022 11:02,13/10/2022 11:04,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-02-16Z/tfruns.d/source.tar.gz,local,training
284,runs/2022-10-13T07-07-40Z,1020.1136,996.9179,23.1957,3.6075,3.7019,NA,NA,32,64,32,0.1,32,64,NA,NA,NA,NA,NA,35,28,runs/2022-10-13T07-07-40Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC14D130>,0.100000001,nn_exp.R,13/10/2022 7:07,13/10/2022 7:08,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-07-40Z/tfruns.d/source.tar.gz,local,training
45,runs/2022-10-13T13-20-47Z,940.4359,916.7299,23.706,3.3219,3.1681,NA,NA,64,64,32,0.001,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T13-20-47Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A8727F0>,0.001,nn_exp.R,13/10/2022 13:20,13/10/2022 13:21,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-20-47Z/tfruns.d/source.tar.gz,local,training
192,runs/2022-10-13T08-57-58Z,1012.4263,987.3407,25.0856,3.582,3.376,NA,NA,128,64,64,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-57-58Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B84118DF10>,0.100000001,nn_exp.R,13/10/2022 8:57,13/10/2022 8:58,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-57-58Z/tfruns.d/source.tar.gz,local,training
69,runs/2022-10-13T12-11-36Z,941.4835,916.2867,25.1968,3.3204,3.4487,NA,NA,64,64,32,0.001,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T12-11-36Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81248B1F0>,0.001,nn_exp.R,13/10/2022 12:11,13/10/2022 12:12,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-11-36Z/tfruns.d/source.tar.gz,local,training
256,runs/2022-10-13T07-44-48Z,967.3201,993.2993,25.9792,3.5964,3.286,NA,NA,64,64,32,0.1,32,128,NA,NA,NA,NA,NA,35,28,runs/2022-10-13T07-44-48Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B825493CA0>,0.100000001,nn_exp.R,13/10/2022 7:44,13/10/2022 7:46,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-44-48Z/tfruns.d/source.tar.gz,local,training
96,runs/2022-10-13T11-12-43Z,973.4688,1000.6214,27.1526,3.6352,3.3188,NA,NA,128,64,128,0.001,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-12-43Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B05661C0>,0.001,nn_exp.R,13/10/2022 11:12,13/10/2022 11:13,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-12-43Z/tfruns.d/source.tar.gz,local,training
49,runs/2022-10-13T13-14-12Z,732.9169,760.8085,27.8916,2.7253,2.4216,NA,NA,64,64,16,0.01,64,NA,NA,NA,NA,NA,NA,35,28,runs/2022-10-13T13-14-12Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814883820>,0.01,nn_exp.R,13/10/2022 13:14,13/10/2022 13:15,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-14-12Z/tfruns.d/source.tar.gz,local,training
104,runs/2022-10-13T11-05-00Z,1205.1072,1233.1964,28.0892,4.5398,4.2152,NA,NA,64,64,16,1.00E-04,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-05-00Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BE0BC10>,1.00E-04,nn_exp.R,13/10/2022 11:05,13/10/2022 11:06,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-05-00Z/tfruns.d/source.tar.gz,local,training
391,runs/2022-10-13T03-44-18Z,981.7161,1011.0101,29.294,3.6489,3.5748,NA,NA,64,64,16,0.1,32,32,NA,NA,NA,NA,NA,35,29,runs/2022-10-13T03-44-18Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B053BDC0>,0.100000001,nn_exp.R,13/10/2022 3:44,13/10/2022 3:45,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T03-44-18Z/tfruns.d/source.tar.gz,local,training
101,runs/2022-10-13T11-07-41Z,804.3284,773.9287,30.3997,2.785,2.6938,NA,NA,128,64,16,0.01,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-07-41Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0233340>,0.01,nn_exp.R,13/10/2022 11:07,13/10/2022 11:09,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-07-41Z/tfruns.d/source.tar.gz,local,training
394,runs/2022-10-13T03-39-54Z,974.2383,1004.9836,30.7453,3.6312,3.2429,NA,NA,64,32,16,0.1,32,32,NA,NA,NA,NA,NA,35,29,runs/2022-10-13T03-39-54Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8121C58B0>,0.100000001,nn_exp.R,13/10/2022 3:39,13/10/2022 3:41,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T03-39-54Z/tfruns.d/source.tar.gz,local,training
26,runs/2022-10-13T14-38-20Z,683.0994,713.8539,30.7545,2.5655,2.3754,NA,NA,64,64,32,0.01,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T14-38-20Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC0C4D00>,0.01,nn_exp.R,13/10/2022 14:38,13/10/2022 14:39,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T14-38-20Z/tfruns.d/source.tar.gz,local,training
193,runs/2022-10-13T08-57-24Z,976.3008,945.382,30.9188,3.4071,3.5392,NA,NA,64,64,64,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-57-24Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81B91FE20>,0.100000001,nn_exp.R,13/10/2022 8:57,13/10/2022 8:57,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-57-24Z/tfruns.d/source.tar.gz,local,training
141,runs/2022-10-13T10-10-13Z,813.2297,847.1533,33.9236,3.0604,2.7591,NA,NA,64,64,32,0.001,16,16,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-10-13Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0355C70>,0.001,nn_exp.R,13/10/2022 10:10,13/10/2022 10:11,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-10-13Z/tfruns.d/source.tar.gz,local,training
240,runs/2022-10-13T08-07-42Z,1013.9594,1048.4156,34.4562,3.7988,3.417,NA,NA,128,32,32,0.1,128,128,NA,NA,NA,NA,NA,35,26,runs/2022-10-13T08-07-42Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8254DB1C0>,0.100000001,nn_exp.R,13/10/2022 8:07,13/10/2022 8:09,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-07-42Z/tfruns.d/source.tar.gz,local,training
114,runs/2022-10-13T10-49-06Z,1226.1162,1260.9957,34.8795,4.6063,4.2426,NA,NA,64,128,32,1.00E-04,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-49-06Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B82544D0D0>,1.00E-04,nn_exp.R,13/10/2022 10:49,13/10/2022 10:50,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-49-06Z/tfruns.d/source.tar.gz,local,training
63,runs/2022-10-13T12-25-17Z,942.1797,907.0999,35.0798,3.2898,3.1864,NA,NA,128,64,32,0.001,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T12-25-17Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCB7E400>,0.001,nn_exp.R,13/10/2022 12:25,13/10/2022 12:26,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-25-17Z/tfruns.d/source.tar.gz,local,training
27,runs/2022-10-13T14-34-19Z,907.9915,944.8542,36.8627,3.4126,3.0612,NA,NA,128,64,32,0.005,64,NA,NA,NA,NA,NA,NA,35,23,runs/2022-10-13T14-34-19Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E0FF0A0>,0.005,nn_exp.R,13/10/2022 14:34,13/10/2022 14:35,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,runs/2022-10-13T14-34-19Z/tfruns.d/source.tar.gz,local,training
109,runs/2022-10-13T11-01-32Z,1008.9725,1046.6525,37.68,3.7814,3.656,NA,NA,64,64,64,0.01,32,32,NA,NA,NA,NA,NA,35,17,runs/2022-10-13T11-01-32Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B825660310>,0.01,nn_exp.R,13/10/2022 11:01,13/10/2022 11:01,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-01-32Z/tfruns.d/source.tar.gz,local,training
58,runs/2022-10-13T12-36-19Z,760.2899,798.7978,38.5079,2.8717,2.5914,NA,NA,64,128,16,0.001,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T12-36-19Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BE2FC70>,0.001,nn_exp.R,13/10/2022 12:36,13/10/2022 12:38,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-36-19Z/tfruns.d/source.tar.gz,local,training
157,runs/2022-10-13T09-25-53Z,4728.5649,4689.7778,38.7871,17.3916,17.4136,NA,NA,64,64,64,0.1,128,128,NA,NA,NA,NA,NA,35,22,runs/2022-10-13T09-25-53Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FAC5F40>,0.100000001,nn_exp.R,13/10/2022 9:25,13/10/2022 9:26,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-25-53Z/tfruns.d/source.tar.gz,local,training
28,runs/2022-10-13T14-33-11Z,730.3518,770.4768,40.125,2.7689,2.6242,NA,NA,128,64,32,0.01,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T14-33-11Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCAEB370>,0.01,nn_exp.R,13/10/2022 14:33,13/10/2022 14:34,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T14-33-11Z/tfruns.d/source.tar.gz,local,training
65,runs/2022-10-13T12-18-50Z,836.7427,795.7618,40.9809,2.8646,3.021,NA,NA,128,64,8,0.001,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T12-18-50Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0AD7A90>,0.001,nn_exp.R,13/10/2022 12:18,13/10/2022 12:23,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-18-50Z/tfruns.d/source.tar.gz,local,training
181,runs/2022-10-13T09-06-25Z,1071.6261,1028.483,43.1431,3.7001,3.572,NA,NA,64,128,64,0.1,128,64,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T09-06-25Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A81F910>,0.100000001,nn_exp.R,13/10/2022 9:06,13/10/2022 9:07,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-06-25Z/tfruns.d/source.tar.gz,local,training
47,runs/2022-10-13T13-16-21Z,736.1179,781.28,45.1621,2.8132,2.48,NA,NA,64,64,8,0.001,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T13-16-21Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8255AA5B0>,0.001,nn_exp.R,13/10/2022 13:16,13/10/2022 13:19,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-16-21Z/tfruns.d/source.tar.gz,local,training
77,runs/2022-10-13T11-34-24Z,1002.7194,1050.0387,47.3193,3.8375,3.458,NA,NA,128,128,16,1.00E-04,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-34-24Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FAE51F0>,1.00E-04,nn_exp.R,13/10/2022 11:34,13/10/2022 11:36,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-34-24Z/tfruns.d/source.tar.gz,local,training
150,runs/2022-10-13T09-35-03Z,752.0068,799.3423,47.3355,2.8748,2.5727,NA,NA,128,32,16,0.01,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-35-03Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8298FA6A0>,0.01,nn_exp.R,13/10/2022 9:35,13/10/2022 9:36,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-35-03Z/tfruns.d/source.tar.gz,local,training
127,runs/2022-10-13T10-28-17Z,928.3111,880.879,47.4321,3.1768,3.3356,NA,NA,128,64,32,0.001,16,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-28-17Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81483DF40>,0.001,nn_exp.R,13/10/2022 10:28,13/10/2022 10:29,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-28-17Z/tfruns.d/source.tar.gz,local,training
133,runs/2022-10-13T10-22-18Z,1221.972,1270.0258,48.0538,4.6414,4.2459,NA,NA,128,64,32,1.00E-04,16,16,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-22-18Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A8F7AF0>,1.00E-04,nn_exp.R,13/10/2022 10:22,13/10/2022 10:23,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-22-18Z/tfruns.d/source.tar.gz,local,training
85,runs/2022-10-13T11-24-32Z,1135.8209,1184.9032,49.0823,4.3435,3.9615,NA,NA,64,128,64,1.00E-04,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-24-32Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8254A55E0>,1.00E-04,nn_exp.R,13/10/2022 11:24,13/10/2022 11:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-24-32Z/tfruns.d/source.tar.gz,local,training
76,runs/2022-10-13T11-36-57Z,1194.037,1243.2622,49.2252,4.5553,4.1468,NA,NA,128,128,64,1.00E-04,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-36-57Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC06F640>,1.00E-04,nn_exp.R,13/10/2022 11:36,13/10/2022 11:38,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-36-57Z/tfruns.d/source.tar.gz,local,training
285,runs/2022-10-13T07-06-46Z,887.6373,938.6382,51.0009,3.378,2.948,NA,NA,128,32,32,0.1,32,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-06-46Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC2B9A00>,0.100000001,nn_exp.R,13/10/2022 7:06,13/10/2022 7:07,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-06-46Z/tfruns.d/source.tar.gz,local,training
97,runs/2022-10-13T11-12-03Z,816.3896,869.0675,52.6779,3.144,2.7887,NA,NA,128,64,64,0.001,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-12-03Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8255AA5E0>,0.001,nn_exp.R,13/10/2022 11:12,13/10/2022 11:12,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-12-03Z/tfruns.d/source.tar.gz,local,training
112,runs/2022-10-13T10-51-44Z,890.5915,837.2363,53.3552,3.0145,2.9865,NA,NA,128,128,32,0.001,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-51-44Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81484F0D0>,0.001,nn_exp.R,13/10/2022 10:51,13/10/2022 10:53,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-51-44Z/tfruns.d/source.tar.gz,local,training
137,runs/2022-10-13T10-16-29Z,1160.9761,1214.5958,53.6197,4.4372,4.063,NA,NA,128,128,32,1.00E-04,16,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-16-29Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BD864C0>,1.00E-04,nn_exp.R,13/10/2022 10:16,13/10/2022 10:19,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-16-29Z/tfruns.d/source.tar.gz,local,training
38,runs/2022-10-13T13-32-41Z,777.204,831.6577,54.4537,2.9941,2.5978,NA,NA,64,128,8,0.01,64,NA,NA,NA,NA,NA,NA,35,28,runs/2022-10-13T13-32-41Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCB6CB20>,0.01,nn_exp.R,13/10/2022 13:32,13/10/2022 13:35,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-32-41Z/tfruns.d/source.tar.gz,local,training
140,runs/2022-10-13T10-11-18Z,1333.1532,1388.191,55.0378,5.0776,4.6579,NA,NA,64,64,32,1.00E-04,16,16,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-11-18Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829A13B50>,1.00E-04,nn_exp.R,13/10/2022 10:11,13/10/2022 10:12,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-11-18Z/tfruns.d/source.tar.gz,local,training
51,runs/2022-10-13T13-09-00Z,788.2774,843.3306,55.0532,3.0352,2.8304,NA,NA,128,128,32,0.001,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T13-09-00Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BCE07F0>,0.001,nn_exp.R,13/10/2022 13:09,13/10/2022 13:11,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-09-00Z/tfruns.d/source.tar.gz,local,training
357,runs/2022-10-13T04-59-14Z,1102.1112,1047.0219,55.0893,3.777,3.9814,NA,NA,128,32,16,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T04-59-14Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BBFCEFD0>,0.100000001,nn_exp.R,13/10/2022 4:59,13/10/2022 5:01,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-59-14Z/tfruns.d/source.tar.gz,local,training
333,runs/2022-10-13T06-07-06Z,1019.1467,962.4302,56.7165,3.4775,3.4249,NA,NA,128,128,16,0.1,32,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-07-06Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8146B5AF0>,0.100000001,nn_exp.R,13/10/2022 6:07,13/10/2022 6:09,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-07-06Z/tfruns.d/source.tar.gz,local,training
120,runs/2022-10-13T10-42-30Z,1240.5459,1297.8674,57.3215,4.7432,4.3514,NA,NA,64,64,32,1.00E-04,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-42-30Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC274AC0>,1.00E-04,nn_exp.R,13/10/2022 10:42,13/10/2022 10:43,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-42-30Z/tfruns.d/source.tar.gz,local,training
43,runs/2022-10-13T13-24-31Z,675.803,733.3235,57.5205,2.6329,2.3551,NA,NA,128,64,16,0.01,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T13-24-31Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E2ADC70>,0.01,nn_exp.R,13/10/2022 13:24,13/10/2022 13:26,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-24-31Z/tfruns.d/source.tar.gz,local,training
382,runs/2022-10-13T04-02-16Z,936.2036,994.5126,58.309,3.5901,3.4002,NA,NA,64,64,16,0.1,64,32,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T04-02-16Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC0CBC70>,0.100000001,nn_exp.R,13/10/2022 4:02,13/10/2022 4:04,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-02-16Z/tfruns.d/source.tar.gz,local,training
351,runs/2022-10-13T05-09-53Z,912.5528,971.1873,58.6345,3.5079,3.2975,NA,NA,128,128,16,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T05-09-53Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FB06700>,0.100000001,nn_exp.R,13/10/2022 5:09,13/10/2022 5:11,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-09-53Z/tfruns.d/source.tar.gz,local,training
349,runs/2022-10-13T05-12-47Z,1058.9517,999.9555,58.9962,3.6106,3.5301,NA,NA,64,32,16,0.1,128,64,NA,NA,NA,NA,NA,35,33,runs/2022-10-13T05-12-47Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B03DE430>,0.100000001,nn_exp.R,13/10/2022 5:12,13/10/2022 5:14,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-12-47Z/tfruns.d/source.tar.gz,local,training
131,runs/2022-10-13T10-24-33Z,881.0616,821.8014,59.2602,2.9583,3.1763,NA,NA,128,64,32,0.01,16,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-24-33Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81488D130>,0.01,nn_exp.R,13/10/2022 10:24,13/10/2022 10:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-24-33Z/tfruns.d/source.tar.gz,local,training
10,runs/2022-10-13T15-43-22Z,751.9464,811.3526,59.4062,2.9234,2.6711,NA,NA,64,64,32,0.01,NA,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T15-43-22Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B775E499A0>,0.01,nn_exp.R,13/10/2022 15:43,13/10/2022 15:44,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   #flag_integer(""d ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T15-43-22Z/tfruns.d/source.tar.gz,local,training
111,runs/2022-10-13T10-53-29Z,1091.8094,1152.2526,60.4432,4.2135,3.8312,NA,NA,128,128,32,1.00E-04,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-53-29Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FBE9DF0>,1.00E-04,nn_exp.R,13/10/2022 10:53,13/10/2022 10:55,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-53-29Z/tfruns.d/source.tar.gz,local,training
119,runs/2022-10-13T10-43-30Z,685.9342,747.293,61.3588,2.6805,2.4044,NA,NA,128,64,32,0.01,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-43-30Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E083F70>,0.01,nn_exp.R,13/10/2022 10:43,13/10/2022 10:44,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-43-30Z/tfruns.d/source.tar.gz,local,training
216,runs/2022-10-13T08-37-25Z,1077.0625,1015.696,61.3665,3.6516,3.9684,NA,NA,128,128,64,0.1,64,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-37-25Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BBFE9E50>,0.100000001,nn_exp.R,13/10/2022 8:37,13/10/2022 8:38,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-37-25Z/tfruns.d/source.tar.gz,local,training
18,runs/2022-10-13T15-35-44Z,741.4724,804.4031,62.9307,2.9045,2.6618,NA,NA,32,32,32,0.01,NA,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T15-35-44Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8201ABBE0>,0.01,nn_exp.R,13/10/2022 15:35,13/10/2022 15:36,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   #flag_integer(""d ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T15-35-44Z/tfruns.d/source.tar.gz,local,training
390,runs/2022-10-13T03-45-48Z,1056.9753,993.2366,63.7387,3.5939,3.8014,NA,NA,128,64,16,0.1,32,32,NA,NA,NA,NA,NA,35,34,runs/2022-10-13T03-45-48Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0ACA520>,0.100000001,nn_exp.R,13/10/2022 3:45,13/10/2022 3:47,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T03-45-48Z/tfruns.d/source.tar.gz,local,training
14,runs/2022-10-13T15-39-32Z,771.3915,837.3229,65.9314,3.0143,2.6285,NA,NA,128,32,32,0.01,NA,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T15-39-32Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B775E73040>,0.01,nn_exp.R,13/10/2022 15:39,13/10/2022 15:40,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   #flag_integer(""d ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T15-39-32Z/tfruns.d/source.tar.gz,local,training
180,runs/2022-10-13T09-07-29Z,956.6288,1025.5282,68.8994,3.708,3.4869,NA,NA,128,128,64,0.1,128,64,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T09-07-29Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B825524340>,0.100000001,nn_exp.R,13/10/2022 9:07,13/10/2022 9:08,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-07-29Z/tfruns.d/source.tar.gz,local,training
20,runs/2022-10-13T14-45-16Z,690.5095,760.1475,69.638,2.7322,2.2978,NA,NA,128,128,32,0.01,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T14-45-16Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E25E940>,0.01,nn_exp.R,13/10/2022 14:45,13/10/2022 14:46,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T14-45-16Z/tfruns.d/source.tar.gz,local,training
67,runs/2022-10-13T12-15-30Z,838.9448,768.7811,70.1637,2.7606,2.783,NA,NA,128,64,16,0.01,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T12-15-30Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0578340>,0.01,nn_exp.R,13/10/2022 12:15,13/10/2022 12:17,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-15-30Z/tfruns.d/source.tar.gz,local,training
324,runs/2022-10-13T06-21-31Z,898.3422,969.0165,70.6743,3.4992,2.9841,NA,NA,128,128,16,0.1,64,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-21-31Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BE12340>,0.100000001,nn_exp.R,13/10/2022 6:21,13/10/2022 6:23,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-21-31Z/tfruns.d/source.tar.gz,local,training
262,runs/2022-10-13T07-33-58Z,1018.798,942.5775,76.2205,3.3947,3.3185,NA,NA,64,128,32,0.1,128,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-33-58Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E2A0250>,0.100000001,nn_exp.R,13/10/2022 7:33,13/10/2022 7:36,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-33-58Z/tfruns.d/source.tar.gz,local,training
386,runs/2022-10-13T03-53-47Z,1068.9457,989.8843,79.0614,3.5716,3.6261,NA,NA,32,32,16,0.1,64,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T03-53-47Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8123D3580>,0.100000001,nn_exp.R,13/10/2022 3:53,13/10/2022 3:55,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T03-53-47Z/tfruns.d/source.tar.gz,local,training
95,runs/2022-10-13T11-13-24Z,948.5336,1027.8625,79.3289,3.7557,3.3403,NA,NA,128,64,16,1.00E-04,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-13-24Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A9117C0>,1.00E-04,nn_exp.R,13/10/2022 11:13,13/10/2022 11:15,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-13-24Z/tfruns.d/source.tar.gz,local,training
15,runs/2022-10-13T15-38-36Z,782.9069,862.5372,79.6303,3.1273,2.8143,NA,NA,64,32,32,0.005,NA,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T15-38-36Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8299DBB50>,0.005,nn_exp.R,13/10/2022 15:38,13/10/2022 15:39,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   #flag_integer(""d ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T15-38-36Z/tfruns.d/source.tar.gz,local,training
39,runs/2022-10-13T13-31-45Z,750.5103,831.4087,80.8984,2.9933,2.5637,NA,NA,128,64,32,0.001,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T13-31-45Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCA6EAF0>,0.001,nn_exp.R,13/10/2022 13:31,13/10/2022 13:32,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-31-45Z/tfruns.d/source.tar.gz,local,training
113,runs/2022-10-13T10-50-15Z,628.9081,710.1049,81.1968,2.554,2.2562,NA,NA,128,128,32,0.01,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-50-15Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC146310>,0.01,nn_exp.R,13/10/2022 10:50,13/10/2022 10:51,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-50-15Z/tfruns.d/source.tar.gz,local,training
309,runs/2022-10-13T06-45-12Z,905.461,988.5712,83.1102,3.5773,3.2974,NA,NA,128,64,32,0.1,32,32,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T06-45-12Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FBB2E20>,0.100000001,nn_exp.R,13/10/2022 6:45,13/10/2022 6:46,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-45-12Z/tfruns.d/source.tar.gz,local,training
275,runs/2022-10-13T07-15-30Z,1076.2748,993.1613,83.1135,3.5967,3.6018,NA,NA,32,64,32,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-15-30Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FB9DF70>,0.100000001,nn_exp.R,13/10/2022 7:15,13/10/2022 7:16,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-15-30Z/tfruns.d/source.tar.gz,local,training
307,runs/2022-10-13T06-47-05Z,984.1505,900.8336,83.3169,3.2443,3.5804,NA,NA,64,128,32,0.1,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-47-05Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B820387370>,0.100000001,nn_exp.R,13/10/2022 6:47,13/10/2022 6:48,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-47-05Z/tfruns.d/source.tar.gz,local,training
118,runs/2022-10-13T10-44-39Z,764.4935,849.1895,84.696,3.063,2.6209,NA,NA,128,64,32,0.001,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-44-39Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A878850>,0.001,nn_exp.R,13/10/2022 10:44,13/10/2022 10:45,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-44-39Z/tfruns.d/source.tar.gz,local,training
17,runs/2022-10-13T15-36-43Z,843.9943,930.014,86.0197,3.3779,2.9219,NA,NA,32,32,32,0.005,NA,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T15-36-43Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8202E11F0>,0.005,nn_exp.R,13/10/2022 15:36,13/10/2022 15:37,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   #flag_integer(""d ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T15-36-43Z/tfruns.d/source.tar.gz,local,training
22,runs/2022-10-13T14-42-45Z,824.2456,738.0824,86.1632,2.6518,2.7339,NA,NA,64,128,32,0.01,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T14-42-45Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81B92F4F0>,0.01,nn_exp.R,13/10/2022 14:42,13/10/2022 14:44,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T14-42-45Z/tfruns.d/source.tar.gz,local,training
287,runs/2022-10-13T07-05-03Z,912.4183,1000.4395,88.0212,3.618,3.0552,NA,NA,32,32,32,0.1,32,64,NA,NA,NA,NA,NA,35,34,runs/2022-10-13T07-05-03Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC2C3E50>,0.100000001,nn_exp.R,13/10/2022 7:05,13/10/2022 7:05,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-05-03Z/tfruns.d/source.tar.gz,local,training
203,runs/2022-10-13T08-51-04Z,796.9973,885.3781,88.3808,3.1803,2.6642,NA,NA,32,64,64,0.1,32,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-51-04Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829959940>,0.100000001,nn_exp.R,13/10/2022 8:51,13/10/2022 8:51,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-51-04Z/tfruns.d/source.tar.gz,local,training
188,runs/2022-10-13T09-01-10Z,1103.3638,1013.9874,89.3764,3.6517,3.9773,NA,NA,32,32,64,0.1,128,64,NA,NA,NA,NA,NA,35,33,runs/2022-10-13T09-01-10Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829AB1310>,0.100000001,nn_exp.R,13/10/2022 9:01,13/10/2022 9:01,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-01-10Z/tfruns.d/source.tar.gz,local,training
132,runs/2022-10-13T10-23-15Z,705.0474,801.8581,96.8107,2.8869,2.4053,NA,NA,128,128,32,0.001,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-23-15Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B09C1CA0>,0.001,nn_exp.R,13/10/2022 10:23,13/10/2022 10:24,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-23-15Z/tfruns.d/source.tar.gz,local,training
91,runs/2022-10-13T11-18-02Z,700.1338,797.9001,97.7663,2.8689,2.3945,NA,NA,64,128,64,0.01,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-18-02Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8146E8A90>,0.01,nn_exp.R,13/10/2022 11:18,13/10/2022 11:18,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-18-02Z/tfruns.d/source.tar.gz,local,training
178,runs/2022-10-13T09-09-19Z,1060.3972,962.2084,98.1888,3.4681,3.5653,NA,NA,64,32,64,0.1,32,128,NA,NA,NA,NA,NA,35,33,runs/2022-10-13T09-09-19Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8255D3C70>,0.100000001,nn_exp.R,13/10/2022 9:09,13/10/2022 9:09,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-09-19Z/tfruns.d/source.tar.gz,local,training
73,runs/2022-10-13T12-04-11Z,636.2484,734.4656,98.2172,2.6405,2.1636,NA,NA,64,64,16,0.01,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T12-04-11Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A8E2AC0>,0.01,nn_exp.R,13/10/2022 12:04,13/10/2022 12:05,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-04-11Z/tfruns.d/source.tar.gz,local,training
202,runs/2022-10-13T08-51-37Z,808.1075,906.9078,98.8003,3.2605,2.9453,NA,NA,64,64,64,0.1,32,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-51-37Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829980C70>,0.100000001,nn_exp.R,13/10/2022 8:51,13/10/2022 8:52,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-51-37Z/tfruns.d/source.tar.gz,local,training
88,runs/2022-10-13T11-21-24Z,767.1169,866.0399,98.923,3.1243,2.7169,NA,NA,64,128,64,0.001,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-21-24Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BC02580>,0.001,nn_exp.R,13/10/2022 11:21,13/10/2022 11:22,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-21-24Z/tfruns.d/source.tar.gz,local,training
343,runs/2022-10-13T05-21-10Z,1002.3497,1101.4695,99.1198,3.9691,3.6357,NA,NA,64,128,16,0.1,128,64,NA,NA,NA,NA,NA,35,22,runs/2022-10-13T05-21-10Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812463220>,0.100000001,nn_exp.R,13/10/2022 5:21,13/10/2022 5:22,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-21-10Z/tfruns.d/source.tar.gz,local,training
87,runs/2022-10-13T11-22-18Z,841.5389,940.8476,99.3087,3.4058,2.9327,NA,NA,64,128,128,0.001,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-22-18Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81B92F1F0>,0.001,nn_exp.R,13/10/2022 11:22,13/10/2022 11:22,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-22-18Z/tfruns.d/source.tar.gz,local,training
57,runs/2022-10-13T12-38-13Z,771.9224,871.4642,99.5418,3.1443,2.6769,NA,NA,64,128,32,0.001,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T12-38-13Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8134B76D0>,0.001,nn_exp.R,13/10/2022 12:38,13/10/2022 12:39,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-38-13Z/tfruns.d/source.tar.gz,local,training
25,runs/2022-10-13T14-39-22Z,907.2463,806.4687,100.7776,2.9009,3.113,NA,NA,64,64,32,0.005,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T14-39-22Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BC04F70>,0.005,nn_exp.R,13/10/2022 14:39,13/10/2022 14:40,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T14-39-22Z/tfruns.d/source.tar.gz,local,training
397,runs/2022-10-13T03-35-43Z,921.2556,1023.7064,102.4508,3.7063,3.374,NA,NA,32,32,16,0.1,32,32,0,0,0,0,NA,35,20,runs/2022-10-13T03-35-43Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E43C3A0>,0.100000001,nn_exp.R,13/10/2022 3:35,13/10/2022 3:36,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   layer_dropout(rate = FLAGS$dropout1)  .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=8, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,runs/2022-10-13T03-35-43Z/tfruns.d/source.tar.gz,local,training
168,runs/2022-10-13T09-15-56Z,1194.1021,1091.5535,102.5486,3.9552,4.4067,NA,NA,128,32,64,0.1,64,128,NA,NA,NA,NA,NA,35,22,runs/2022-10-13T09-15-56Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8411C9910>,0.100000001,nn_exp.R,13/10/2022 9:15,13/10/2022 9:16,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-15-56Z/tfruns.d/source.tar.gz,local,training
198,runs/2022-10-13T08-53-56Z,882.7057,985.3989,102.6932,3.5655,3.2164,NA,NA,128,128,64,0.1,32,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-53-56Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BE177C0>,0.100000001,nn_exp.R,13/10/2022 8:53,13/10/2022 8:55,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-53-56Z/tfruns.d/source.tar.gz,local,training
305,runs/2022-10-13T06-48-54Z,1094.1666,990.4941,103.6725,3.5799,4.0564,NA,NA,32,32,32,0.1,64,32,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T06-48-54Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81241C430>,0.100000001,nn_exp.R,13/10/2022 6:48,13/10/2022 6:49,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-48-54Z/tfruns.d/source.tar.gz,local,training
70,runs/2022-10-13T12-09-56Z,708.1361,812.0664,103.9303,2.9299,2.4353,NA,NA,64,64,16,0.001,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T12-09-56Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B820469C40>,0.001,nn_exp.R,13/10/2022 12:09,13/10/2022 12:11,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-09-56Z/tfruns.d/source.tar.gz,local,training
106,runs/2022-10-13T11-04-00Z,986.0564,882.0944,103.962,3.1836,3.3234,NA,NA,64,64,64,0.001,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-04-00Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FBE9610>,0.001,nn_exp.R,13/10/2022 11:04,13/10/2022 11:04,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-04-00Z/tfruns.d/source.tar.gz,local,training
78,runs/2022-10-13T11-33-33Z,870.5926,975.2017,104.6091,3.5429,3.0594,NA,NA,128,128,128,0.001,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-33-33Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81470BBE0>,0.001,nn_exp.R,13/10/2022 11:33,13/10/2022 11:34,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-33-33Z/tfruns.d/source.tar.gz,local,training
123,runs/2022-10-13T10-32-40Z,955.1935,849.48,105.7135,3.0605,3.197,NA,NA,64,128,32,0.001,16,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-32-40Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B811E82400>,0.001,nn_exp.R,13/10/2022 10:32,13/10/2022 10:33,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-32-40Z/tfruns.d/source.tar.gz,local,training
121,runs/2022-10-13T10-41-31Z,740.4147,848.7314,108.3167,3.066,2.5807,NA,NA,64,64,32,0.001,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-41-31Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0318040>,0.001,nn_exp.R,13/10/2022 10:41,13/10/2022 10:42,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-41-31Z/tfruns.d/source.tar.gz,local,training
135,runs/2022-10-13T10-19-59Z,724.3657,833.2083,108.8426,3.0043,2.4865,NA,NA,128,128,32,0.001,16,16,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-19-59Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81499E610>,0.001,nn_exp.R,13/10/2022 10:20,13/10/2022 10:21,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-19-59Z/tfruns.d/source.tar.gz,local,training
228,runs/2022-10-13T08-26-16Z,849.7093,958.8494,109.1401,3.4451,3.0145,NA,NA,128,64,64,0.1,32,32,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T08-26-16Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0B564C0>,0.100000001,nn_exp.R,13/10/2022 8:26,13/10/2022 8:27,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-26-16Z/tfruns.d/source.tar.gz,local,training
313,runs/2022-10-13T06-41-35Z,905.8267,1014.9744,109.1477,3.67,3.061,NA,NA,64,32,32,0.1,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-41-35Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FA51A90>,0.100000001,nn_exp.R,13/10/2022 6:41,13/10/2022 6:42,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-41-35Z/tfruns.d/source.tar.gz,local,training
138,runs/2022-10-13T10-15-09Z,901.0681,1010.6567,109.5886,3.6711,3.128,NA,NA,128,64,32,0.001,16,16,NA,NA,NA,NA,NA,35,14,runs/2022-10-13T10-15-09Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829956E80>,0.001,nn_exp.R,13/10/2022 10:15,13/10/2022 10:16,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,runs/2022-10-13T10-15-09Z/tfruns.d/source.tar.gz,local,training
155,runs/2022-10-13T09-27-55Z,1193.2365,1082.632,110.6045,3.9209,4.3759,NA,NA,32,128,64,0.1,128,128,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T09-27-55Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814711190>,0.100000001,nn_exp.R,13/10/2022 9:27,13/10/2022 9:29,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-27-55Z/tfruns.d/source.tar.gz,local,training
60,runs/2022-10-13T12-31-29Z,637.652,748.3308,110.6788,2.6962,2.1954,NA,NA,64,128,32,0.01,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T12-31-29Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B820387CA0>,0.01,nn_exp.R,13/10/2022 12:31,13/10/2022 12:32,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-31-29Z/tfruns.d/source.tar.gz,local,training
136,runs/2022-10-13T10-19-02Z,735.2396,845.9794,110.7398,3.0446,2.586,NA,NA,64,64,32,0.001,32,16,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-19-02Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A89E940>,0.001,nn_exp.R,13/10/2022 10:19,13/10/2022 10:19,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-19-02Z/tfruns.d/source.tar.gz,local,training
325,runs/2022-10-13T06-19-50Z,892.3072,1005.5813,113.2741,3.6322,3.2195,NA,NA,64,128,16,0.1,64,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-19-50Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC2978B0>,0.100000001,nn_exp.R,13/10/2022 6:19,13/10/2022 6:21,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-19-50Z/tfruns.d/source.tar.gz,local,training
105,runs/2022-10-13T11-04-35Z,851.8894,965.2969,113.4075,3.508,2.985,NA,NA,64,64,128,0.001,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-04-35Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCB27E20>,0.001,nn_exp.R,13/10/2022 11:04,13/10/2022 11:05,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-04-35Z/tfruns.d/source.tar.gz,local,training
32,runs/2022-10-13T14-19-18Z,616.8336,732.4242,115.5906,2.6349,2.1863,NA,NA,64,64,32,0.01,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T14-19-18Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8410441C0>,0.01,nn_exp.R,13/10/2022 14:19,13/10/2022 14:20,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T14-19-18Z/tfruns.d/source.tar.gz,local,training
125,runs/2022-10-13T10-30-36Z,740.8794,856.6744,115.795,3.0868,2.5813,NA,NA,128,64,32,0.001,16,16,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-30-36Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814A20E20>,0.001,nn_exp.R,13/10/2022 10:30,13/10/2022 10:31,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-30-36Z/tfruns.d/source.tar.gz,local,training
334,runs/2022-10-13T06-05-36Z,925.3678,1042.1637,116.7959,3.7653,3.3487,NA,NA,64,128,16,0.1,32,128,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T06-05-36Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812412EB0>,0.100000001,nn_exp.R,13/10/2022 6:05,13/10/2022 6:07,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-05-36Z/tfruns.d/source.tar.gz,local,training
196,runs/2022-10-13T08-55-41Z,812.2194,930.6768,118.4574,3.3593,2.7173,NA,NA,64,32,64,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-55-41Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC0C5AC0>,0.100000001,nn_exp.R,13/10/2022 8:55,13/10/2022 8:56,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-55-41Z/tfruns.d/source.tar.gz,local,training
1,runs/2022-10-13T15-52-37Z,685.629,804.5583,118.9293,2.8883,2.4298,NA,NA,128,128,32,0.005,NA,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T15-52-37Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B058D4F0>,0.005,nn_exp.R,13/10/2022 15:52,13/10/2022 15:54,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   #flag_integer(""d ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T15-52-37Z/tfruns.d/source.tar.gz,local,training
174,runs/2022-10-13T09-11-33Z,827.3054,946.5643,119.2589,3.4092,2.9942,NA,NA,128,64,64,0.1,32,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-11-33Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BC73BE0>,0.100000001,nn_exp.R,13/10/2022 9:11,13/10/2022 9:12,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-11-33Z/tfruns.d/source.tar.gz,local,training
146,runs/2022-10-13T09-41-19Z,707.8422,827.291,119.4488,2.9767,2.4187,NA,NA,32,128,16,0.01,32,32,NA,NA,NA,NA,NA,35,27,runs/2022-10-13T09-41-19Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BCACA60>,0.01,nn_exp.R,13/10/2022 9:41,13/10/2022 9:42,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-41-19Z/tfruns.d/source.tar.gz,local,training
173,runs/2022-10-13T09-12-13Z,1153.8892,1033.8179,120.0713,3.7288,3.8355,NA,NA,32,128,64,0.1,32,128,NA,NA,NA,NA,NA,35,30,runs/2022-10-13T09-12-13Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC08DBB0>,0.100000001,nn_exp.R,13/10/2022 9:12,13/10/2022 9:12,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-12-13Z/tfruns.d/source.tar.gz,local,training
229,runs/2022-10-13T08-25-20Z,841.0805,961.5798,120.4993,3.4679,2.8125,NA,NA,64,64,64,0.1,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-25-20Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829A4F910>,0.100000001,nn_exp.R,13/10/2022 8:25,13/10/2022 8:26,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-25-20Z/tfruns.d/source.tar.gz,local,training
5,runs/2022-10-13T15-48-06Z,698.2008,818.7193,120.5185,2.9576,2.4632,NA,NA,32,128,32,0.005,NA,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T15-48-06Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E0E7310>,0.005,nn_exp.R,13/10/2022 15:48,13/10/2022 15:48,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   #flag_integer(""d ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T15-48-06Z/tfruns.d/source.tar.gz,local,training
64,runs/2022-10-13T12-23-15Z,674.1743,795.567,121.3927,2.8629,2.3585,NA,NA,128,64,16,0.001,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T12-23-15Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FAD3DF0>,0.001,nn_exp.R,13/10/2022 12:23,13/10/2022 12:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-23-15Z/tfruns.d/source.tar.gz,local,training
80,runs/2022-10-13T11-29-52Z,660.3192,782.6967,122.3775,2.8145,2.2824,NA,NA,128,128,16,0.001,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-29-52Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0253BB0>,0.001,nn_exp.R,13/10/2022 11:29,13/10/2022 11:32,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-29-52Z/tfruns.d/source.tar.gz,local,training
189,runs/2022-10-13T09-00-03Z,1073.0122,950.3361,122.6761,3.4423,3.9211,NA,NA,128,128,64,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-00-03Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B810E33340>,0.100000001,nn_exp.R,13/10/2022 9:00,13/10/2022 9:01,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-00-03Z/tfruns.d/source.tar.gz,local,training
383,runs/2022-10-13T04-00-06Z,867.4589,990.3648,122.9059,3.5764,3.1447,NA,NA,32,64,16,0.1,64,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T04-00-06Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B82997C8E0>,0.100000001,nn_exp.R,13/10/2022 4:00,13/10/2022 4:02,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-00-06Z/tfruns.d/source.tar.gz,local,training
41,runs/2022-10-13T13-27-06Z,893.0192,769.6216,123.3976,2.7627,3.2163,NA,NA,128,64,8,0.001,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T13-27-06Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E170EE0>,0.001,nn_exp.R,13/10/2022 13:27,13/10/2022 13:30,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-27-06Z/tfruns.d/source.tar.gz,local,training
44,runs/2022-10-13T13-21-42Z,694.3729,818.0577,123.6848,2.9461,2.3368,NA,NA,128,64,8,0.01,64,NA,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T13-21-42Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BBF37F0>,0.01,nn_exp.R,13/10/2022 13:21,13/10/2022 13:24,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-21-42Z/tfruns.d/source.tar.gz,local,training
129,runs/2022-10-13T10-26-25Z,907.5557,783.8362,123.7195,2.8279,3.2548,NA,NA,64,128,32,0.01,32,32,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T10-26-25Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BDDB8E0>,0.01,nn_exp.R,13/10/2022 10:26,13/10/2022 10:27,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-26-25Z/tfruns.d/source.tar.gz,local,training
89,runs/2022-10-13T11-19-27Z,664.6467,788.4581,123.8114,2.8275,2.2934,NA,NA,64,128,16,0.001,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-19-27Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B092EA30>,0.001,nn_exp.R,13/10/2022 11:19,13/10/2022 11:21,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-19-27Z/tfruns.d/source.tar.gz,local,training
306,runs/2022-10-13T06-48-11Z,1195.5125,1070.6895,124.823,3.8619,3.897,NA,NA,128,128,32,0.1,32,32,NA,NA,NA,NA,NA,35,21,runs/2022-10-13T06-48-11Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC23FC40>,0.100000001,nn_exp.R,13/10/2022 6:48,13/10/2022 6:48,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-48-11Z/tfruns.d/source.tar.gz,local,training
35,runs/2022-10-13T13-38-06Z,614.8256,739.7491,124.9235,2.6585,2.0873,NA,NA,64,128,8,0.001,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T13-38-06Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A85C8E0>,0.001,nn_exp.R,13/10/2022 13:38,13/10/2022 13:41,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-38-06Z/tfruns.d/source.tar.gz,local,training
59,runs/2022-10-13T12-32-45Z,659.0134,784.0979,125.0845,2.8164,2.3252,NA,NA,64,128,8,0.001,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T12-32-45Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8297B0130>,0.001,nn_exp.R,13/10/2022 12:32,13/10/2022 12:36,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-32-45Z/tfruns.d/source.tar.gz,local,training
7,runs/2022-10-13T15-46-11Z,719.691,845.6874,125.9964,3.0495,2.5492,NA,NA,128,64,32,0.005,NA,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T15-46-11Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC04F370>,0.005,nn_exp.R,13/10/2022 15:46,13/10/2022 15:47,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   #flag_integer(""d ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T15-46-11Z/tfruns.d/source.tar.gz,local,training
52,runs/2022-10-13T13-03-18Z,661.9487,788.538,126.5893,2.8377,2.2925,NA,NA,128,128,16,0.001,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T13-03-18Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812436790>,0.001,nn_exp.R,13/10/2022 13:03,13/10/2022 13:09,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-03-18Z/tfruns.d/source.tar.gz,local,training
289,runs/2022-10-13T07-02-33Z,910.7504,1038.1671,127.4167,3.7329,3.381,NA,NA,64,128,32,0.1,128,32,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T07-02-33Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8298B2A30>,0.100000001,nn_exp.R,13/10/2022 7:02,13/10/2022 7:03,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-02-33Z/tfruns.d/source.tar.gz,local,training
329,runs/2022-10-13T06-13-42Z,886.5594,1015.3176,128.7582,3.6651,3.2209,NA,NA,32,64,16,0.1,64,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-13-42Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A9113A0>,0.100000001,nn_exp.R,13/10/2022 6:13,13/10/2022 6:15,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-13-42Z/tfruns.d/source.tar.gz,local,training
40,runs/2022-10-13T13-30-10Z,667.5883,798.5529,130.9646,2.8694,2.3282,NA,NA,128,64,16,0.001,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T13-30-10Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B820374460>,0.001,nn_exp.R,13/10/2022 13:30,13/10/2022 13:31,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-30-10Z/tfruns.d/source.tar.gz,local,training
149,runs/2022-10-13T09-36-44Z,964.7985,833.3412,131.4573,3.0023,3.2366,NA,NA,32,64,16,0.01,32,32,NA,NA,NA,NA,NA,35,23,runs/2022-10-13T09-36-44Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8124203A0>,0.01,nn_exp.R,13/10/2022 9:36,13/10/2022 9:37,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-36-44Z/tfruns.d/source.tar.gz,local,training
151,runs/2022-10-13T09-33-29Z,606.8895,738.5018,131.6123,2.6485,2.1384,NA,NA,64,32,16,0.01,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-33-29Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B825480190>,0.01,nn_exp.R,13/10/2022 9:33,13/10/2022 9:35,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-33-29Z/tfruns.d/source.tar.gz,local,training
108,runs/2022-10-13T11-01-51Z,827.4738,961.8481,134.3743,3.4709,2.9733,NA,NA,64,64,128,0.01,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-01-51Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81B9AF250>,0.01,nn_exp.R,13/10/2022 11:01,13/10/2022 11:02,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-01-51Z/tfruns.d/source.tar.gz,local,training
6,runs/2022-10-13T15-47-12Z,660.4816,794.8979,134.4163,2.8681,2.3211,NA,NA,32,128,32,0.01,NA,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T15-47-12Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829A60BB0>,0.01,nn_exp.R,13/10/2022 15:47,13/10/2022 15:48,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   #flag_integer(""d ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T15-47-12Z/tfruns.d/source.tar.gz,local,training
81,runs/2022-10-13T11-29-01Z,749.1198,883.7994,134.6796,3.1825,2.6862,NA,NA,128,128,128,0.01,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-29-01Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B092E4F0>,0.01,nn_exp.R,13/10/2022 11:29,13/10/2022 11:29,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-29-01Z/tfruns.d/source.tar.gz,local,training
204,runs/2022-10-13T08-50-31Z,840.4367,975.2511,134.8144,3.5068,2.8136,NA,NA,128,32,64,0.1,32,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-50-31Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E30A670>,0.100000001,nn_exp.R,13/10/2022 8:50,13/10/2022 8:51,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-50-31Z/tfruns.d/source.tar.gz,local,training
55,runs/2022-10-13T12-46-47Z,633.2009,768.973,135.7721,2.7617,2.1683,NA,NA,128,128,16,0.01,32,NA,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T12-46-47Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829802C70>,0.01,nn_exp.R,13/10/2022 12:46,13/10/2022 12:51,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-46-47Z/tfruns.d/source.tar.gz,local,training
98,runs/2022-10-13T11-10-20Z,666.9567,803.4091,136.4524,2.891,2.3594,NA,NA,128,64,16,0.001,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-10-20Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8254FE880>,0.001,nn_exp.R,13/10/2022 11:10,13/10/2022 11:12,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-10-20Z/tfruns.d/source.tar.gz,local,training
46,runs/2022-10-13T13-19-12Z,651.2499,788.0273,136.7774,2.8431,2.3015,NA,NA,64,64,16,0.001,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T13-19-12Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BC9ECD0>,0.001,nn_exp.R,13/10/2022 13:19,13/10/2022 13:20,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-19-12Z/tfruns.d/source.tar.gz,local,training
243,runs/2022-10-13T08-02-26Z,1124.3403,987.207,137.1333,3.5753,3.7981,NA,NA,128,128,32,0.1,64,128,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T08-02-26Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8149AF910>,0.100000001,nn_exp.R,13/10/2022 8:02,13/10/2022 8:03,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-02-26Z/tfruns.d/source.tar.gz,local,training
53,runs/2022-10-13T12-54-00Z,638.1017,777.699,139.5973,2.7931,2.1911,NA,NA,128,128,8,0.001,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T12-54-00Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814993340>,0.001,nn_exp.R,13/10/2022 12:54,13/10/2022 13:03,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-54-00Z/tfruns.d/source.tar.gz,local,training
8,runs/2022-10-13T15-45-05Z,694.5933,834.9632,140.3699,3.0073,2.4461,NA,NA,128,64,32,0.01,NA,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T15-45-05Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B825431F10>,0.01,nn_exp.R,13/10/2022 15:45,13/10/2022 15:46,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   #flag_integer(""d ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T15-45-05Z/tfruns.d/source.tar.gz,local,training
301,runs/2022-10-13T06-52-26Z,1112.0538,971.4443,140.6095,3.5133,3.7491,NA,NA,64,64,32,0.1,64,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-52-26Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812455A90>,0.100000001,nn_exp.R,13/10/2022 6:52,13/10/2022 6:53,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-52-26Z/tfruns.d/source.tar.gz,local,training
37,runs/2022-10-13T13-35-09Z,617.2352,759.094,141.8588,2.7339,2.13,NA,NA,64,128,16,0.01,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T13-35-09Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8148B96D0>,0.01,nn_exp.R,13/10/2022 13:35,13/10/2022 13:36,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-35-09Z/tfruns.d/source.tar.gz,local,training
327,runs/2022-10-13T06-16-47Z,844.5129,986.7578,142.2449,3.5653,3.0493,NA,NA,128,64,16,0.1,64,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-16-47Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC165700>,0.100000001,nn_exp.R,13/10/2022 6:16,13/10/2022 6:18,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-16-47Z/tfruns.d/source.tar.gz,local,training
11,runs/2022-10-13T15-42-24Z,741.6974,883.9486,142.2512,3.2037,2.5994,NA,NA,32,64,32,0.005,NA,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T15-42-24Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8679640D0>,0.005,nn_exp.R,13/10/2022 15:42,13/10/2022 15:43,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   #flag_integer(""d ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T15-42-24Z/tfruns.d/source.tar.gz,local,training
3,runs/2022-10-13T15-49-55Z,689.8538,832.6502,142.7964,3.0035,2.4421,NA,NA,64,128,32,0.005,NA,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T15-49-55Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81B9A57C0>,0.005,nn_exp.R,13/10/2022 15:49,13/10/2022 15:51,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   #flag_integer(""d ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T15-49-55Z/tfruns.d/source.tar.gz,local,training
235,runs/2022-10-13T08-17-09Z,862.5484,1006.3796,143.8312,3.6384,3.1252,NA,NA,64,128,32,0.1,128,128,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T08-17-09Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812431D90>,0.100000001,nn_exp.R,13/10/2022 8:17,13/10/2022 8:19,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-17-09Z/tfruns.d/source.tar.gz,local,training
30,runs/2022-10-13T14-31-09Z,661.7906,806.0034,144.2128,2.8964,2.25,NA,NA,64,64,32,0.01,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T14-31-09Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E1BB460>,0.01,nn_exp.R,13/10/2022 14:31,13/10/2022 14:32,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T14-31-09Z/tfruns.d/source.tar.gz,local,training
12,runs/2022-10-13T15-41-28Z,685.3387,830.0131,144.6744,2.9966,2.3837,NA,NA,32,64,32,0.01,NA,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T15-41-28Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B84CB31EB0>,0.01,nn_exp.R,13/10/2022 15:41,13/10/2022 15:42,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   #flag_integer(""d ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T15-41-28Z/tfruns.d/source.tar.gz,local,training
304,runs/2022-10-13T06-49-43Z,811.5713,956.3127,144.7414,3.442,2.8009,NA,NA,64,32,32,0.1,64,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-49-43Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81493FE80>,0.100000001,nn_exp.R,13/10/2022 6:49,13/10/2022 6:50,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-49-43Z/tfruns.d/source.tar.gz,local,training
222,runs/2022-10-13T08-32-16Z,806.086,951.4152,145.3292,3.4281,2.6926,NA,NA,128,32,64,0.1,64,32,NA,NA,NA,NA,NA,35,33,runs/2022-10-13T08-32-16Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8146F9670>,0.100000001,nn_exp.R,13/10/2022 8:32,13/10/2022 8:33,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-32-16Z/tfruns.d/source.tar.gz,local,training
171,runs/2022-10-13T09-13-29Z,832.7817,978.7689,145.9872,3.5267,2.9853,NA,NA,128,128,64,0.1,32,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-13-29Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B029DE20>,0.100000001,nn_exp.R,13/10/2022 9:13,13/10/2022 9:14,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-13-29Z/tfruns.d/source.tar.gz,local,training
2,runs/2022-10-13T15-51-03Z,669.541,815.7961,146.2551,2.9434,2.2959,NA,NA,128,128,32,0.01,NA,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T15-51-03Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B82047E6A0>,0.01,nn_exp.R,13/10/2022 15:51,13/10/2022 15:52,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   #flag_integer(""d ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T15-51-03Z/tfruns.d/source.tar.gz,local,training
200,runs/2022-10-13T08-52-47Z,849.265,995.735,146.47,3.5991,3.0733,NA,NA,32,128,64,0.1,32,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-52-47Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0B565B0>,0.100000001,nn_exp.R,13/10/2022 8:52,13/10/2022 8:53,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-52-47Z/tfruns.d/source.tar.gz,local,training
362,runs/2022-10-13T04-49-37Z,1156.495,1008.9667,147.5283,3.6482,4.2039,NA,NA,32,128,16,0.1,32,64,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T04-49-37Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B04CD370>,0.100000001,nn_exp.R,13/10/2022 4:49,13/10/2022 4:51,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-49-37Z/tfruns.d/source.tar.gz,local,training
130,runs/2022-10-13T10-25-29Z,1036.7899,888.5781,148.2118,3.2163,3.7745,NA,NA,128,64,32,0.001,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-25-29Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E0C6EE0>,0.001,nn_exp.R,13/10/2022 10:25,13/10/2022 10:26,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-25-29Z/tfruns.d/source.tar.gz,local,training
34,runs/2022-10-13T13-41-32Z,934.9442,785.5357,149.4085,2.8224,3.1194,NA,NA,64,128,16,0.001,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T13-41-32Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B82983CE20>,0.001,nn_exp.R,13/10/2022 13:41,13/10/2022 13:43,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-41-32Z/tfruns.d/source.tar.gz,local,training
90,runs/2022-10-13T11-18-45Z,1030.5121,880.8661,149.646,3.1725,3.4457,NA,NA,64,128,128,0.01,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-18-45Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8254DB220>,0.01,nn_exp.R,13/10/2022 11:18,13/10/2022 11:19,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-18-45Z/tfruns.d/source.tar.gz,local,training
72,runs/2022-10-13T12-05-47Z,910.0694,759.2474,150.822,2.7327,3.0442,NA,NA,64,64,32,0.01,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T12-05-47Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0B0CFD0>,0.01,nn_exp.R,13/10/2022 12:05,13/10/2022 12:06,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-05-47Z/tfruns.d/source.tar.gz,local,training
16,runs/2022-10-13T15-37-38Z,654.7838,807.0643,152.2805,2.8991,2.2786,NA,NA,64,32,32,0.01,NA,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T15-37-38Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B84229F3D0>,0.01,nn_exp.R,13/10/2022 15:37,13/10/2022 15:38,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   #flag_integer(""d ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T15-37-38Z/tfruns.d/source.tar.gz,local,training
54,runs/2022-10-13T12-51-22Z,683.1708,835.7157,152.5449,3.0135,2.3238,NA,NA,128,128,32,0.01,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T12-51-22Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A866D00>,0.01,nn_exp.R,13/10/2022 12:51,13/10/2022 12:54,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-51-22Z/tfruns.d/source.tar.gz,local,training
68,runs/2022-10-13T12-12-27Z,657.5773,810.4531,152.8758,2.9174,2.3226,NA,NA,128,64,8,0.01,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T12-12-27Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BE0D2B0>,0.01,nn_exp.R,13/10/2022 12:12,13/10/2022 12:15,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-12-27Z/tfruns.d/source.tar.gz,local,training
62,runs/2022-10-13T12-26-25Z,670.723,824.1182,153.3952,2.9689,2.2846,NA,NA,64,128,8,0.01,32,NA,NA,NA,NA,NA,NA,35,34,runs/2022-10-13T12-26-25Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B09C12B0>,0.01,nn_exp.R,13/10/2022 12:26,13/10/2022 12:29,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-26-25Z/tfruns.d/source.tar.gz,local,training
219,runs/2022-10-13T08-34-51Z,804.556,959.4099,154.8539,3.4802,2.9085,NA,NA,128,64,64,0.1,64,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-34-51Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BC97190>,0.100000001,nn_exp.R,13/10/2022 8:34,13/10/2022 8:35,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-34-51Z/tfruns.d/source.tar.gz,local,training
4,runs/2022-10-13T15-48-56Z,965.3602,810.1176,155.2426,2.9266,3.5026,NA,NA,64,128,32,0.01,NA,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T15-48-56Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8679346A0>,0.01,nn_exp.R,13/10/2022 15:48,13/10/2022 15:49,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   #flag_integer(""d ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T15-48-56Z/tfruns.d/source.tar.gz,local,training
331,runs/2022-10-13T06-10-37Z,1163.379,1007.1464,156.2326,3.6392,3.9279,NA,NA,64,32,16,0.1,64,128,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T06-10-37Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FAC7BB0>,0.100000001,nn_exp.R,13/10/2022 6:10,13/10/2022 6:12,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-10-37Z/tfruns.d/source.tar.gz,local,training
350,runs/2022-10-13T05-11-35Z,1203.2909,1046.9213,156.3696,3.7792,4.0256,NA,NA,32,32,16,0.1,128,64,NA,NA,NA,NA,NA,35,27,runs/2022-10-13T05-11-35Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B05C8880>,0.100000001,nn_exp.R,13/10/2022 5:11,13/10/2022 5:12,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-11-35Z/tfruns.d/source.tar.gz,local,training
145,runs/2022-10-13T09-42-45Z,670.8395,828.6305,157.791,2.985,2.2956,NA,NA,64,128,16,0.01,32,32,NA,NA,NA,NA,NA,35,24,runs/2022-10-13T09-42-45Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B820369580>,0.01,nn_exp.R,13/10/2022 9:42,13/10/2022 9:44,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-42-45Z/tfruns.d/source.tar.gz,local,training
176,runs/2022-10-13T09-10-25Z,828.1655,989.0777,160.9122,3.5776,2.7844,NA,NA,32,64,64,0.1,32,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-10-25Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7ADF1F7F0>,0.100000001,nn_exp.R,13/10/2022 9:10,13/10/2022 9:10,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-10-25Z/tfruns.d/source.tar.gz,local,training
128,runs/2022-10-13T10-27-22Z,625.6758,786.8459,161.1701,2.835,2.1571,NA,NA,64,128,32,0.01,16,16,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-27-22Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814821A90>,0.01,nn_exp.R,13/10/2022 10:27,13/10/2022 10:28,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-27-22Z/tfruns.d/source.tar.gz,local,training
276,runs/2022-10-13T07-14-40Z,767.1024,928.6301,161.5277,3.3414,2.5709,NA,NA,128,32,32,0.1,64,64,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T07-14-40Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8134C3460>,0.100000001,nn_exp.R,13/10/2022 7:14,13/10/2022 7:15,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-14-40Z/tfruns.d/source.tar.gz,local,training
48,runs/2022-10-13T13-15-28Z,584.2437,746.137,161.8933,2.682,2.0191,NA,NA,64,64,32,0.01,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T13-15-28Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81F188AF0>,0.01,nn_exp.R,13/10/2022 13:15,13/10/2022 13:16,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-15-28Z/tfruns.d/source.tar.gz,local,training
323,runs/2022-10-13T06-23-35Z,843.7504,1006.6983,162.9479,3.6373,3.0486,NA,NA,32,32,16,0.1,128,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-23-35Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E404F70>,0.100000001,nn_exp.R,13/10/2022 6:23,13/10/2022 6:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-23-35Z/tfruns.d/source.tar.gz,local,training
215,runs/2022-10-13T08-38-57Z,776.8474,939.8525,163.0051,3.3856,2.6039,NA,NA,32,32,64,0.1,128,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-38-57Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814982EB0>,0.100000001,nn_exp.R,13/10/2022 8:38,13/10/2022 8:39,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-38-57Z/tfruns.d/source.tar.gz,local,training
124,runs/2022-10-13T10-31-39Z,647.9362,810.9839,163.0477,2.9151,2.2288,NA,NA,64,64,32,0.01,16,16,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-31-39Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8298E3DC0>,0.01,nn_exp.R,13/10/2022 10:31,13/10/2022 10:32,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-31-39Z/tfruns.d/source.tar.gz,local,training
253,runs/2022-10-13T07-48-49Z,1126.4031,963.3139,163.0892,3.4691,4.0887,NA,NA,64,128,32,0.1,32,128,NA,NA,NA,NA,NA,35,33,runs/2022-10-13T07-48-49Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8149D5D60>,0.100000001,nn_exp.R,13/10/2022 7:48,13/10/2022 7:50,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-48-49Z/tfruns.d/source.tar.gz,local,training
226,runs/2022-10-13T08-28-03Z,1103.0951,939.0421,164.053,3.3833,3.9794,NA,NA,64,128,64,0.1,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-28-03Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCBA0460>,0.100000001,nn_exp.R,13/10/2022 8:28,13/10/2022 8:28,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-28-03Z/tfruns.d/source.tar.gz,local,training
261,runs/2022-10-13T07-36-15Z,859.4688,1024.0839,164.6151,3.7021,3.1229,NA,NA,128,128,32,0.1,128,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-36-15Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0B20F40>,0.100000001,nn_exp.R,13/10/2022 7:36,13/10/2022 7:39,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-36-15Z/tfruns.d/source.tar.gz,local,training
110,runs/2022-10-13T10-59-56Z,586.3549,752.4854,166.1305,2.6995,2.0045,NA,NA,64,64,16,0.01,32,32,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T10-59-56Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCC258E0>,0.01,nn_exp.R,13/10/2022 10:59,13/10/2022 11:01,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-59-56Z/tfruns.d/source.tar.gz,local,training
165,runs/2022-10-13T09-18-01Z,1184.0806,1015.9115,168.1691,3.6598,4.2861,NA,NA,128,64,64,0.1,64,128,NA,NA,NA,NA,NA,35,33,runs/2022-10-13T09-18-01Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8204C5E50>,0.100000001,nn_exp.R,13/10/2022 9:18,13/10/2022 9:18,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-18-01Z/tfruns.d/source.tar.gz,local,training
21,runs/2022-10-13T14-44-00Z,634.0458,802.2465,168.2007,2.8874,2.1992,NA,NA,64,128,32,0.005,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T14-44-00Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8254868E0>,0.005,nn_exp.R,13/10/2022 14:44,13/10/2022 14:45,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T14-44-00Z/tfruns.d/source.tar.gz,local,training
337,runs/2022-10-13T06-01-02Z,837.0997,1006.698,169.5983,3.6361,3.0239,NA,NA,64,64,16,0.1,32,128,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T06-01-02Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FBD0430>,0.100000001,nn_exp.R,13/10/2022 6:01,13/10/2022 6:02,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-01-02Z/tfruns.d/source.tar.gz,local,training
147,runs/2022-10-13T09-39-24Z,606.2936,777.8643,171.5707,2.8,2.1152,NA,NA,128,64,16,0.01,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-39-24Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B810E47EE0>,0.01,nn_exp.R,13/10/2022 9:39,13/10/2022 9:41,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-39-24Z/tfruns.d/source.tar.gz,local,training
220,runs/2022-10-13T08-33-57Z,1149.3365,977.5995,171.737,3.5399,3.8531,NA,NA,64,64,64,0.1,64,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-33-57Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8147DED00>,0.100000001,nn_exp.R,13/10/2022 8:33,13/10/2022 8:34,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-33-57Z/tfruns.d/source.tar.gz,local,training
24,runs/2022-10-13T14-40-18Z,642.0803,815.4514,173.3711,2.9346,2.2141,NA,NA,128,64,32,0.01,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T14-40-18Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829AB11F0>,0.01,nn_exp.R,13/10/2022 14:40,13/10/2022 14:41,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T14-40-18Z/tfruns.d/source.tar.gz,local,training
236,runs/2022-10-13T08-14-57Z,1136.4598,963.076,173.3838,3.461,3.7493,NA,NA,32,128,32,0.1,128,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-14-57Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BCEB8E0>,0.100000001,nn_exp.R,13/10/2022 8:14,13/10/2022 8:17,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-14-57Z/tfruns.d/source.tar.gz,local,training
134,runs/2022-10-13T10-21-21Z,671.4872,845.4623,173.9751,3.0543,2.3209,NA,NA,128,64,32,0.01,16,16,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-21-21Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8254232E0>,0.01,nn_exp.R,13/10/2022 10:21,13/10/2022 10:22,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-21-21Z/tfruns.d/source.tar.gz,local,training
139,runs/2022-10-13T10-12-38Z,654.668,829.2277,174.5597,2.9889,2.2842,NA,NA,128,64,32,0.01,16,16,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-12-38Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81241B520>,0.01,nn_exp.R,13/10/2022 10:12,13/10/2022 10:15,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-12-38Z/tfruns.d/source.tar.gz,local,training
384,runs/2022-10-13T03-58-00Z,1153.5172,978.9028,174.6144,3.5379,3.8813,NA,NA,128,32,16,0.1,64,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T03-58-00Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A844DF0>,0.100000001,nn_exp.R,13/10/2022 3:58,13/10/2022 4:00,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T03-58-00Z/tfruns.d/source.tar.gz,local,training
99,runs/2022-10-13T11-10-01Z,893.2234,1067.8896,174.6662,3.8616,3.0638,NA,NA,128,64,128,0.01,32,32,NA,NA,NA,NA,NA,35,16,runs/2022-10-13T11-10-01Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B056AE50>,0.01,nn_exp.R,13/10/2022 11:10,13/10/2022 11:10,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-10-01Z/tfruns.d/source.tar.gz,local,training
340,runs/2022-10-13T05-56-06Z,821.6153,996.8666,175.2513,3.6017,2.7437,NA,NA,64,32,16,0.1,32,128,NA,NA,NA,NA,NA,35,29,runs/2022-10-13T05-56-06Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC02F100>,0.100000001,nn_exp.R,13/10/2022 5:56,13/10/2022 5:57,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-56-06Z/tfruns.d/source.tar.gz,local,training
257,runs/2022-10-13T07-43-20Z,774.3018,951.3013,176.9995,3.4407,2.603,NA,NA,32,64,32,0.1,32,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-43-20Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8147A0250>,0.100000001,nn_exp.R,13/10/2022 7:43,13/10/2022 7:44,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-43-20Z/tfruns.d/source.tar.gz,local,training
339,runs/2022-10-13T05-57-33Z,1074.1317,1252.5413,178.4096,4.5448,3.6546,NA,NA,128,32,16,0.1,32,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T05-57-33Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E117CA0>,0.100000001,nn_exp.R,13/10/2022 5:57,13/10/2022 5:59,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-57-33Z/tfruns.d/source.tar.gz,local,training
231,runs/2022-10-13T08-23-32Z,1152.4528,972.7756,179.6772,3.501,4.3203,NA,NA,128,32,64,0.1,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-23-32Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814895D00>,0.100000001,nn_exp.R,13/10/2022 8:23,13/10/2022 8:24,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-23-32Z/tfruns.d/source.tar.gz,local,training
148,runs/2022-10-13T09-37-51Z,573.3195,753.126,179.8065,2.6974,2.0018,NA,NA,64,64,16,0.01,32,32,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T09-37-51Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829B215E0>,0.01,nn_exp.R,13/10/2022 9:37,13/10/2022 9:39,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-37-51Z/tfruns.d/source.tar.gz,local,training
9,runs/2022-10-13T15-44-13Z,1022.7404,842.9271,179.8133,3.0409,3.7005,NA,NA,64,64,32,0.005,NA,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T15-44-13Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B057D2B0>,0.005,nn_exp.R,13/10/2022 15:44,13/10/2022 15:45,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   #flag_integer(""d ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T15-44-13Z/tfruns.d/source.tar.gz,local,training
152,runs/2022-10-13T09-32-19Z,643.4521,823.7964,180.3443,2.9679,2.2343,NA,NA,32,32,16,0.01,32,32,NA,NA,NA,NA,NA,35,26,runs/2022-10-13T09-32-19Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8203DF400>,0.01,nn_exp.R,13/10/2022 9:32,13/10/2022 9:33,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-32-19Z/tfruns.d/source.tar.gz,local,training
227,runs/2022-10-13T08-27-15Z,1147.6346,966.9114,180.7232,3.4814,3.7948,NA,NA,32,128,64,0.1,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-27-15Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8255A7FA0>,0.100000001,nn_exp.R,13/10/2022 8:27,13/10/2022 8:28,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-27-15Z/tfruns.d/source.tar.gz,local,training
160,runs/2022-10-13T09-22-55Z,832.8457,1014.6603,181.8146,3.6455,2.7763,NA,NA,64,32,64,0.1,128,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-22-55Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B031EFD0>,0.100000001,nn_exp.R,13/10/2022 9:22,13/10/2022 9:23,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-22-55Z/tfruns.d/source.tar.gz,local,training
74,runs/2022-10-13T12-01-20Z,616.441,798.2841,181.8431,2.8751,2.1438,NA,NA,64,64,8,0.01,32,NA,NA,NA,NA,NA,NA,35,33,runs/2022-10-13T12-01-20Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81485BFA0>,0.01,nn_exp.R,13/10/2022 12:01,13/10/2022 12:04,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-01-20Z/tfruns.d/source.tar.gz,local,training
61,runs/2022-10-13T12-29-41Z,601.0447,783.7181,182.6734,2.8184,2.0978,NA,NA,64,128,16,0.01,32,NA,NA,NA,NA,NA,NA,35,33,runs/2022-10-13T12-29-41Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCB0BEB0>,0.01,nn_exp.R,13/10/2022 12:29,13/10/2022 12:31,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-29-41Z/tfruns.d/source.tar.gz,local,training
377,runs/2022-10-13T04-14-46Z,853.838,1036.8448,183.0068,3.7504,3.0965,NA,NA,32,32,16,0.1,128,32,NA,NA,NA,NA,NA,35,28,runs/2022-10-13T04-14-46Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829A68820>,0.100000001,nn_exp.R,13/10/2022 4:14,13/10/2022 4:16,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-14-46Z/tfruns.d/source.tar.gz,local,training
342,runs/2022-10-13T05-22-19Z,827.1652,1012.9291,185.7639,3.6584,2.7972,NA,NA,128,128,16,0.1,128,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T05-22-19Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812429370>,0.100000001,nn_exp.R,13/10/2022 5:22,13/10/2022 5:54,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-22-19Z/tfruns.d/source.tar.gz,local,training
345,runs/2022-10-13T05-18-32Z,850.2242,1036.3231,186.0989,3.7408,3.0836,NA,NA,128,64,16,0.1,128,64,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T05-18-32Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B820371100>,0.100000001,nn_exp.R,13/10/2022 5:18,13/10/2022 5:19,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-18-32Z/tfruns.d/source.tar.gz,local,training
191,runs/2022-10-13T08-58-39Z,1194.4586,1007.8854,186.5732,3.6514,3.9727,NA,NA,32,128,64,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-58-39Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E3EE730>,0.100000001,nn_exp.R,13/10/2022 8:58,13/10/2022 8:59,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-58-39Z/tfruns.d/source.tar.gz,local,training
347,runs/2022-10-13T05-15-43Z,1192.5499,1005.1627,187.3872,3.6296,3.8838,NA,NA,32,64,16,0.1,128,64,NA,NA,NA,NA,NA,35,33,runs/2022-10-13T05-15-43Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B02933D0>,0.100000001,nn_exp.R,13/10/2022 5:15,13/10/2022 5:17,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-15-43Z/tfruns.d/source.tar.gz,local,training
312,runs/2022-10-13T06-42-30Z,721.8854,910.5837,188.6983,3.2805,2.5825,NA,NA,128,32,32,0.1,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-42-30Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8148593D0>,0.100000001,nn_exp.R,13/10/2022 6:42,13/10/2022 6:43,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-42-30Z/tfruns.d/source.tar.gz,local,training
374,runs/2022-10-13T04-21-14Z,1169.0171,980.3063,188.7108,3.5401,3.8917,NA,NA,32,64,16,0.1,128,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T04-21-14Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC306CA0>,0.100000001,nn_exp.R,13/10/2022 4:21,13/10/2022 4:23,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-21-14Z/tfruns.d/source.tar.gz,local,training
328,runs/2022-10-13T06-15-16Z,812.8424,1002.6391,189.7967,3.6183,2.931,NA,NA,64,64,16,0.1,64,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-15-16Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8149F3310>,0.100000001,nn_exp.R,13/10/2022 6:15,13/10/2022 6:16,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-15-16Z/tfruns.d/source.tar.gz,local,training
23,runs/2022-10-13T14-41-34Z,668.9581,858.759,189.8009,3.0922,2.3063,NA,NA,128,64,32,0.005,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T14-41-34Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829977460>,0.005,nn_exp.R,13/10/2022 14:41,13/10/2022 14:42,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T14-41-34Z/tfruns.d/source.tar.gz,local,training
83,runs/2022-10-13T11-25-50Z,635.6913,825.5165,189.8252,2.9703,2.2285,NA,NA,128,128,16,0.01,32,32,NA,NA,NA,NA,NA,35,28,runs/2022-10-13T11-25-50Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B059E460>,0.01,nn_exp.R,13/10/2022 11:25,13/10/2022 11:27,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-25-50Z/tfruns.d/source.tar.gz,local,training
297,runs/2022-10-13T06-55-51Z,790.3299,982.6049,192.275,3.559,2.7164,NA,NA,128,128,32,0.1,64,32,NA,NA,NA,NA,NA,35,30,runs/2022-10-13T06-55-51Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FAD3820>,0.100000001,nn_exp.R,13/10/2022 6:55,13/10/2022 6:56,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-55-51Z/tfruns.d/source.tar.gz,local,training
182,runs/2022-10-13T09-05-11Z,783.8005,976.3524,192.5519,3.5162,2.6849,NA,NA,32,128,64,0.1,128,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-05-11Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0351B80>,0.100000001,nn_exp.R,13/10/2022 9:05,13/10/2022 9:06,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-05-11Z/tfruns.d/source.tar.gz,local,training
36,runs/2022-10-13T13-36-58Z,560.108,753.0737,192.9657,2.7008,1.9337,NA,NA,64,128,32,0.01,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T13-36-58Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E34B190>,0.01,nn_exp.R,13/10/2022 13:36,13/10/2022 13:38,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-36-58Z/tfruns.d/source.tar.gz,local,training
282,runs/2022-10-13T07-09-15Z,1153.5327,958.0444,195.4883,3.4615,4.2159,NA,NA,128,64,32,0.1,32,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-09-15Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0544190>,0.100000001,nn_exp.R,13/10/2022 7:09,13/10/2022 7:10,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-09-15Z/tfruns.d/source.tar.gz,local,training
232,runs/2022-10-13T08-22-43Z,716.6061,914.2659,197.6598,3.2874,2.4306,NA,NA,64,32,64,0.1,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-22-43Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC0C4DC0>,0.100000001,nn_exp.R,13/10/2022 8:22,13/10/2022 8:23,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-22-43Z/tfruns.d/source.tar.gz,local,training
177,runs/2022-10-13T09-09-50Z,719.8474,918.6698,198.8224,3.3085,2.5425,NA,NA,128,32,64,0.1,32,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-09-50Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8254E3B80>,0.100000001,nn_exp.R,13/10/2022 9:09,13/10/2022 9:10,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-09-50Z/tfruns.d/source.tar.gz,local,training
50,runs/2022-10-13T13-11-16Z,1019.7896,819.0836,200.706,2.9515,3.4052,NA,NA,64,64,8,0.01,64,NA,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T13-11-16Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8255CEEE0>,0.01,nn_exp.R,13/10/2022 13:11,13/10/2022 13:14,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T13-11-16Z/tfruns.d/source.tar.gz,local,training
56,runs/2022-10-13T12-39-31Z,581.3786,782.1801,200.8015,2.8128,2.0388,NA,NA,128,128,8,0.01,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T12-39-31Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0A169A0>,0.01,nn_exp.R,13/10/2022 12:39,13/10/2022 12:46,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-39-31Z/tfruns.d/source.tar.gz,local,training
154,runs/2022-10-13T09-29-16Z,1163.2611,961.7108,201.5503,3.4608,4.2766,NA,NA,64,128,64,0.1,128,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-29-16Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8147B7610>,0.100000001,nn_exp.R,13/10/2022 9:29,13/10/2022 9:30,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-29-16Z/tfruns.d/source.tar.gz,local,training
311,runs/2022-10-13T06-43-28Z,761.5934,963.764,202.1706,3.4839,2.7371,NA,NA,32,64,32,0.1,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-43-28Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829A44640>,0.100000001,nn_exp.R,13/10/2022 6:43,13/10/2022 6:44,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-43-28Z/tfruns.d/source.tar.gz,local,training
322,runs/2022-10-13T06-25-32Z,851.3,1053.6884,202.3884,3.8126,2.8541,NA,NA,64,32,16,0.1,128,128,NA,NA,NA,NA,NA,35,26,runs/2022-10-13T06-25-32Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BE12FA0>,0.100000001,nn_exp.R,13/10/2022 6:25,13/10/2022 6:27,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-25-32Z/tfruns.d/source.tar.gz,local,training
288,runs/2022-10-13T07-03-43Z,1136.5266,933.3997,203.1269,3.3803,4.1306,NA,NA,128,128,32,0.1,128,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-03-43Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0B6DDF0>,0.100000001,nn_exp.R,13/10/2022 7:03,13/10/2022 7:05,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-03-43Z/tfruns.d/source.tar.gz,local,training
302,runs/2022-10-13T06-51-32Z,1198.3274,995.1461,203.1813,3.6008,4.0429,NA,NA,32,64,32,0.1,64,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-51-32Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A841BE0>,0.100000001,nn_exp.R,13/10/2022 6:51,13/10/2022 6:52,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-51-32Z/tfruns.d/source.tar.gz,local,training
344,runs/2022-10-13T05-19-57Z,1255.2155,1048.6259,206.5896,3.7893,4.2017,NA,NA,32,128,16,0.1,128,64,NA,NA,NA,NA,NA,35,24,runs/2022-10-13T05-19-57Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814824640>,0.100000001,nn_exp.R,13/10/2022 5:19,13/10/2022 5:21,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-19-57Z/tfruns.d/source.tar.gz,local,training
194,runs/2022-10-13T08-56-49Z,707.3043,915.9071,208.6028,3.298,2.5124,NA,NA,32,64,64,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-56-49Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BBFACD30>,0.100000001,nn_exp.R,13/10/2022 8:56,13/10/2022 8:57,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-56-49Z/tfruns.d/source.tar.gz,local,training
321,runs/2022-10-13T06-27-04Z,811.3246,1020.1242,208.7996,3.6757,2.9254,NA,NA,128,32,16,0.1,128,128,NA,NA,NA,NA,NA,35,34,runs/2022-10-13T06-27-04Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0AE2BE0>,0.100000001,nn_exp.R,13/10/2022 6:27,13/10/2022 6:28,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-27-04Z/tfruns.d/source.tar.gz,local,training
213,runs/2022-10-13T08-40-21Z,795.8943,1005.6692,209.7749,3.6168,2.6753,NA,NA,128,32,64,0.1,128,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-40-21Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E2BC760>,0.100000001,nn_exp.R,13/10/2022 8:40,13/10/2022 8:41,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-40-21Z/tfruns.d/source.tar.gz,local,training
144,runs/2022-10-13T09-44-01Z,595.1414,805.2947,210.1533,2.9054,2.0907,NA,NA,128,128,16,0.01,32,32,NA,NA,NA,NA,NA,35,28,runs/2022-10-13T09-44-01Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B091C310>,0.01,nn_exp.R,13/10/2022 9:44,13/10/2022 9:46,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,runs/2022-10-13T09-44-01Z/tfruns.d/source.tar.gz,local,training
247,runs/2022-10-13T07-57-15Z,731.5897,942.1647,210.575,3.4128,2.6232,NA,NA,64,64,32,0.1,64,128,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T07-57-15Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0341F70>,0.100000001,nn_exp.R,13/10/2022 7:57,13/10/2022 7:58,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-57-15Z/tfruns.d/source.tar.gz,local,training
251,runs/2022-10-13T07-52-09Z,1260.0668,1048.7966,211.2702,3.7826,4.6118,NA,NA,32,32,32,0.1,64,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-52-09Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B841061760>,0.100000001,nn_exp.R,13/10/2022 7:52,13/10/2022 7:53,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-52-09Z/tfruns.d/source.tar.gz,local,training
100,runs/2022-10-13T11-09-24Z,1119.3184,907.7315,211.5869,3.2727,3.7144,NA,NA,128,64,64,0.01,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-09-24Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829A26CD0>,0.01,nn_exp.R,13/10/2022 11:09,13/10/2022 11:10,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-09-24Z/tfruns.d/source.tar.gz,local,training
195,runs/2022-10-13T08-56-15Z,696.3671,908.045,211.6779,3.2645,2.4427,NA,NA,128,32,64,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-56-15Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCBD3550>,0.100000001,nn_exp.R,13/10/2022 8:56,13/10/2022 8:56,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-56-15Z/tfruns.d/source.tar.gz,local,training
292,runs/2022-10-13T07-00-10Z,747.5146,962.3405,214.8259,3.4822,2.6782,NA,NA,64,64,32,0.1,128,32,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T07-00-10Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BCE2E20>,0.100000001,nn_exp.R,13/10/2022 7:00,13/10/2022 7:00,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-00-10Z/tfruns.d/source.tar.gz,local,training
299,runs/2022-10-13T06-54-13Z,846.4653,1061.5596,215.0943,3.8408,2.8844,NA,NA,32,128,32,0.1,64,32,NA,NA,NA,NA,NA,35,23,runs/2022-10-13T06-54-13Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC14DD60>,0.100000001,nn_exp.R,13/10/2022 6:54,13/10/2022 6:54,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-54-13Z/tfruns.d/source.tar.gz,local,training
369,runs/2022-10-13T04-33-11Z,806.2631,1022.0961,215.833,3.688,2.7118,NA,NA,128,128,16,0.1,128,32,NA,NA,NA,NA,NA,35,34,runs/2022-10-13T04-33-11Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E3E8A00>,0.100000001,nn_exp.R,13/10/2022 4:33,13/10/2022 4:36,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-33-11Z/tfruns.d/source.tar.gz,local,training
161,runs/2022-10-13T09-21-53Z,767.9617,984.6464,216.6847,3.566,2.6176,NA,NA,32,32,64,0.1,128,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-21-53Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B84105C5B0>,0.100000001,nn_exp.R,13/10/2022 9:21,13/10/2022 9:22,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-21-53Z/tfruns.d/source.tar.gz,local,training
258,runs/2022-10-13T07-41-52Z,766.8297,984.0184,217.1887,3.5531,2.6213,NA,NA,128,32,32,0.1,32,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-41-52Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FAC7790>,0.100000001,nn_exp.R,13/10/2022 7:41,13/10/2022 7:43,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-41-52Z/tfruns.d/source.tar.gz,local,training
389,runs/2022-10-13T03-47-49Z,791.991,1013.9828,221.9918,3.659,2.6914,NA,NA,32,128,16,0.1,32,32,NA,NA,NA,NA,NA,35,34,runs/2022-10-13T03-47-49Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81F1953D0>,0.100000001,nn_exp.R,13/10/2022 3:47,13/10/2022 3:49,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T03-47-49Z/tfruns.d/source.tar.gz,local,training
224,runs/2022-10-13T08-30-51Z,1176.2267,952.0683,224.1584,3.4201,3.9779,NA,NA,32,32,64,0.1,64,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-30-51Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812420D60>,0.100000001,nn_exp.R,13/10/2022 8:30,13/10/2022 8:31,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-30-51Z/tfruns.d/source.tar.gz,local,training
82,runs/2022-10-13T11-27-53Z,1111.5964,887.4063,224.1901,3.2054,3.7136,NA,NA,128,128,64,0.01,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T11-27-53Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E27CFA0>,0.01,nn_exp.R,13/10/2022 11:27,13/10/2022 11:29,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T11-27-53Z/tfruns.d/source.tar.gz,local,training
255,runs/2022-10-13T07-46-13Z,1236.4423,1010.6617,225.7806,3.6607,4.5001,NA,NA,128,64,32,0.1,32,128,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T07-46-13Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B84105CB80>,0.100000001,nn_exp.R,13/10/2022 7:46,13/10/2022 7:47,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-46-13Z/tfruns.d/source.tar.gz,local,training
207,runs/2022-10-13T08-48-07Z,1207.051,980.4387,226.6123,3.5338,4.4402,NA,NA,128,128,64,0.1,128,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-48-07Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A811940>,0.100000001,nn_exp.R,13/10/2022 8:48,13/10/2022 8:49,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-48-07Z/tfruns.d/source.tar.gz,local,training
163,runs/2022-10-13T09-19-48Z,1208.0292,980.6392,227.39,3.5496,4.0615,NA,NA,64,128,64,0.1,64,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-19-48Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8299C8280>,0.100000001,nn_exp.R,13/10/2022 9:19,13/10/2022 9:20,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-19-48Z/tfruns.d/source.tar.gz,local,training
205,runs/2022-10-13T08-49-59Z,1201.6692,971.5535,230.1157,3.5227,4.3586,NA,NA,64,32,64,0.1,32,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-49-59Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8149AE0D0>,0.100000001,nn_exp.R,13/10/2022 8:50,13/10/2022 8:50,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-49-59Z/tfruns.d/source.tar.gz,local,training
238,runs/2022-10-13T08-11-09Z,719.8677,951.2838,231.4161,3.4394,2.5704,NA,NA,64,64,32,0.1,128,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-11-09Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC3062E0>,0.100000001,nn_exp.R,13/10/2022 8:11,13/10/2022 8:13,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-11-09Z/tfruns.d/source.tar.gz,local,training
239,runs/2022-10-13T08-09-01Z,747.2878,978.809,231.5212,3.5401,2.6835,NA,NA,32,64,32,0.1,128,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-09-01Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B058D910>,0.100000001,nn_exp.R,13/10/2022 8:09,13/10/2022 8:11,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-09-01Z/tfruns.d/source.tar.gz,local,training
233,runs/2022-10-13T08-22-01Z,733.9169,965.5461,231.6292,3.4791,2.5034,NA,NA,32,32,64,0.1,32,32,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T08-22-01Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8254ADEB0>,0.100000001,nn_exp.R,13/10/2022 8:22,13/10/2022 8:22,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-22-01Z/tfruns.d/source.tar.gz,local,training
170,runs/2022-10-13T09-14-38Z,803.7413,1035.5541,231.8128,3.7525,2.7106,NA,NA,32,32,64,0.1,64,128,NA,NA,NA,NA,NA,35,30,runs/2022-10-13T09-14-38Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8149F5C70>,0.100000001,nn_exp.R,13/10/2022 9:14,13/10/2022 9:15,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-14-38Z/tfruns.d/source.tar.gz,local,training
378,runs/2022-10-13T04-11-24Z,1233.9713,1001.6599,232.3114,3.6154,4.5094,NA,NA,128,128,16,0.1,64,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T04-11-24Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A818DF0>,0.100000001,nn_exp.R,13/10/2022 4:11,13/10/2022 4:14,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-11-24Z/tfruns.d/source.tar.gz,local,training
250,runs/2022-10-13T07-53-23Z,1197.0718,964.0203,233.0515,3.4697,4.4798,NA,NA,64,32,32,0.1,64,128,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T07-53-23Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BD512B0>,0.100000001,nn_exp.R,13/10/2022 7:53,13/10/2022 7:54,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-53-23Z/tfruns.d/source.tar.gz,local,training
166,runs/2022-10-13T09-17-12Z,771.3149,1007.2397,235.9248,3.648,2.6308,NA,NA,64,64,64,0.1,64,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-17-12Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCC11D90>,0.100000001,nn_exp.R,13/10/2022 9:17,13/10/2022 9:18,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-17-12Z/tfruns.d/source.tar.gz,local,training
286,runs/2022-10-13T07-05-54Z,726.5903,964.0244,237.4341,3.4898,2.5948,NA,NA,64,32,32,0.1,32,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-05-54Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8255976D0>,0.100000001,nn_exp.R,13/10/2022 7:05,13/10/2022 7:06,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-05-54Z/tfruns.d/source.tar.gz,local,training
310,runs/2022-10-13T06-44-23Z,734.3703,971.8352,237.4649,3.5209,2.6292,NA,NA,64,64,32,0.1,32,32,NA,NA,NA,NA,NA,35,28,runs/2022-10-13T06-44-23Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0662DF0>,0.100000001,nn_exp.R,13/10/2022 6:44,13/10/2022 6:45,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-44-23Z/tfruns.d/source.tar.gz,local,training
371,runs/2022-10-13T04-28-37Z,750.6362,988.9749,238.3387,3.5695,2.6864,NA,NA,32,128,16,0.1,128,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T04-28-37Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E3A1D90>,0.100000001,nn_exp.R,13/10/2022 4:28,13/10/2022 4:31,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-28-37Z/tfruns.d/source.tar.gz,local,training
295,runs/2022-10-13T06-57-37Z,1228.8214,989.4431,239.3783,3.5852,4.5202,NA,NA,64,32,32,0.1,128,32,NA,NA,NA,NA,NA,35,26,runs/2022-10-13T06-57-37Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCB60580>,0.100000001,nn_exp.R,13/10/2022 6:57,13/10/2022 6:58,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-57-37Z/tfruns.d/source.tar.gz,local,training
218,runs/2022-10-13T08-35-52Z,902.6379,1143.5377,240.8998,4.1358,3.2875,NA,NA,32,128,64,0.1,64,32,NA,NA,NA,NA,NA,35,21,runs/2022-10-13T08-35-52Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B82553EE80>,0.100000001,nn_exp.R,13/10/2022 8:35,13/10/2022 8:36,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-35-52Z/tfruns.d/source.tar.gz,local,training
283,runs/2022-10-13T07-08-23Z,675.5659,916.6062,241.0403,3.3004,2.3148,NA,NA,64,64,32,0.1,32,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-08-23Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0ABCAC0>,0.100000001,nn_exp.R,13/10/2022 7:08,13/10/2022 7:09,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-08-23Z/tfruns.d/source.tar.gz,local,training
303,runs/2022-10-13T06-50-37Z,695.0385,936.5186,241.4801,3.386,2.4702,NA,NA,128,32,32,0.1,64,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-50-37Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B058BCA0>,0.100000001,nn_exp.R,13/10/2022 6:50,13/10/2022 6:51,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-50-37Z/tfruns.d/source.tar.gz,local,training
248,runs/2022-10-13T07-55-55Z,701.7139,943.7407,242.0268,3.4022,2.4738,NA,NA,32,64,32,0.1,64,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-55-55Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0955970>,0.100000001,nn_exp.R,13/10/2022 7:55,13/10/2022 7:57,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-55-55Z/tfruns.d/source.tar.gz,local,training
335,runs/2022-10-13T06-03-55Z,747.5043,990.6082,243.1039,3.5804,2.6842,NA,NA,32,128,16,0.1,32,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-03-55Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8146B1490>,0.100000001,nn_exp.R,13/10/2022 6:03,13/10/2022 6:05,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-03-55Z/tfruns.d/source.tar.gz,local,training
381,runs/2022-10-13T04-04-53Z,1292.7147,1048.3816,244.3331,3.785,4.7555,NA,NA,128,64,16,0.1,64,32,NA,NA,NA,NA,NA,35,30,runs/2022-10-13T04-04-53Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B820440F70>,0.100000001,nn_exp.R,13/10/2022 4:04,13/10/2022 4:07,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-04-53Z/tfruns.d/source.tar.gz,local,training
184,runs/2022-10-13T09-03-35Z,753.1688,999.184,246.0152,3.6172,2.6848,NA,NA,64,64,64,0.1,128,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-03-35Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B841096520>,0.100000001,nn_exp.R,13/10/2022 9:03,13/10/2022 9:04,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-03-35Z/tfruns.d/source.tar.gz,local,training
346,runs/2022-10-13T05-17-12Z,757.1614,1003.3287,246.1673,3.6281,2.565,NA,NA,64,64,16,0.1,128,64,NA,NA,NA,NA,NA,35,29,runs/2022-10-13T05-17-12Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8149AF190>,0.100000001,nn_exp.R,13/10/2022 5:17,13/10/2022 5:18,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-17-12Z/tfruns.d/source.tar.gz,local,training
245,runs/2022-10-13T07-59-50Z,737.1013,983.3694,246.2681,3.5556,2.6156,NA,NA,32,128,32,0.1,64,128,NA,NA,NA,NA,NA,35,30,runs/2022-10-13T07-59-50Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B84109E040>,0.100000001,nn_exp.R,13/10/2022 7:59,13/10/2022 8:01,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-59-50Z/tfruns.d/source.tar.gz,local,training
242,runs/2022-10-13T08-03-59Z,720.6308,967.1148,246.484,3.5015,2.4772,NA,NA,32,32,32,0.1,128,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-03-59Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B04D9D60>,0.100000001,nn_exp.R,13/10/2022 8:04,13/10/2022 8:05,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-03-59Z/tfruns.d/source.tar.gz,local,training
259,runs/2022-10-13T07-40-32Z,709.8116,956.9755,247.1639,3.4466,2.5148,NA,NA,64,32,32,0.1,32,128,NA,NA,NA,NA,NA,35,28,runs/2022-10-13T07-40-32Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812412B50>,0.100000001,nn_exp.R,13/10/2022 7:40,13/10/2022 7:41,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-40-32Z/tfruns.d/source.tar.gz,local,training
237,runs/2022-10-13T08-13-00Z,712.2429,960.6792,248.4363,3.4765,2.446,NA,NA,128,64,32,0.1,128,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-13-00Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC2217F0>,0.100000001,nn_exp.R,13/10/2022 8:13,13/10/2022 8:14,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-13-00Z/tfruns.d/source.tar.gz,local,training
395,runs/2022-10-13T03-38-23Z,777.4327,1027.4783,250.0456,3.7113,2.8009,NA,NA,32,32,16,0.1,32,32,NA,NA,NA,NA,NA,35,27,runs/2022-10-13T03-38-23Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B04953D0>,0.100000001,nn_exp.R,13/10/2022 3:38,13/10/2022 3:39,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T03-38-23Z/tfruns.d/source.tar.gz,local,training
252,runs/2022-10-13T07-50-23Z,698.9678,949.5912,250.6234,3.4128,2.4638,NA,NA,128,128,32,0.1,32,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-50-23Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8148D9490>,0.100000001,nn_exp.R,13/10/2022 7:50,13/10/2022 7:52,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-50-23Z/tfruns.d/source.tar.gz,local,training
300,runs/2022-10-13T06-53-21Z,711.205,962.1656,250.9606,3.4804,2.5365,NA,NA,128,64,32,0.1,64,32,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T06-53-21Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0B56340>,0.100000001,nn_exp.R,13/10/2022 6:53,13/10/2022 6:54,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-53-21Z/tfruns.d/source.tar.gz,local,training
234,runs/2022-10-13T08-19-21Z,704.4305,955.9229,251.4924,3.4537,2.4136,NA,NA,128,128,32,0.1,128,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-19-21Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0ABAA30>,0.100000001,nn_exp.R,13/10/2022 8:19,13/10/2022 8:22,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-19-21Z/tfruns.d/source.tar.gz,local,training
338,runs/2022-10-13T05-59-21Z,751.6656,1004.4008,252.7352,3.6254,2.7004,NA,NA,32,64,16,0.1,32,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T05-59-21Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC158190>,0.100000001,nn_exp.R,13/10/2022 5:59,13/10/2022 6:01,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-59-21Z/tfruns.d/source.tar.gz,local,training
361,runs/2022-10-13T04-51-41Z,779.2065,1033.2399,254.0334,3.7347,2.6953,NA,NA,64,128,16,0.1,32,64,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T04-51-41Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8203A38B0>,0.100000001,nn_exp.R,13/10/2022 4:51,13/10/2022 4:53,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-51-41Z/tfruns.d/source.tar.gz,local,training
164,runs/2022-10-13T09-18-55Z,772.5775,1026.7312,254.1537,3.7145,2.6666,NA,NA,32,128,64,0.1,64,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-18-55Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B04DBE50>,0.100000001,nn_exp.R,13/10/2022 9:18,13/10/2022 9:19,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-18-55Z/tfruns.d/source.tar.gz,local,training
348,runs/2022-10-13T05-14-11Z,707.4478,962.5492,255.1014,3.4804,2.5137,NA,NA,128,32,16,0.1,128,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T05-14-11Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E1DB940>,0.100000001,nn_exp.R,13/10/2022 5:14,13/10/2022 5:15,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-14-11Z/tfruns.d/source.tar.gz,local,training
143,runs/2022-10-13T09-56-19Z,887.1378,1142.8221,255.6843,4.1426,3.0879,NA,NA,64,64,16,0.01,16,16,NA,NA,NA,NA,NA,35,5,runs/2022-10-13T09-56-19Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812412040>,0.01,nn_exp.R,13/10/2022 9:56,13/10/2022 9:56,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,runs/2022-10-13T09-56-19Z/tfruns.d/source.tar.gz,local,training
260,runs/2022-10-13T07-39-03Z,791.0208,1047.0531,256.0323,3.782,2.7046,NA,NA,32,32,32,0.1,32,128,NA,NA,NA,NA,NA,35,29,runs/2022-10-13T07-39-03Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B02D2B80>,0.100000001,nn_exp.R,13/10/2022 7:39,13/10/2022 7:40,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-39-03Z/tfruns.d/source.tar.gz,local,training
308,runs/2022-10-13T06-46-05Z,679.5828,936.4544,256.8716,3.3726,2.3152,NA,NA,32,128,32,0.1,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-46-05Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8148595B0>,0.100000001,nn_exp.R,13/10/2022 6:46,13/10/2022 6:47,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-46-05Z/tfruns.d/source.tar.gz,local,training
368,runs/2022-10-13T04-36-26Z,777.4027,1036.7533,259.3506,3.7484,2.6393,NA,NA,32,32,16,0.1,32,64,NA,NA,NA,NA,NA,35,25,runs/2022-10-13T04-36-26Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8134D9E50>,0.100000001,nn_exp.R,13/10/2022 4:36,13/10/2022 4:38,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-36-26Z/tfruns.d/source.tar.gz,local,training
179,runs/2022-10-13T09-08-46Z,725.1211,985.0038,259.8827,3.5678,2.5242,NA,NA,32,32,64,0.1,32,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-08-46Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8297F8D00>,0.100000001,nn_exp.R,13/10/2022 9:08,13/10/2022 9:09,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-08-46Z/tfruns.d/source.tar.gz,local,training
212,runs/2022-10-13T08-41-32Z,698.6015,960.0576,261.4561,3.4602,2.4572,NA,NA,32,64,64,0.1,128,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-41-32Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B825606E80>,0.100000001,nn_exp.R,13/10/2022 8:41,13/10/2022 8:42,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-41-32Z/tfruns.d/source.tar.gz,local,training
172,runs/2022-10-13T09-12-46Z,711.3318,974.0612,262.7294,3.5257,2.4587,NA,NA,64,128,64,0.1,32,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-12-46Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B82981EEB0>,0.100000001,nn_exp.R,13/10/2022 9:12,13/10/2022 9:13,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-12-46Z/tfruns.d/source.tar.gz,local,training
319,runs/2022-10-13T06-30-52Z,745.293,1010.4839,265.1909,3.6473,2.6743,NA,NA,64,64,16,0.1,128,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-30-52Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814A2A340>,0.100000001,nn_exp.R,13/10/2022 6:30,13/10/2022 6:32,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-30-52Z/tfruns.d/source.tar.gz,local,training
208,runs/2022-10-13T08-46-59Z,729.361,994.5977,265.2367,3.6077,2.534,NA,NA,64,128,64,0.1,128,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-46-59Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A8B0910>,0.100000001,nn_exp.R,13/10/2022 8:46,13/10/2022 8:48,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-46-59Z/tfruns.d/source.tar.gz,local,training
315,runs/2022-10-13T06-38-25Z,724.8901,991.1508,266.2607,3.5815,2.5818,NA,NA,128,128,16,0.1,128,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-38-25Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B04D9D00>,0.100000001,nn_exp.R,13/10/2022 6:38,13/10/2022 6:40,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-38-25Z/tfruns.d/source.tar.gz,local,training
254,runs/2022-10-13T07-47-31Z,722.7789,989.8367,267.0578,3.5828,2.5781,NA,NA,32,128,32,0.1,32,128,NA,NA,NA,NA,NA,35,34,runs/2022-10-13T07-47-31Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B825481E20>,0.100000001,nn_exp.R,13/10/2022 7:47,13/10/2022 7:48,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-47-31Z/tfruns.d/source.tar.gz,local,training
116,runs/2022-10-13T10-46-51Z,1022.8418,755.3333,267.5085,2.7204,3.4282,NA,NA,64,128,32,0.01,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-46-51Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81484F9D0>,0.01,nn_exp.R,13/10/2022 10:46,13/10/2022 10:47,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-46-51Z/tfruns.d/source.tar.gz,local,training
246,runs/2022-10-13T07-58-29Z,711.5007,979.9584,268.4577,3.5426,2.453,NA,NA,128,64,32,0.1,64,128,NA,NA,NA,NA,NA,35,34,runs/2022-10-13T07-58-29Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FB0DE80>,0.100000001,nn_exp.R,13/10/2022 7:58,13/10/2022 7:59,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-58-29Z/tfruns.d/source.tar.gz,local,training
291,runs/2022-10-13T07-00-59Z,696.3672,966.3555,269.9883,3.4943,2.4293,NA,NA,128,64,32,0.1,128,32,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T07-00-59Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BD129A0>,0.100000001,nn_exp.R,13/10/2022 7:00,13/10/2022 7:01,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-00-59Z/tfruns.d/source.tar.gz,local,training
187,runs/2022-10-13T09-01-49Z,805.2579,1078.4153,273.1574,3.8936,2.761,NA,NA,64,32,64,0.1,128,64,NA,NA,NA,NA,NA,35,21,runs/2022-10-13T09-01-49Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81493F700>,0.100000001,nn_exp.R,13/10/2022 9:01,13/10/2022 9:02,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-01-49Z/tfruns.d/source.tar.gz,local,training
314,runs/2022-10-13T06-40-41Z,753.6954,1027.4912,273.7958,3.7164,2.699,NA,NA,32,32,32,0.1,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-40-41Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829A8F760>,0.100000001,nn_exp.R,13/10/2022 6:40,13/10/2022 6:41,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-40-41Z/tfruns.d/source.tar.gz,local,training
294,runs/2022-10-13T06-58-19Z,718.2172,992.67,274.4528,3.5926,2.5631,NA,NA,128,32,32,0.1,128,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-58-19Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCB23070>,0.100000001,nn_exp.R,13/10/2022 6:58,13/10/2022 6:59,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-58-19Z/tfruns.d/source.tar.gz,local,training
330,runs/2022-10-13T06-12-09Z,725.1715,999.7997,274.6282,3.6117,2.5893,NA,NA,128,32,16,0.1,64,128,NA,NA,NA,NA,NA,35,33,runs/2022-10-13T06-12-09Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E05BAC0>,0.100000001,nn_exp.R,13/10/2022 6:12,13/10/2022 6:13,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-12-09Z/tfruns.d/source.tar.gz,local,training
296,runs/2022-10-13T06-56-47Z,699.9622,979.5118,279.5496,3.5459,2.4881,NA,NA,32,32,32,0.1,128,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-56-47Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8134D5DC0>,0.100000001,nn_exp.R,13/10/2022 6:56,13/10/2022 6:57,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-56-47Z/tfruns.d/source.tar.gz,local,training
316,runs/2022-10-13T06-36-34Z,741.0732,1021.6611,280.5879,3.6923,2.5239,NA,NA,64,128,16,0.1,128,128,NA,NA,NA,NA,NA,35,30,runs/2022-10-13T06-36-34Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC02FB20>,0.100000001,nn_exp.R,13/10/2022 6:36,13/10/2022 6:38,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-36-34Z/tfruns.d/source.tar.gz,local,training
263,runs/2022-10-13T07-31-42Z,1241.4045,955.6705,285.734,3.4615,4.1797,NA,NA,32,128,32,0.1,128,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-31-42Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B035A3A0>,0.100000001,nn_exp.R,13/10/2022 7:31,13/10/2022 7:33,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-31-42Z/tfruns.d/source.tar.gz,local,training
230,runs/2022-10-13T08-24-31Z,701.6896,988.3389,286.6493,3.5774,2.4607,NA,NA,32,64,64,0.1,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-24-31Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8299959A0>,0.100000001,nn_exp.R,13/10/2022 8:24,13/10/2022 8:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-24-31Z/tfruns.d/source.tar.gz,local,training
385,runs/2022-10-13T03-55-57Z,699.1981,985.9274,286.7293,3.5644,2.4545,NA,NA,64,32,16,0.1,64,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T03-55-57Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8123DB8E0>,0.100000001,nn_exp.R,13/10/2022 3:55,13/10/2022 3:58,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T03-55-57Z/tfruns.d/source.tar.gz,local,training
279,runs/2022-10-13T07-11-49Z,694.9576,983.6371,288.6795,3.5607,2.415,NA,NA,128,128,32,0.1,32,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-11-49Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC2BC7F0>,0.100000001,nn_exp.R,13/10/2022 7:11,13/10/2022 7:12,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-11-49Z/tfruns.d/source.tar.gz,local,training
142,runs/2022-10-13T10-09-16Z,1037.4772,748.1241,289.3531,2.6809,3.4378,NA,NA,64,64,32,0.01,16,16,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-09-16Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC123E80>,0.01,nn_exp.R,13/10/2022 10:09,13/10/2022 10:10,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-09-16Z/tfruns.d/source.tar.gz,local,training
375,runs/2022-10-13T04-18-38Z,694.9407,985.7605,290.8198,3.5635,2.4116,NA,NA,128,32,16,0.1,128,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T04-18-38Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B04D9880>,0.100000001,nn_exp.R,13/10/2022 4:18,13/10/2022 4:21,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-18-38Z/tfruns.d/source.tar.gz,local,training
221,runs/2022-10-13T08-33-04Z,743.1846,1035.0873,291.9027,3.7419,2.6332,NA,NA,32,64,64,0.1,64,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-33-04Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81483D1F0>,0.100000001,nn_exp.R,13/10/2022 8:33,13/10/2022 8:33,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-33-04Z/tfruns.d/source.tar.gz,local,training
217,runs/2022-10-13T08-36-30Z,728.0418,1020.9727,292.9309,3.6973,2.5775,NA,NA,64,128,64,0.1,64,32,NA,NA,NA,NA,NA,35,34,runs/2022-10-13T08-36-30Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8297A0AC0>,0.100000001,nn_exp.R,13/10/2022 8:36,13/10/2022 8:37,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-36-30Z/tfruns.d/source.tar.gz,local,training
159,runs/2022-10-13T09-23-57Z,825.8978,1119.5721,293.6743,4.0515,2.8224,NA,NA,128,32,64,0.1,128,128,NA,NA,NA,NA,NA,35,25,runs/2022-10-13T09-23-57Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B84107B220>,0.100000001,nn_exp.R,13/10/2022 9:23,13/10/2022 9:24,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-23-57Z/tfruns.d/source.tar.gz,local,training
185,runs/2022-10-13T09-02-54Z,740.509,1036.2501,295.7411,3.745,2.6578,NA,NA,32,64,64,0.1,128,64,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T09-02-54Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC32DD00>,0.100000001,nn_exp.R,13/10/2022 9:02,13/10/2022 9:03,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-02-54Z/tfruns.d/source.tar.gz,local,training
272,runs/2022-10-13T07-18-38Z,757.5097,1056.0367,298.527,3.8263,2.718,NA,NA,32,128,32,0.1,64,64,NA,NA,NA,NA,NA,35,23,runs/2022-10-13T07-18-38Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0B6D970>,0.100000001,nn_exp.R,13/10/2022 7:18,13/10/2022 7:19,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-18-38Z/tfruns.d/source.tar.gz,local,training
341,runs/2022-10-13T05-54-21Z,680.8427,979.9597,299.117,3.54,2.3742,NA,NA,32,32,16,0.1,32,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T05-54-21Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A911940>,0.100000001,nn_exp.R,13/10/2022 5:54,13/10/2022 5:56,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-54-21Z/tfruns.d/source.tar.gz,local,training
352,runs/2022-10-13T05-08-43Z,738.9039,1040.745,301.8411,3.7605,2.6329,NA,NA,64,128,16,0.1,64,64,NA,NA,NA,NA,NA,35,27,runs/2022-10-13T05-08-43Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B08DC280>,0.100000001,nn_exp.R,13/10/2022 5:08,13/10/2022 5:09,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-08-43Z/tfruns.d/source.tar.gz,local,training
364,runs/2022-10-13T04-45-19Z,696.407,998.7519,302.3449,3.6099,2.4422,NA,NA,64,64,16,0.1,32,64,NA,NA,NA,NA,NA,35,29,runs/2022-10-13T04-45-19Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814A287F0>,0.100000001,nn_exp.R,13/10/2022 4:45,13/10/2022 4:47,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-45-19Z/tfruns.d/source.tar.gz,local,training
197,runs/2022-10-13T08-55-03Z,722.2324,1024.8286,302.5962,3.7084,2.5448,NA,NA,32,32,64,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-55-03Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FB0D490>,0.100000001,nn_exp.R,13/10/2022 8:55,13/10/2022 8:55,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-55-03Z/tfruns.d/source.tar.gz,local,training
298,runs/2022-10-13T06-54-51Z,695.1966,998.3041,303.1075,3.6126,2.42,NA,NA,64,128,32,0.1,64,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-54-51Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC14DEE0>,0.100000001,nn_exp.R,13/10/2022 6:54,13/10/2022 6:55,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-54-51Z/tfruns.d/source.tar.gz,local,training
358,runs/2022-10-13T04-57-27Z,723.619,1028.9165,305.2975,3.7139,2.5729,NA,NA,64,32,16,0.1,64,64,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T04-57-27Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC1772E0>,0.100000001,nn_exp.R,13/10/2022 4:57,13/10/2022 4:59,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-57-27Z/tfruns.d/source.tar.gz,local,training
326,runs/2022-10-13T06-18-25Z,724.3434,1030.2456,305.9022,3.7152,2.5853,NA,NA,32,128,16,0.1,64,128,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T06-18-25Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FAE9EB0>,0.100000001,nn_exp.R,13/10/2022 6:18,13/10/2022 6:19,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-18-25Z/tfruns.d/source.tar.gz,local,training
209,runs/2022-10-13T08-45-55Z,751.6548,1058.0009,306.3461,3.8082,2.6347,NA,NA,32,128,64,0.1,128,32,NA,NA,NA,NA,NA,35,29,runs/2022-10-13T08-45-55Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCBF8EE0>,0.100000001,nn_exp.R,13/10/2022 8:45,13/10/2022 8:46,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-45-55Z/tfruns.d/source.tar.gz,local,training
336,runs/2022-10-13T06-02-36Z,698.1401,1007.8236,309.6835,3.6452,2.4406,NA,NA,128,64,16,0.1,32,128,NA,NA,NA,NA,NA,35,26,runs/2022-10-13T06-02-36Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC01EE50>,0.100000001,nn_exp.R,13/10/2022 6:02,13/10/2022 6:03,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-02-36Z/tfruns.d/source.tar.gz,local,training
318,runs/2022-10-13T06-32-49Z,714.5499,1024.7784,310.2285,3.712,2.5385,NA,NA,128,64,16,0.1,128,128,NA,NA,NA,NA,NA,35,27,runs/2022-10-13T06-32-49Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E3D9E50>,0.100000001,nn_exp.R,13/10/2022 6:32,13/10/2022 6:34,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-32-49Z/tfruns.d/source.tar.gz,local,training
249,runs/2022-10-13T07-54-34Z,1252.3851,941.1305,311.2546,3.3859,4.1422,NA,NA,128,32,32,0.1,64,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-54-34Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC219310>,0.100000001,nn_exp.R,13/10/2022 7:54,13/10/2022 7:55,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-54-34Z/tfruns.d/source.tar.gz,local,training
370,runs/2022-10-13T04-31-10Z,722.5732,1034.9698,312.3966,3.7422,2.579,NA,NA,64,128,16,0.1,128,32,NA,NA,NA,NA,NA,35,23,runs/2022-10-13T04-31-10Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E258850>,0.100000001,nn_exp.R,13/10/2022 4:31,13/10/2022 4:33,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-31-10Z/tfruns.d/source.tar.gz,local,training
392,runs/2022-10-13T03-42-56Z,710.9014,1024.4674,313.566,3.6993,2.5175,NA,NA,32,64,16,0.1,32,32,NA,NA,NA,NA,NA,35,28,runs/2022-10-13T03-42-56Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8121C5FD0>,0.100000001,nn_exp.R,13/10/2022 3:42,13/10/2022 3:44,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T03-42-56Z/tfruns.d/source.tar.gz,local,training
332,runs/2022-10-13T06-09-10Z,709.8505,1024.3672,314.5167,3.6927,2.45,NA,NA,32,32,16,0.1,64,128,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T06-09-10Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0314BB0>,0.100000001,nn_exp.R,13/10/2022 6:09,13/10/2022 6:10,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-09-10Z/tfruns.d/source.tar.gz,local,training
265,runs/2022-10-13T07-28-22Z,716.7641,1031.2864,314.5223,3.7292,2.4742,NA,NA,64,64,32,0.1,128,64,NA,NA,NA,NA,NA,35,29,runs/2022-10-13T07-28-22Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8147C5A90>,0.100000001,nn_exp.R,13/10/2022 7:28,13/10/2022 7:29,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-28-22Z/tfruns.d/source.tar.gz,local,training
122,runs/2022-10-13T10-40-32Z,1047.7935,729.193,318.6005,2.618,3.5045,NA,NA,64,64,32,0.01,32,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T10-40-32Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0B0C1C0>,0.01,nn_exp.R,13/10/2022 10:40,13/10/2022 10:41,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T10-40-32Z/tfruns.d/source.tar.gz,local,training
367,runs/2022-10-13T04-38-13Z,715.309,1035.0679,319.7589,3.7342,2.5571,NA,NA,64,32,16,0.1,32,64,NA,NA,NA,NA,NA,35,30,runs/2022-10-13T04-38-13Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E446FA0>,0.100000001,nn_exp.R,13/10/2022 4:38,13/10/2022 4:40,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-38-13Z/tfruns.d/source.tar.gz,local,training
269,runs/2022-10-13T07-22-57Z,731.1362,1052.4835,321.3473,3.8115,2.5527,NA,NA,32,32,32,0.1,128,64,NA,NA,NA,NA,NA,35,22,runs/2022-10-13T07-22-57Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829973850>,0.100000001,nn_exp.R,13/10/2022 7:22,13/10/2022 7:23,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-22-57Z/tfruns.d/source.tar.gz,local,training
366,runs/2022-10-13T04-40-17Z,685.2194,1007.7404,322.521,3.6382,2.3868,NA,NA,128,32,16,0.1,32,64,NA,NA,NA,NA,NA,35,33,runs/2022-10-13T04-40-17Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E240AC0>,0.100000001,nn_exp.R,13/10/2022 4:40,13/10/2022 4:43,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-40-17Z/tfruns.d/source.tar.gz,local,training
156,runs/2022-10-13T09-26-43Z,1342.7174,1012.478,330.2394,3.6691,4.4759,NA,NA,128,64,64,0.1,128,128,NA,NA,NA,NA,NA,35,33,runs/2022-10-13T09-26-43Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A8DC490>,0.100000001,nn_exp.R,13/10/2022 9:26,13/10/2022 9:27,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-26-43Z/tfruns.d/source.tar.gz,local,training
376,runs/2022-10-13T04-16-57Z,715.7474,1046.884,331.1366,3.7821,2.4927,NA,NA,64,32,16,0.1,128,32,NA,NA,NA,NA,NA,35,24,runs/2022-10-13T04-16-57Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC2196D0>,0.100000001,nn_exp.R,13/10/2022 4:16,13/10/2022 4:18,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-16-57Z/tfruns.d/source.tar.gz,local,training
320,runs/2022-10-13T06-28-57Z,1320.6223,989.0985,331.5238,3.571,4.4225,NA,NA,32,64,16,0.1,128,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-28-57Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A941D90>,0.100000001,nn_exp.R,13/10/2022 6:28,13/10/2022 6:30,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-28-57Z/tfruns.d/source.tar.gz,local,training
199,runs/2022-10-13T08-53-24Z,1356.2473,1021.6733,334.574,3.7095,4.5769,NA,NA,64,128,64,0.1,32,64,NA,NA,NA,NA,NA,35,27,runs/2022-10-13T08-53-24Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A8DCF70>,0.100000001,nn_exp.R,13/10/2022 8:53,13/10/2022 8:53,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-53-24Z/tfruns.d/source.tar.gz,local,training
271,runs/2022-10-13T07-19-32Z,1289.6759,952.5585,337.1174,3.4476,4.2748,NA,NA,64,128,32,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-19-32Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0B6DF10>,0.100000001,nn_exp.R,13/10/2022 7:19,13/10/2022 7:20,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-19-32Z/tfruns.d/source.tar.gz,local,training
214,runs/2022-10-13T08-39-49Z,864.3041,1203.8066,339.5025,4.3694,3.0986,NA,NA,64,32,64,0.1,128,32,NA,NA,NA,NA,NA,35,17,runs/2022-10-13T08-39-49Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8149BE910>,0.100000001,nn_exp.R,13/10/2022 8:39,13/10/2022 8:40,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-39-49Z/tfruns.d/source.tar.gz,local,training
365,runs/2022-10-13T04-43-15Z,704.5776,1047.1064,342.5288,3.785,2.4842,NA,NA,32,64,16,0.1,32,64,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T04-43-15Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FA41460>,0.100000001,nn_exp.R,13/10/2022 4:43,13/10/2022 4:45,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-43-15Z/tfruns.d/source.tar.gz,local,training
266,runs/2022-10-13T07-26-54Z,1312.8118,968.2813,344.5305,3.499,4.784,NA,NA,32,64,32,0.1,128,64,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T07-26-54Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BD1D2E0>,0.100000001,nn_exp.R,13/10/2022 7:26,13/10/2022 7:28,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-26-54Z/tfruns.d/source.tar.gz,local,training
372,runs/2022-10-13T04-25-38Z,681.5782,1029.4475,347.8693,3.7151,2.3864,NA,NA,128,64,16,0.1,128,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T04-25-38Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E0DFBE0>,0.100000001,nn_exp.R,13/10/2022 4:25,13/10/2022 4:28,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-25-38Z/tfruns.d/source.tar.gz,local,training
270,runs/2022-10-13T07-20-57Z,1330.6743,978.2769,352.3974,3.542,4.5165,NA,NA,128,128,32,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-20-57Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BDD91F0>,0.100000001,nn_exp.R,13/10/2022 7:20,13/10/2022 7:22,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-20-57Z/tfruns.d/source.tar.gz,local,training
403,runs/2022-10-13T01-49-47Z,838.0759,1193.4175,355.3416,NA,NA,NA,NA,32,16,1,0.1,NA,NA,0,0,NA,NA,10,40,31,runs/2022-10-13T01-49-47Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000002E5B1D0F310>,0.100000001,nn_exp.R,13/10/2022 1:49,13/10/2022 2:07,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_numeric(""dr ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   layer_dropout(rate = FLAGS$dropout1)  .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,runs/2022-10-13T01-49-47Z/tfruns.d/source.tar.gz,local,training
241,runs/2022-10-13T08-05-49Z,1267.1277,904.7509,362.3768,3.2594,4.5219,NA,NA,64,32,32,0.1,128,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-05-49Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC1234C0>,0.100000001,nn_exp.R,13/10/2022 8:05,13/10/2022 8:07,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-05-49Z/tfruns.d/source.tar.gz,local,training
278,runs/2022-10-13T07-12-54Z,1368.8341,1006.2159,362.6182,3.6392,4.5593,NA,NA,32,32,32,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-12-54Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E42BB50>,0.100000001,nn_exp.R,13/10/2022 7:12,13/10/2022 7:13,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-12-54Z/tfruns.d/source.tar.gz,local,training
66,runs/2022-10-13T12-17-24Z,1129.6602,760.5561,369.1041,2.7245,3.7895,NA,NA,128,64,32,0.01,32,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T12-17-24Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B05669D0>,0.01,nn_exp.R,13/10/2022 12:17,13/10/2022 12:18,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T12-17-24Z/tfruns.d/source.tar.gz,local,training
396,runs/2022-10-13T03-37-46Z,1362.0369,1743.424,381.3871,6.317,5.0298,NA,NA,32,32,16,0.1,32,32,NA,NA,NA,NA,NA,35,2,runs/2022-10-13T03-37-46Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FB12FA0>,0.100000001,nn_exp.R,13/10/2022 3:37,13/10/2022 3:37,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,runs/2022-10-13T03-37-46Z/tfruns.d/source.tar.gz,local,training
404,runs/2022-10-13T01-29-55Z,779.8922,1171.0027,391.1105,NA,NA,NA,NA,16,16,1,0.1,NA,NA,0,0,NA,NA,10,40,40,runs/2022-10-13T01-29-55Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000002E5BE70CAC0>,0.100000001,nn_exp.R,13/10/2022 1:29,13/10/2022 1:49,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_numeric(""dr ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   layer_dropout(rate = FLAGS$dropout1)  .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T01-29-55Z/tfruns.d/source.tar.gz,local,training
264,runs/2022-10-13T07-29-49Z,1367.4835,974.9393,392.5442,3.5252,4.975,NA,NA,128,64,32,0.1,128,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-29-49Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8299E7760>,0.100000001,nn_exp.R,13/10/2022 7:29,13/10/2022 7:31,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-29-49Z/tfruns.d/source.tar.gz,local,training
167,runs/2022-10-13T09-16-25Z,1421.8154,1026.1096,395.7058,3.7155,4.7814,NA,NA,32,64,64,0.1,64,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-16-25Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8146E8130>,0.100000001,nn_exp.R,13/10/2022 9:16,13/10/2022 9:17,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-16-25Z/tfruns.d/source.tar.gz,local,training
183,runs/2022-10-13T09-04-20Z,1374.95,973.8774,401.0726,3.5353,4.6606,NA,NA,128,64,64,0.1,128,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-04-20Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B08F5610>,0.100000001,nn_exp.R,13/10/2022 9:04,13/10/2022 9:05,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-04-20Z/tfruns.d/source.tar.gz,local,training
210,runs/2022-10-13T08-44-25Z,1378.679,970.457,408.222,3.5218,4.9808,NA,NA,128,64,64,0.1,128,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-44-25Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8297CD640>,0.100000001,nn_exp.R,13/10/2022 8:44,13/10/2022 8:45,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-44-25Z/tfruns.d/source.tar.gz,local,training
190,runs/2022-10-13T08-59-19Z,1315.655,904.1025,411.5525,3.2566,4.3752,NA,NA,64,128,64,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-59-19Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BE1EBE0>,0.100000001,nn_exp.R,13/10/2022 8:59,13/10/2022 9:00,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-59-19Z/tfruns.d/source.tar.gz,local,training
356,runs/2022-10-13T05-01-14Z,1430.3751,1004.9393,425.4358,3.6308,4.8559,NA,NA,32,64,16,0.1,64,64,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T05-01-14Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E39E2E0>,0.100000001,nn_exp.R,13/10/2022 5:01,13/10/2022 5:03,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-01-14Z/tfruns.d/source.tar.gz,local,training
355,runs/2022-10-13T05-03-14Z,1452.6279,999.3696,453.2583,3.6111,4.8942,NA,NA,64,64,16,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T05-03-14Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E0A1460>,0.100000001,nn_exp.R,13/10/2022 5:03,13/10/2022 5:05,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-03-14Z/tfruns.d/source.tar.gz,local,training
206,runs/2022-10-13T08-49-25Z,1431.9827,977.4875,454.4952,3.5234,5.2371,NA,NA,32,32,64,0.1,32,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-49-25Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BBFDA580>,0.100000001,nn_exp.R,13/10/2022 8:49,13/10/2022 8:49,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-49-25Z/tfruns.d/source.tar.gz,local,training
175,runs/2022-10-13T09-10-59Z,1504.309,1047.8455,456.4635,3.7719,4.8236,NA,NA,64,64,64,0.1,32,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-10-59Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812424BE0>,0.100000001,nn_exp.R,13/10/2022 9:10,13/10/2022 9:11,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-10-59Z/tfruns.d/source.tar.gz,local,training
273,runs/2022-10-13T07-17-25Z,1482.9464,989.8863,493.0601,3.5784,5.4365,NA,NA,128,64,32,0.1,64,64,NA,NA,NA,NA,NA,35,30,runs/2022-10-13T07-17-25Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BD59490>,0.100000001,nn_exp.R,13/10/2022 7:17,13/10/2022 7:18,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-17-25Z/tfruns.d/source.tar.gz,local,training
373,runs/2022-10-13T04-23-39Z,1550.5358,1022.5385,527.9973,3.7011,5.243,NA,NA,64,64,16,0.1,128,32,NA,NA,NA,NA,NA,35,24,runs/2022-10-13T04-23-39Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E4A4F70>,0.100000001,nn_exp.R,13/10/2022 4:23,13/10/2022 4:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-23-39Z/tfruns.d/source.tar.gz,local,training
393,runs/2022-10-13T03-41-32Z,1554.1295,993.4129,560.7166,3.5866,5.6699,NA,NA,128,32,16,0.1,32,32,NA,NA,NA,NA,NA,35,28,runs/2022-10-13T03-41-32Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FB0D8E0>,0.100000001,nn_exp.R,13/10/2022 3:41,13/10/2022 3:42,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T03-41-32Z/tfruns.d/source.tar.gz,local,training
359,runs/2022-10-13T04-55-35Z,1567.2192,993.4254,573.7938,3.583,5.2927,NA,NA,32,32,16,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T04-55-35Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC274EB0>,0.100000001,nn_exp.R,13/10/2022 4:55,13/10/2022 4:57,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-55-35Z/tfruns.d/source.tar.gz,local,training
153,runs/2022-10-13T09-30-48Z,1594.468,1017.7444,576.7236,3.6863,5.3458,NA,NA,128,128,64,0.1,128,128,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T09-30-48Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B825444670>,0.100000001,nn_exp.R,13/10/2022 9:30,13/10/2022 9:32,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-30-48Z/tfruns.d/source.tar.gz,local,training
274,runs/2022-10-13T07-16-28Z,1547.9032,969.1349,578.7683,3.4915,5.2861,NA,NA,64,64,32,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-16-28Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B82558E610>,0.100000001,nn_exp.R,13/10/2022 7:16,13/10/2022 7:17,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-16-28Z/tfruns.d/source.tar.gz,local,training
158,runs/2022-10-13T09-24-46Z,777.18,1360.0151,582.8351,4.9112,2.6753,NA,NA,32,64,64,0.1,128,128,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T09-24-46Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B820393640>,0.100000001,nn_exp.R,13/10/2022 9:24,13/10/2022 9:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-24-46Z/tfruns.d/source.tar.gz,local,training
169,runs/2022-10-13T09-15-15Z,1596.2595,1003.9138,592.3457,3.6176,5.3429,NA,NA,64,32,64,0.1,64,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-15-15Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E103610>,0.100000001,nn_exp.R,13/10/2022 9:15,13/10/2022 9:15,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-15-15Z/tfruns.d/source.tar.gz,local,training
268,runs/2022-10-13T07-23-48Z,1614.6024,987.4085,627.1939,3.5788,5.8964,NA,NA,64,32,32,0.1,128,64,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T07-23-48Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BCB75B0>,0.100000001,nn_exp.R,13/10/2022 7:23,13/10/2022 7:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-23-48Z/tfruns.d/source.tar.gz,local,training
211,runs/2022-10-13T08-42-56Z,1663.2373,1005.2368,658.0005,3.6402,5.6057,NA,NA,64,64,64,0.1,128,32,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T08-42-56Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC2F99A0>,0.100000001,nn_exp.R,13/10/2022 8:42,13/10/2022 8:44,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-42-56Z/tfruns.d/source.tar.gz,local,training
29,runs/2022-10-13T14-32-08Z,1545.1498,817.2949,727.8549,2.9395,5.0628,NA,NA,64,64,32,0.005,64,NA,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T14-32-08Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8298CE760>,0.005,nn_exp.R,13/10/2022 14:32,13/10/2022 14:33,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T14-32-08Z/tfruns.d/source.tar.gz,local,training
380,runs/2022-10-13T04-07-05Z,1754.396,999.0527,755.3433,3.6059,5.8939,NA,NA,32,128,16,0.1,64,32,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T04-07-05Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829A6C8E0>,0.100000001,nn_exp.R,13/10/2022 4:07,13/10/2022 4:09,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-07-05Z/tfruns.d/source.tar.gz,local,training
162,runs/2022-10-13T09-20-42Z,1771.552,966.5164,805.0356,3.5097,5.9508,NA,NA,128,128,64,0.1,64,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-20-42Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BE0B3A0>,0.100000001,nn_exp.R,13/10/2022 9:20,13/10/2022 9:21,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-20-42Z/tfruns.d/source.tar.gz,local,training
280,runs/2022-10-13T07-10-57Z,1812.2209,981.0834,831.1375,3.5505,6.119,NA,NA,64,128,32,0.1,32,64,NA,NA,NA,NA,NA,35,32,runs/2022-10-13T07-10-57Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E3BD190>,0.100000001,nn_exp.R,13/10/2022 7:10,13/10/2022 7:11,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-10-57Z/tfruns.d/source.tar.gz,local,training
277,runs/2022-10-13T07-13-46Z,1747.6436,913.9027,833.7409,3.2908,5.7958,NA,NA,64,32,32,0.1,64,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T07-13-46Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCB71CD0>,0.100000001,nn_exp.R,13/10/2022 7:13,13/10/2022 7:14,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T07-13-46Z/tfruns.d/source.tar.gz,local,training
186,runs/2022-10-13T09-02-14Z,2088.874,982.4146,1106.4594,3.533,7.6934,NA,NA,128,32,64,0.1,128,64,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T09-02-14Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B825621BE0>,0.100000001,nn_exp.R,13/10/2022 9:02,13/10/2022 9:02,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T09-02-14Z/tfruns.d/source.tar.gz,local,training
31,runs/2022-10-13T14-20-16Z,1645.897,2794.9514,1149.0544,10.188,6.1598,NA,NA,64,64,32,0.005,64,NA,NA,NA,NA,NA,NA,35,1,runs/2022-10-13T14-20-16Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B84111A5E0>,0.005,nn_exp.R,13/10/2022 14:20,13/10/2022 14:20,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,runs/2022-10-13T14-20-16Z/tfruns.d/source.tar.gz,local,training
398,runs/2022-10-13T02-57-46Z,2532.3203,1371.5194,1160.8009,4.9522,9.1258,1370.8242,2531.6221,32,32,1,0.1,NA,NA,0,0,NA,NA,10,35,9,runs/2022-10-13T02-57-46Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7A1A1E850>,0.100000001,nn_exp.R,13/10/2022 2:57,13/10/2022 3:04,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_numeric(""dr ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   layer_dropout(rate = FLAGS$dropout1)  .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,runs/2022-10-13T02-57-46Z/tfruns.d/source.tar.gz,local,training
354,runs/2022-10-13T05-05-26Z,2248.8782,1009.3513,1239.5269,3.6474,8.1096,NA,NA,128,64,16,0.1,64,64,NA,NA,NA,NA,NA,35,33,runs/2022-10-13T05-05-26Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A86A310>,0.100000001,nn_exp.R,13/10/2022 5:05,13/10/2022 5:07,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T05-05-26Z/tfruns.d/source.tar.gz,local,training
360,runs/2022-10-13T04-53-34Z,2253.2153,1011.3956,1241.8197,3.6503,7.6224,NA,NA,128,128,16,0.1,32,64,NA,NA,NA,NA,NA,35,29,runs/2022-10-13T04-53-34Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FA8DD90>,0.100000001,nn_exp.R,13/10/2022 4:53,13/10/2022 4:55,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T04-53-34Z/tfruns.d/source.tar.gz,local,training
223,runs/2022-10-13T08-31-39Z,2770.2532,976.1826,1794.0706,3.5127,9.9967,NA,NA,64,32,64,0.1,64,32,NA,NA,NA,NA,NA,35,31,runs/2022-10-13T08-31-39Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0AA1DF0>,0.100000001,nn_exp.R,13/10/2022 8:31,13/10/2022 8:32,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T08-31-39Z/tfruns.d/source.tar.gz,local,training
317,runs/2022-10-13T06-34-24Z,2866.5496,999.6578,1866.8918,3.6156,10.2334,NA,NA,32,128,16,0.1,128,128,NA,NA,NA,NA,NA,35,35,runs/2022-10-13T06-34-24Z/tfruns.d/metrics.json,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC0E2F10>,0.100000001,nn_exp.R,13/10/2022 6:34,13/10/2022 6:36,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,runs/2022-10-13T06-34-24Z/tfruns.d/source.tar.gz,local,training
33,runs/2022-10-13T13-43-19Z,NA,NA,#VALUE!,NA,NA,NA,NA,64,128,32,0.001,64,NA,NA,NA,NA,NA,NA,35,0,NA,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BC644C0>,0.001,nn_exp.R,13/10/2022 13:43,13/10/2022 13:43,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,runs/2022-10-13T13-43-19Z/tfruns.d/source.tar.gz,local,training
399,runs/2022-10-13T02-55-46Z,NA,NA,#VALUE!,NA,NA,NA,NA,32,32,1,0.1,NA,NA,0,0,NA,NA,10,35,0,NA,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001D52DD9F940>,0.100000001,nn_exp.R,13/10/2022 2:55,13/10/2022 2:55,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_numeric(""dr ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   layer_dropout(rate = FLAGS$dropout1)  .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ","RuntimeError: reentrant call inside <_io.BufferedWriter name='<stdout>'>
","py_call_impl(callable, dots$args, dots$keywords)
(structure(function (...) 
{
    dots <- py_resolve_dots(list(...))
    result <- py_call_impl(callable, dots$args, dots$keywords)
    if (convert) 
        result <- py_to_r(result)
    if (is.null(result)) 
        invisible(result)
    else result
}, class = c(""python.builtin.method"", ""python.builtin.object""
), py_object = <environment>))(batch_size = 1L, epochs = 35L, 
    verbose = ""auto"", validation_split = 0.2, shuffle = TRUE, 
    class_weight = NULL, sample_weight = NULL, initial_epoch = 0L, 
    x = <environment>, y = <environment>, callbacks = list(<environment>, 
        <environment>))
do.call(object$fit, args)
fit.keras.engine.training.Model(., x = as.matrix(X_train), y = as.matrix(y_train), 
    validation_split = 0.2, batch_size = FLAGS$batch_size, epochs = 35)
fit(., x = as.matrix(X_train), y = as.matrix(y_train), validation_split = 0.2, 
    batch_size = FLAGS$batch_size, epochs = 35)
model %>% fit(x = as.matrix(X_train), y = as.matrix(y_train), 
    validation_split = 0.2, batch_size = FLAGS$batch_size, epochs = 35)
eval(ei, envir)
eval(ei, envir)
withVisible(eval(ei, envir))
tuning_run(""nn_exp.R"", flags = list(dense_units1 = c(32, 64, 
    128), dense_units2 = c(32, 64, 128), dropout1 = c(0, 0.25, 
    0.5), dropout2 = c(0, 0.25, 0.5), batch_size = c(1, 16, 32), 
    learning_rate = c(0.1, 0.01, 0.001, 1e-04)))",runs/2022-10-13T02-55-46Z/tfruns.d/source.tar.gz,local,training
400,runs/2022-10-13T02-54-40Z,NA,NA,#VALUE!,NA,NA,NA,NA,32,32,1,0.1,NA,NA,0,0,NA,NA,10,35,0,NA,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001D52DDE1A60>,0.100000001,nn_exp.R,13/10/2022 2:54,13/10/2022 2:54,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_numeric(""dr ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   layer_dropout(rate = FLAGS$dropout1)  .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ","RuntimeError: reentrant call inside <_io.BufferedWriter name='<stdout>'>
","py_call_impl(callable, dots$args, dots$keywords)
(structure(function (...) 
{
    dots <- py_resolve_dots(list(...))
    result <- py_call_impl(callable, dots$args, dots$keywords)
    if (convert) 
        result <- py_to_r(result)
    if (is.null(result)) 
        invisible(result)
    else result
}, class = c(""python.builtin.method"", ""python.builtin.object""
), py_object = <environment>))(batch_size = 1L, epochs = 35L, 
    verbose = ""auto"", validation_split = 0.2, shuffle = TRUE, 
    class_weight = NULL, sample_weight = NULL, initial_epoch = 0L, 
    x = <environment>, y = <environment>, callbacks = list(<environment>, 
        <environment>))
do.call(object$fit, args)
fit.keras.engine.training.Model(., x = as.matrix(X_train), y = as.matrix(y_train), 
    validation_split = 0.2, batch_size = FLAGS$batch_size, epochs = 35)
fit(., x = as.matrix(X_train), y = as.matrix(y_train), validation_split = 0.2, 
    batch_size = FLAGS$batch_size, epochs = 35)
model %>% fit(x = as.matrix(X_train), y = as.matrix(y_train), 
    validation_split = 0.2, batch_size = FLAGS$batch_size, epochs = 35)
eval(ei, envir)
eval(ei, envir)
withVisible(eval(ei, envir))
tuning_run(""nn_exp.R"", flags = list(dense_units1 = c(32, 64, 
    128), dense_units2 = c(32, 64, 128), dropout1 = c(0, 0.25, 
    0.5), dropout2 = c(0, 0.25, 0.5), batch_size = c(1, 16, 32), 
    learning_rate = c(0.1, 0.01, 0.001, 1e-04)))",runs/2022-10-13T02-54-40Z/tfruns.d/source.tar.gz,local,training
401,runs/2022-10-13T02-54-28Z,NA,NA,#VALUE!,NA,NA,NA,NA,32,32,1,0.1,NA,NA,0,0,NA,NA,10,35,0,NA,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001D52DD611F0>,0.100000001,nn_exp.R,13/10/2022 2:54,13/10/2022 2:54,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_numeric(""dr ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   layer_dropout(rate = FLAGS$dropout1)  .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ","RuntimeError: reentrant call inside <_io.BufferedWriter name='<stdout>'>
","py_call_impl(callable, dots$args, dots$keywords)
(structure(function (...) 
{
    dots <- py_resolve_dots(list(...))
    result <- py_call_impl(callable, dots$args, dots$keywords)
    if (convert) 
        result <- py_to_r(result)
    if (is.null(result)) 
        invisible(result)
    else result
}, class = c(""python.builtin.method"", ""python.builtin.object""
), py_object = <environment>))(batch_size = 1L, epochs = 35L, 
    verbose = ""auto"", validation_split = 0.2, shuffle = TRUE, 
    class_weight = NULL, sample_weight = NULL, initial_epoch = 0L, 
    x = <environment>, y = <environment>, callbacks = list(<environment>, 
        <environment>))
do.call(object$fit, args)
fit.keras.engine.training.Model(., x = as.matrix(X_train), y = as.matrix(y_train), 
    validation_split = 0.2, batch_size = FLAGS$batch_size, epochs = 35)
fit(., x = as.matrix(X_train), y = as.matrix(y_train), validation_split = 0.2, 
    batch_size = FLAGS$batch_size, epochs = 35)
model %>% fit(x = as.matrix(X_train), y = as.matrix(y_train), 
    validation_split = 0.2, batch_size = FLAGS$batch_size, epochs = 35)
eval(ei, envir)
eval(ei, envir)
withVisible(eval(ei, envir))
tuning_run(""nn_exp.R"", flags = list(dense_units1 = c(32, 64, 
    128), dense_units2 = c(32, 64, 128), dropout1 = c(0, 0.25, 
    0.5), dropout2 = c(0, 0.25, 0.5), batch_size = c(1, 16, 32), 
    learning_rate = c(0.1, 0.01, 0.001, 1e-04)))",runs/2022-10-13T02-54-28Z/tfruns.d/source.tar.gz,local,training
402,runs/2022-10-13T02-53-14Z,NA,NA,#VALUE!,NA,NA,NA,NA,32,32,1,0.1,NA,NA,0,0,NA,NA,10,35,0,NA,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001D5020228B0>,0.100000001,nn_exp.R,13/10/2022 2:53,13/10/2022 2:53,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_numeric(""dr ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   layer_dropout(rate = FLAGS$dropout1)  .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,runs/2022-10-13T02-53-14Z/tfruns.d/source.tar.gz,local,training
405,runs/2022-10-13T01-08-01Z,NA,NA,#VALUE!,NA,NA,NA,NA,16,16,1,0.1,NA,NA,0,0,NA,NA,5,40,0,NA,"Model: <no summary available, model was not built>",mean_absolute_error,<keras.optimizers.optimizer_v2.adam.Adam object at 0x000002E582054E80>,NA,nn_exp.R,13/10/2022 1:08,13/10/2022 1:08,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_numeric(""dr ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   layer_dropout(rate = FLAGS$dropout1)  .... [TRUNCATED] 

> # LR scheduler
> lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)

> # Compile w/ optimiser and loss
> model %>% compile(optimizer = optimizer_adam(learning_rate = lr_schedule), loss = 'mean_absolute_error')

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,runs/2022-10-13T01-08-01Z/tfruns.d/source.tar.gz,local,training
