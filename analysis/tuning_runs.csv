"","run_dir","metric_val_loss","metric_loss","metric_mape","metric_val_mape","metric_log_cosh","metric_val_log_cosh","flag_dense_units1","flag_dense_units2","flag_dense_units3","flag_dense_units4","flag_batch_size","flag_learning_rate","flag_dropout1","flag_dropout2","flag_dropout3","flag_dropout4","flag_decay_steps","epochs","epochs_completed","metrics","model","loss_function","optimizer","learning_rate","script","start","end","completed","output","error_message","error_traceback","source_code","context","type"
"26","runs/2022-10-13T09-37-51Z",573.3195,753.126,2.6974,2.0018,NA,NA,64,64,32,32,16,0.01,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T09-37-51Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829B215E0>",0.00999999977648258,"nn_exp.R",2022-10-13 09:37:52,2022-10-13 09:39:24,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-37-51Z/tfruns.d/source.tar.gz","local","training"
"22","runs/2022-10-13T09-44-01Z",595.1414,805.2947,2.9054,2.0907,NA,NA,128,128,32,32,16,0.01,NA,NA,NA,NA,NA,35,28,"runs/2022-10-13T09-44-01Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B091C310>",0.00999999977648258,"nn_exp.R",2022-10-13 09:44:02,2022-10-13 09:46:08,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,"runs/2022-10-13T09-44-01Z/tfruns.d/source.tar.gz","local","training"
"25","runs/2022-10-13T09-39-24Z",606.2936,777.8643,2.8,2.1152,NA,NA,128,64,32,32,16,0.01,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-39-24Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B810E47EE0>",0.00999999977648258,"nn_exp.R",2022-10-13 09:39:25,2022-10-13 09:41:18,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-39-24Z/tfruns.d/source.tar.gz","local","training"
"29","runs/2022-10-13T09-33-29Z",606.8895,738.5018,2.6485,2.1384,NA,NA,64,32,32,32,16,0.01,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-33-29Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B825480190>",0.00999999977648258,"nn_exp.R",2022-10-13 09:33:30,2022-10-13 09:35:03,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-33-29Z/tfruns.d/source.tar.gz","local","training"
"6","runs/2022-10-13T10-27-22Z",625.6758,786.8459,2.835,2.1571,NA,NA,64,128,16,16,32,0.01,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T10-27-22Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814821A90>",0.00999999977648258,"nn_exp.R",2022-10-13 10:27:23,2022-10-13 10:28:17,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-27-22Z/tfruns.d/source.tar.gz","local","training"
"30","runs/2022-10-13T09-32-19Z",643.4521,823.7964,2.9679,2.2343,NA,NA,32,32,32,32,16,0.01,NA,NA,NA,NA,NA,35,26,"runs/2022-10-13T09-32-19Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8203DF400>",0.00999999977648258,"nn_exp.R",2022-10-13 09:32:20,2022-10-13 09:33:29,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-32-19Z/tfruns.d/source.tar.gz","local","training"
"2","runs/2022-10-13T10-31-39Z",647.9362,810.9839,2.9151,2.2288,NA,NA,64,64,16,16,32,0.01,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T10-31-39Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8298E3DC0>",0.00999999977648258,"nn_exp.R",2022-10-13 10:31:40,2022-10-13 10:32:40,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-31-39Z/tfruns.d/source.tar.gz","local","training"
"17","runs/2022-10-13T10-12-38Z",654.668,829.2277,2.9889,2.2842,NA,NA,128,64,16,16,32,0.01,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T10-12-38Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81241B520>",0.00999999977648258,"nn_exp.R",2022-10-13 10:12:39,2022-10-13 10:15:09,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-12-38Z/tfruns.d/source.tar.gz","local","training"
"23","runs/2022-10-13T09-42-45Z",670.8395,828.6305,2.985,2.2956,NA,NA,64,128,32,32,16,0.01,NA,NA,NA,NA,NA,35,24,"runs/2022-10-13T09-42-45Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B820369580>",0.00999999977648258,"nn_exp.R",2022-10-13 09:42:46,2022-10-13 09:44:01,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-42-45Z/tfruns.d/source.tar.gz","local","training"
"12","runs/2022-10-13T10-21-21Z",671.4872,845.4623,3.0543,2.3209,NA,NA,128,64,16,16,32,0.01,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T10-21-21Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8254232E0>",0.00999999977648258,"nn_exp.R",2022-10-13 10:21:22,2022-10-13 10:22:18,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-21-21Z/tfruns.d/source.tar.gz","local","training"
"161","runs/2022-10-13T07-08-23Z",675.5659,916.6062,3.3004,2.3148,NA,NA,64,64,32,64,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-08-23Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0ABCAC0>",0.100000001490116,"nn_exp.R",2022-10-13 07:08:23,2022-10-13 07:09:15,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-08-23Z/tfruns.d/source.tar.gz","local","training"
"186","runs/2022-10-13T06-46-05Z",679.5828,936.4544,3.3726,2.3152,NA,NA,32,128,32,32,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-46-05Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8148595B0>",0.100000001490116,"nn_exp.R",2022-10-13 06:46:05,2022-10-13 06:47:05,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-46-05Z/tfruns.d/source.tar.gz","local","training"
"219","runs/2022-10-13T05-54-21Z",680.8427,979.9597,3.54,2.3742,NA,NA,32,32,32,128,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T05-54-21Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A911940>",0.100000001490116,"nn_exp.R",2022-10-13 05:54:22,2022-10-13 05:56:05,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-54-21Z/tfruns.d/source.tar.gz","local","training"
"250","runs/2022-10-13T04-25-38Z",681.5782,1029.4475,3.7151,2.3864,NA,NA,128,64,128,32,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T04-25-38Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E0DFBE0>",0.100000001490116,"nn_exp.R",2022-10-13 04:25:38,2022-10-13 04:28:37,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-25-38Z/tfruns.d/source.tar.gz","local","training"
"244","runs/2022-10-13T04-40-17Z",685.2194,1007.7404,3.6382,2.3868,NA,NA,128,32,32,64,16,0.1,NA,NA,NA,NA,NA,35,33,"runs/2022-10-13T04-40-17Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E240AC0>",0.100000001490116,"nn_exp.R",2022-10-13 04:40:17,2022-10-13 04:43:15,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-40-17Z/tfruns.d/source.tar.gz","local","training"
"253","runs/2022-10-13T04-18-38Z",694.9407,985.7605,3.5635,2.4116,NA,NA,128,32,128,32,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T04-18-38Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B04D9880>",0.100000001490116,"nn_exp.R",2022-10-13 04:18:38,2022-10-13 04:21:13,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-18-38Z/tfruns.d/source.tar.gz","local","training"
"157","runs/2022-10-13T07-11-49Z",694.9576,983.6371,3.5607,2.415,NA,NA,128,128,32,64,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-11-49Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC2BC7F0>",0.100000001490116,"nn_exp.R",2022-10-13 07:11:50,2022-10-13 07:12:54,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-11-49Z/tfruns.d/source.tar.gz","local","training"
"181","runs/2022-10-13T06-50-37Z",695.0385,936.5186,3.386,2.4702,NA,NA,128,32,64,32,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-50-37Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B058BCA0>",0.100000001490116,"nn_exp.R",2022-10-13 06:50:37,2022-10-13 06:51:32,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-50-37Z/tfruns.d/source.tar.gz","local","training"
"176","runs/2022-10-13T06-54-51Z",695.1966,998.3041,3.6126,2.42,NA,NA,64,128,64,32,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-54-51Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC14DEE0>",0.100000001490116,"nn_exp.R",2022-10-13 06:54:51,2022-10-13 06:55:51,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-54-51Z/tfruns.d/source.tar.gz","local","training"
"73","runs/2022-10-13T08-56-15Z",696.3671,908.045,3.2645,2.4427,NA,NA,128,32,64,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-56-15Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCBD3550>",0.100000001490116,"nn_exp.R",2022-10-13 08:56:16,2022-10-13 08:56:49,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-56-15Z/tfruns.d/source.tar.gz","local","training"
"169","runs/2022-10-13T07-00-59Z",696.3672,966.3555,3.4943,2.4293,NA,NA,128,64,128,32,32,0.1,NA,NA,NA,NA,NA,35,32,"runs/2022-10-13T07-00-59Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BD129A0>",0.100000001490116,"nn_exp.R",2022-10-13 07:00:59,2022-10-13 07:01:53,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-00-59Z/tfruns.d/source.tar.gz","local","training"
"242","runs/2022-10-13T04-45-19Z",696.407,998.7519,3.6099,2.4422,NA,NA,64,64,32,64,16,0.1,NA,NA,NA,NA,NA,35,29,"runs/2022-10-13T04-45-19Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814A287F0>",0.100000001490116,"nn_exp.R",2022-10-13 04:45:19,2022-10-13 04:47:24,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-45-19Z/tfruns.d/source.tar.gz","local","training"
"214","runs/2022-10-13T06-02-36Z",698.1401,1007.8236,3.6452,2.4406,NA,NA,128,64,32,128,16,0.1,NA,NA,NA,NA,NA,35,26,"runs/2022-10-13T06-02-36Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC01EE50>",0.100000001490116,"nn_exp.R",2022-10-13 06:02:37,2022-10-13 06:03:55,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-02-36Z/tfruns.d/source.tar.gz","local","training"
"90","runs/2022-10-13T08-41-32Z",698.6015,960.0576,3.4602,2.4572,NA,NA,32,64,128,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-41-32Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B825606E80>",0.100000001490116,"nn_exp.R",2022-10-13 08:41:33,2022-10-13 08:42:56,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-41-32Z/tfruns.d/source.tar.gz","local","training"
"130","runs/2022-10-13T07-50-23Z",698.9678,949.5912,3.4128,2.4638,NA,NA,128,128,32,128,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-50-23Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8148D9490>",0.100000001490116,"nn_exp.R",2022-10-13 07:50:23,2022-10-13 07:52:09,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-50-23Z/tfruns.d/source.tar.gz","local","training"
"263","runs/2022-10-13T03-55-57Z",699.1981,985.9274,3.5644,2.4545,NA,NA,64,32,64,32,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T03-55-57Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8123DB8E0>",0.100000001490116,"nn_exp.R",2022-10-13 03:55:57,2022-10-13 03:58:00,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T03-55-57Z/tfruns.d/source.tar.gz","local","training"
"174","runs/2022-10-13T06-56-47Z",699.9622,979.5118,3.5459,2.4881,NA,NA,32,32,128,32,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-56-47Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8134D5DC0>",0.100000001490116,"nn_exp.R",2022-10-13 06:56:47,2022-10-13 06:57:37,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-56-47Z/tfruns.d/source.tar.gz","local","training"
"108","runs/2022-10-13T08-24-31Z",701.6896,988.3389,3.5774,2.4607,NA,NA,32,64,32,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-24-31Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8299959A0>",0.100000001490116,"nn_exp.R",2022-10-13 08:24:32,2022-10-13 08:25:20,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-24-31Z/tfruns.d/source.tar.gz","local","training"
"126","runs/2022-10-13T07-55-55Z",701.7139,943.7407,3.4022,2.4738,NA,NA,32,64,64,128,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-55-55Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0955970>",0.100000001490116,"nn_exp.R",2022-10-13 07:55:55,2022-10-13 07:57:15,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-55-55Z/tfruns.d/source.tar.gz","local","training"
"112","runs/2022-10-13T08-19-21Z",704.4305,955.9229,3.4537,2.4136,NA,NA,128,128,128,128,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-19-21Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0ABAA30>",0.100000001490116,"nn_exp.R",2022-10-13 08:19:22,2022-10-13 08:22:01,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-19-21Z/tfruns.d/source.tar.gz","local","training"
"243","runs/2022-10-13T04-43-15Z",704.5776,1047.1064,3.785,2.4842,NA,NA,32,64,32,64,16,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T04-43-15Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FA41460>",0.100000001490116,"nn_exp.R",2022-10-13 04:43:15,2022-10-13 04:45:19,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-43-15Z/tfruns.d/source.tar.gz","local","training"
"10","runs/2022-10-13T10-23-15Z",705.0474,801.8581,2.8869,2.4053,NA,NA,128,128,32,32,32,0.001,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T10-23-15Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B09C1CA0>",0.00100000004749745,"nn_exp.R",2022-10-13 10:23:16,2022-10-13 10:24:33,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-23-15Z/tfruns.d/source.tar.gz","local","training"
"72","runs/2022-10-13T08-56-49Z",707.3043,915.9071,3.298,2.5124,NA,NA,32,64,64,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-56-49Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BBFACD30>",0.100000001490116,"nn_exp.R",2022-10-13 08:56:50,2022-10-13 08:57:24,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-56-49Z/tfruns.d/source.tar.gz","local","training"
"226","runs/2022-10-13T05-14-11Z",707.4478,962.5492,3.4804,2.5137,NA,NA,128,32,128,64,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T05-14-11Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E1DB940>",0.100000001490116,"nn_exp.R",2022-10-13 05:14:11,2022-10-13 05:15:43,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-14-11Z/tfruns.d/source.tar.gz","local","training"
"24","runs/2022-10-13T09-41-19Z",707.8422,827.291,2.9767,2.4187,NA,NA,32,128,32,32,16,0.01,NA,NA,NA,NA,NA,35,27,"runs/2022-10-13T09-41-19Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BCACA60>",0.00999999977648258,"nn_exp.R",2022-10-13 09:41:19,2022-10-13 09:42:45,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-41-19Z/tfruns.d/source.tar.gz","local","training"
"137","runs/2022-10-13T07-40-32Z",709.8116,956.9755,3.4466,2.5148,NA,NA,64,32,32,128,32,0.1,NA,NA,NA,NA,NA,35,28,"runs/2022-10-13T07-40-32Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812412B50>",0.100000001490116,"nn_exp.R",2022-10-13 07:40:33,2022-10-13 07:41:52,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-40-32Z/tfruns.d/source.tar.gz","local","training"
"210","runs/2022-10-13T06-09-10Z",709.8505,1024.3672,3.6927,2.45,NA,NA,32,32,64,128,16,0.1,NA,NA,NA,NA,NA,35,32,"runs/2022-10-13T06-09-10Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0314BB0>",0.100000001490116,"nn_exp.R",2022-10-13 06:09:10,2022-10-13 06:10:37,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-09-10Z/tfruns.d/source.tar.gz","local","training"
"270","runs/2022-10-13T03-42-56Z",710.9014,1024.4674,3.6993,2.5175,NA,NA,32,64,32,32,16,0.1,NA,NA,NA,NA,NA,35,28,"runs/2022-10-13T03-42-56Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8121C5FD0>",0.100000001490116,"nn_exp.R",2022-10-13 03:42:56,2022-10-13 03:44:18,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T03-42-56Z/tfruns.d/source.tar.gz","local","training"
"178","runs/2022-10-13T06-53-21Z",711.205,962.1656,3.4804,2.5365,NA,NA,128,64,64,32,32,0.1,NA,NA,NA,NA,NA,35,32,"runs/2022-10-13T06-53-21Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0B56340>",0.100000001490116,"nn_exp.R",2022-10-13 06:53:21,2022-10-13 06:54:13,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-53-21Z/tfruns.d/source.tar.gz","local","training"
"50","runs/2022-10-13T09-12-46Z",711.3318,974.0612,3.5257,2.4587,NA,NA,64,128,32,128,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-12-46Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B82981EEB0>",0.100000001490116,"nn_exp.R",2022-10-13 09:12:47,2022-10-13 09:13:29,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-12-46Z/tfruns.d/source.tar.gz","local","training"
"124","runs/2022-10-13T07-58-29Z",711.5007,979.9584,3.5426,2.453,NA,NA,128,64,64,128,32,0.1,NA,NA,NA,NA,NA,35,34,"runs/2022-10-13T07-58-29Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FB0DE80>",0.100000001490116,"nn_exp.R",2022-10-13 07:58:29,2022-10-13 07:59:50,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-58-29Z/tfruns.d/source.tar.gz","local","training"
"115","runs/2022-10-13T08-13-00Z",712.2429,960.6792,3.4765,2.446,NA,NA,128,64,128,128,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-13-00Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC2217F0>",0.100000001490116,"nn_exp.R",2022-10-13 08:13:00,2022-10-13 08:14:57,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-13-00Z/tfruns.d/source.tar.gz","local","training"
"196","runs/2022-10-13T06-32-49Z",714.5499,1024.7784,3.712,2.5385,NA,NA,128,64,128,128,16,0.1,NA,NA,NA,NA,NA,35,27,"runs/2022-10-13T06-32-49Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E3D9E50>",0.100000001490116,"nn_exp.R",2022-10-13 06:32:49,2022-10-13 06:34:24,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-32-49Z/tfruns.d/source.tar.gz","local","training"
"245","runs/2022-10-13T04-38-13Z",715.309,1035.0679,3.7342,2.5571,NA,NA,64,32,32,64,16,0.1,NA,NA,NA,NA,NA,35,30,"runs/2022-10-13T04-38-13Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E446FA0>",0.100000001490116,"nn_exp.R",2022-10-13 04:38:13,2022-10-13 04:40:17,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-38-13Z/tfruns.d/source.tar.gz","local","training"
"254","runs/2022-10-13T04-16-57Z",715.7474,1046.884,3.7821,2.4927,NA,NA,64,32,128,32,16,0.1,NA,NA,NA,NA,NA,35,24,"runs/2022-10-13T04-16-57Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC2196D0>",0.100000001490116,"nn_exp.R",2022-10-13 04:16:57,2022-10-13 04:18:38,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-16-57Z/tfruns.d/source.tar.gz","local","training"
"110","runs/2022-10-13T08-22-43Z",716.6061,914.2659,3.2874,2.4306,NA,NA,64,32,32,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-22-43Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC0C4DC0>",0.100000001490116,"nn_exp.R",2022-10-13 08:22:44,2022-10-13 08:23:32,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-22-43Z/tfruns.d/source.tar.gz","local","training"
"143","runs/2022-10-13T07-28-22Z",716.7641,1031.2864,3.7292,2.4742,NA,NA,64,64,128,64,32,0.1,NA,NA,NA,NA,NA,35,29,"runs/2022-10-13T07-28-22Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8147C5A90>",0.100000001490116,"nn_exp.R",2022-10-13 07:28:23,2022-10-13 07:29:49,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-28-22Z/tfruns.d/source.tar.gz","local","training"
"172","runs/2022-10-13T06-58-19Z",718.2172,992.67,3.5926,2.5631,NA,NA,128,32,128,32,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-58-19Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCB23070>",0.100000001490116,"nn_exp.R",2022-10-13 06:58:19,2022-10-13 06:59:12,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-58-19Z/tfruns.d/source.tar.gz","local","training"
"55","runs/2022-10-13T09-09-50Z",719.8474,918.6698,3.3085,2.5425,NA,NA,128,32,32,128,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-09-50Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8254E3B80>",0.100000001490116,"nn_exp.R",2022-10-13 09:09:51,2022-10-13 09:10:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-09-50Z/tfruns.d/source.tar.gz","local","training"
"116","runs/2022-10-13T08-11-09Z",719.8677,951.2838,3.4394,2.5704,NA,NA,64,64,128,128,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-11-09Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC3062E0>",0.100000001490116,"nn_exp.R",2022-10-13 08:11:10,2022-10-13 08:13:00,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-11-09Z/tfruns.d/source.tar.gz","local","training"
"120","runs/2022-10-13T08-03-59Z",720.6308,967.1148,3.5015,2.4772,NA,NA,32,32,128,128,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-03-59Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B04D9D60>",0.100000001490116,"nn_exp.R",2022-10-13 08:04:00,2022-10-13 08:05:49,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-03-59Z/tfruns.d/source.tar.gz","local","training"
"190","runs/2022-10-13T06-42-30Z",721.8854,910.5837,3.2805,2.5825,NA,NA,128,32,32,32,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-42-30Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8148593D0>",0.100000001490116,"nn_exp.R",2022-10-13 06:42:30,2022-10-13 06:43:28,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-42-30Z/tfruns.d/source.tar.gz","local","training"
"75","runs/2022-10-13T08-55-03Z",722.2324,1024.8286,3.7084,2.5448,NA,NA,32,32,64,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-55-03Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FB0D490>",0.100000001490116,"nn_exp.R",2022-10-13 08:55:04,2022-10-13 08:55:41,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-55-03Z/tfruns.d/source.tar.gz","local","training"
"248","runs/2022-10-13T04-31-10Z",722.5732,1034.9698,3.7422,2.579,NA,NA,64,128,128,32,16,0.1,NA,NA,NA,NA,NA,35,23,"runs/2022-10-13T04-31-10Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E258850>",0.100000001490116,"nn_exp.R",2022-10-13 04:31:10,2022-10-13 04:33:11,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-31-10Z/tfruns.d/source.tar.gz","local","training"
"132","runs/2022-10-13T07-47-31Z",722.7789,989.8367,3.5828,2.5781,NA,NA,32,128,32,128,32,0.1,NA,NA,NA,NA,NA,35,34,"runs/2022-10-13T07-47-31Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B825481E20>",0.100000001490116,"nn_exp.R",2022-10-13 07:47:32,2022-10-13 07:48:49,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-47-31Z/tfruns.d/source.tar.gz","local","training"
"236","runs/2022-10-13T04-57-27Z",723.619,1028.9165,3.7139,2.5729,NA,NA,64,32,64,64,16,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T04-57-27Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC1772E0>",0.100000001490116,"nn_exp.R",2022-10-13 04:57:27,2022-10-13 04:59:14,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-57-27Z/tfruns.d/source.tar.gz","local","training"
"204","runs/2022-10-13T06-18-25Z",724.3434,1030.2456,3.7152,2.5853,NA,NA,32,128,64,128,16,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T06-18-25Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FAE9EB0>",0.100000001490116,"nn_exp.R",2022-10-13 06:18:26,2022-10-13 06:19:50,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-18-25Z/tfruns.d/source.tar.gz","local","training"
"13","runs/2022-10-13T10-19-59Z",724.3657,833.2083,3.0043,2.4865,NA,NA,128,128,16,16,32,0.001,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T10-19-59Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81499E610>",0.00100000004749745,"nn_exp.R",2022-10-13 10:20:00,2022-10-13 10:21:21,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-19-59Z/tfruns.d/source.tar.gz","local","training"
"193","runs/2022-10-13T06-38-25Z",724.8901,991.1508,3.5815,2.5818,NA,NA,128,128,128,128,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-38-25Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B04D9D00>",0.100000001490116,"nn_exp.R",2022-10-13 06:38:25,2022-10-13 06:40:41,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-38-25Z/tfruns.d/source.tar.gz","local","training"
"57","runs/2022-10-13T09-08-46Z",725.1211,985.0038,3.5678,2.5242,NA,NA,32,32,32,128,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-08-46Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8297F8D00>",0.100000001490116,"nn_exp.R",2022-10-13 09:08:47,2022-10-13 09:09:18,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-08-46Z/tfruns.d/source.tar.gz","local","training"
"208","runs/2022-10-13T06-12-09Z",725.1715,999.7997,3.6117,2.5893,NA,NA,128,32,64,128,16,0.1,NA,NA,NA,NA,NA,35,33,"runs/2022-10-13T06-12-09Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E05BAC0>",0.100000001490116,"nn_exp.R",2022-10-13 06:12:10,2022-10-13 06:13:42,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-12-09Z/tfruns.d/source.tar.gz","local","training"
"164","runs/2022-10-13T07-05-54Z",726.5903,964.0244,3.4898,2.5948,NA,NA,64,32,32,64,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-05-54Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8255976D0>",0.100000001490116,"nn_exp.R",2022-10-13 07:05:55,2022-10-13 07:06:46,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-05-54Z/tfruns.d/source.tar.gz","local","training"
"95","runs/2022-10-13T08-36-30Z",728.0418,1020.9727,3.6973,2.5775,NA,NA,64,128,64,32,64,0.1,NA,NA,NA,NA,NA,35,34,"runs/2022-10-13T08-36-30Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8297A0AC0>",0.100000001490116,"nn_exp.R",2022-10-13 08:36:31,2022-10-13 08:37:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-36-30Z/tfruns.d/source.tar.gz","local","training"
"86","runs/2022-10-13T08-46-59Z",729.361,994.5977,3.6077,2.534,NA,NA,64,128,128,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-46-59Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A8B0910>",0.100000001490116,"nn_exp.R",2022-10-13 08:46:59,2022-10-13 08:48:07,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-46-59Z/tfruns.d/source.tar.gz","local","training"
"147","runs/2022-10-13T07-22-57Z",731.1362,1052.4835,3.8115,2.5527,NA,NA,32,32,128,64,32,0.1,NA,NA,NA,NA,NA,35,22,"runs/2022-10-13T07-22-57Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829973850>",0.100000001490116,"nn_exp.R",2022-10-13 07:22:58,2022-10-13 07:23:48,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-22-57Z/tfruns.d/source.tar.gz","local","training"
"125","runs/2022-10-13T07-57-15Z",731.5897,942.1647,3.4128,2.6232,NA,NA,64,64,64,128,32,0.1,NA,NA,NA,NA,NA,35,32,"runs/2022-10-13T07-57-15Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0341F70>",0.100000001490116,"nn_exp.R",2022-10-13 07:57:16,2022-10-13 07:58:29,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-57-15Z/tfruns.d/source.tar.gz","local","training"
"111","runs/2022-10-13T08-22-01Z",733.9169,965.5461,3.4791,2.5034,NA,NA,32,32,32,32,64,0.1,NA,NA,NA,NA,NA,35,32,"runs/2022-10-13T08-22-01Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8254ADEB0>",0.100000001490116,"nn_exp.R",2022-10-13 08:22:01,2022-10-13 08:22:43,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-22-01Z/tfruns.d/source.tar.gz","local","training"
"188","runs/2022-10-13T06-44-23Z",734.3703,971.8352,3.5209,2.6292,NA,NA,64,64,32,32,32,0.1,NA,NA,NA,NA,NA,35,28,"runs/2022-10-13T06-44-23Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0662DF0>",0.100000001490116,"nn_exp.R",2022-10-13 06:44:24,2022-10-13 06:45:12,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-44-23Z/tfruns.d/source.tar.gz","local","training"
"14","runs/2022-10-13T10-19-02Z",735.2396,845.9794,3.0446,2.586,NA,NA,64,64,32,16,32,0.001,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T10-19-02Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A89E940>",0.00100000004749745,"nn_exp.R",2022-10-13 10:19:03,2022-10-13 10:19:59,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-19-02Z/tfruns.d/source.tar.gz","local","training"
"123","runs/2022-10-13T07-59-50Z",737.1013,983.3694,3.5556,2.6156,NA,NA,32,128,64,128,32,0.1,NA,NA,NA,NA,NA,35,30,"runs/2022-10-13T07-59-50Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B84109E040>",0.100000001490116,"nn_exp.R",2022-10-13 07:59:51,2022-10-13 08:01:01,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-59-50Z/tfruns.d/source.tar.gz","local","training"
"230","runs/2022-10-13T05-08-43Z",738.9039,1040.745,3.7605,2.6329,NA,NA,64,128,64,64,16,0.1,NA,NA,NA,NA,NA,35,27,"runs/2022-10-13T05-08-43Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B08DC280>",0.100000001490116,"nn_exp.R",2022-10-13 05:08:44,2022-10-13 05:09:53,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-08-43Z/tfruns.d/source.tar.gz","local","training"
"4","runs/2022-10-13T10-29-13Z",739.2665,719.2766,2.5889,2.5008,NA,NA,128,128,32,16,32,0.01,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T10-29-13Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E1F2AC0>",0.00999999977648258,"nn_exp.R",2022-10-13 10:29:14,2022-10-13 10:30:36,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-29-13Z/tfruns.d/source.tar.gz","local","training"
"63","runs/2022-10-13T09-02-54Z",740.509,1036.2501,3.745,2.6578,NA,NA,32,64,128,64,64,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T09-02-54Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC32DD00>",0.100000001490116,"nn_exp.R",2022-10-13 09:02:55,2022-10-13 09:03:35,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-02-54Z/tfruns.d/source.tar.gz","local","training"
"3","runs/2022-10-13T10-30-36Z",740.8794,856.6744,3.0868,2.5813,NA,NA,128,64,16,16,32,0.001,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T10-30-36Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814A20E20>",0.00100000004749745,"nn_exp.R",2022-10-13 10:30:37,2022-10-13 10:31:39,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-30-36Z/tfruns.d/source.tar.gz","local","training"
"194","runs/2022-10-13T06-36-34Z",741.0732,1021.6611,3.6923,2.5239,NA,NA,64,128,128,128,16,0.1,NA,NA,NA,NA,NA,35,30,"runs/2022-10-13T06-36-34Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC02FB20>",0.100000001490116,"nn_exp.R",2022-10-13 06:36:34,2022-10-13 06:38:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-36-34Z/tfruns.d/source.tar.gz","local","training"
"99","runs/2022-10-13T08-33-04Z",743.1846,1035.0873,3.7419,2.6332,NA,NA,32,64,64,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-33-04Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81483D1F0>",0.100000001490116,"nn_exp.R",2022-10-13 08:33:05,2022-10-13 08:33:57,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-33-04Z/tfruns.d/source.tar.gz","local","training"
"197","runs/2022-10-13T06-30-52Z",745.293,1010.4839,3.6473,2.6743,NA,NA,64,64,128,128,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-30-52Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814A2A340>",0.100000001490116,"nn_exp.R",2022-10-13 06:30:52,2022-10-13 06:32:49,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-30-52Z/tfruns.d/source.tar.gz","local","training"
"117","runs/2022-10-13T08-09-01Z",747.2878,978.809,3.5401,2.6835,NA,NA,32,64,128,128,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-09-01Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B058D910>",0.100000001490116,"nn_exp.R",2022-10-13 08:09:01,2022-10-13 08:11:09,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-09-01Z/tfruns.d/source.tar.gz","local","training"
"213","runs/2022-10-13T06-03-55Z",747.5043,990.6082,3.5804,2.6842,NA,NA,32,128,32,128,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-03-55Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8146B1490>",0.100000001490116,"nn_exp.R",2022-10-13 06:03:56,2022-10-13 06:05:36,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-03-55Z/tfruns.d/source.tar.gz","local","training"
"170","runs/2022-10-13T07-00-10Z",747.5146,962.3405,3.4822,2.6782,NA,NA,64,64,128,32,32,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T07-00-10Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BCE2E20>",0.100000001490116,"nn_exp.R",2022-10-13 07:00:11,2022-10-13 07:00:58,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-00-10Z/tfruns.d/source.tar.gz","local","training"
"249","runs/2022-10-13T04-28-37Z",750.6362,988.9749,3.5695,2.6864,NA,NA,32,128,128,32,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T04-28-37Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E3A1D90>",0.100000001490116,"nn_exp.R",2022-10-13 04:28:37,2022-10-13 04:31:10,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-28-37Z/tfruns.d/source.tar.gz","local","training"
"87","runs/2022-10-13T08-45-55Z",751.6548,1058.0009,3.8082,2.6347,NA,NA,32,128,128,32,64,0.1,NA,NA,NA,NA,NA,35,29,"runs/2022-10-13T08-45-55Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCBF8EE0>",0.100000001490116,"nn_exp.R",2022-10-13 08:45:56,2022-10-13 08:46:58,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-45-55Z/tfruns.d/source.tar.gz","local","training"
"216","runs/2022-10-13T05-59-21Z",751.6656,1004.4008,3.6254,2.7004,NA,NA,32,64,32,128,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T05-59-21Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC158190>",0.100000001490116,"nn_exp.R",2022-10-13 05:59:22,2022-10-13 06:01:02,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-59-21Z/tfruns.d/source.tar.gz","local","training"
"28","runs/2022-10-13T09-35-03Z",752.0068,799.3423,2.8748,2.5727,NA,NA,128,32,32,32,16,0.01,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-35-03Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8298FA6A0>",0.00999999977648258,"nn_exp.R",2022-10-13 09:35:04,2022-10-13 09:36:44,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-35-03Z/tfruns.d/source.tar.gz","local","training"
"62","runs/2022-10-13T09-03-35Z",753.1688,999.184,3.6172,2.6848,NA,NA,64,64,128,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-03-35Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B841096520>",0.100000001490116,"nn_exp.R",2022-10-13 09:03:35,2022-10-13 09:04:20,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-03-35Z/tfruns.d/source.tar.gz","local","training"
"192","runs/2022-10-13T06-40-41Z",753.6954,1027.4912,3.7164,2.699,NA,NA,32,32,32,32,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-40-41Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829A8F760>",0.100000001490116,"nn_exp.R",2022-10-13 06:40:42,2022-10-13 06:41:35,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-40-41Z/tfruns.d/source.tar.gz","local","training"
"224","runs/2022-10-13T05-17-12Z",757.1614,1003.3287,3.6281,2.565,NA,NA,64,64,128,64,16,0.1,NA,NA,NA,NA,NA,35,29,"runs/2022-10-13T05-17-12Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8149AF190>",0.100000001490116,"nn_exp.R",2022-10-13 05:17:12,2022-10-13 05:18:32,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-17-12Z/tfruns.d/source.tar.gz","local","training"
"150","runs/2022-10-13T07-18-38Z",757.5097,1056.0367,3.8263,2.718,NA,NA,32,128,64,64,32,0.1,NA,NA,NA,NA,NA,35,23,"runs/2022-10-13T07-18-38Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0B6D970>",0.100000001490116,"nn_exp.R",2022-10-13 07:18:39,2022-10-13 07:19:32,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-18-38Z/tfruns.d/source.tar.gz","local","training"
"189","runs/2022-10-13T06-43-28Z",761.5934,963.764,3.4839,2.7371,NA,NA,32,64,32,32,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-43-28Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829A44640>",0.100000001490116,"nn_exp.R",2022-10-13 06:43:28,2022-10-13 06:44:23,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-43-28Z/tfruns.d/source.tar.gz","local","training"
"136","runs/2022-10-13T07-41-52Z",766.8297,984.0184,3.5531,2.6213,NA,NA,128,32,32,128,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-41-52Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FAC7790>",0.100000001490116,"nn_exp.R",2022-10-13 07:41:53,2022-10-13 07:43:20,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-41-52Z/tfruns.d/source.tar.gz","local","training"
"154","runs/2022-10-13T07-14-40Z",767.1024,928.6301,3.3414,2.5709,NA,NA,128,32,64,64,32,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T07-14-40Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8134C3460>",0.100000001490116,"nn_exp.R",2022-10-13 07:14:40,2022-10-13 07:15:30,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-14-40Z/tfruns.d/source.tar.gz","local","training"
"39","runs/2022-10-13T09-21-53Z",767.9617,984.6464,3.566,2.6176,NA,NA,32,32,128,128,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-21-53Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B84105C5B0>",0.100000001490116,"nn_exp.R",2022-10-13 09:21:54,2022-10-13 09:22:55,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-21-53Z/tfruns.d/source.tar.gz","local","training"
"44","runs/2022-10-13T09-17-12Z",771.3149,1007.2397,3.648,2.6308,NA,NA,64,64,64,128,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-17-12Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCC11D90>",0.100000001490116,"nn_exp.R",2022-10-13 09:17:13,2022-10-13 09:18:01,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-17-12Z/tfruns.d/source.tar.gz","local","training"
"42","runs/2022-10-13T09-18-55Z",772.5775,1026.7312,3.7145,2.6666,NA,NA,32,128,64,128,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-18-55Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B04DBE50>",0.100000001490116,"nn_exp.R",2022-10-13 09:18:56,2022-10-13 09:19:48,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-18-55Z/tfruns.d/source.tar.gz","local","training"
"135","runs/2022-10-13T07-43-20Z",774.3018,951.3013,3.4407,2.603,NA,NA,32,64,32,128,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-43-20Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8147A0250>",0.100000001490116,"nn_exp.R",2022-10-13 07:43:21,2022-10-13 07:44:48,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-43-20Z/tfruns.d/source.tar.gz","local","training"
"93","runs/2022-10-13T08-38-57Z",776.8474,939.8525,3.3856,2.6039,NA,NA,32,32,128,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-38-57Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814982EB0>",0.100000001490116,"nn_exp.R",2022-10-13 08:38:57,2022-10-13 08:39:49,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-38-57Z/tfruns.d/source.tar.gz","local","training"
"36","runs/2022-10-13T09-24-46Z",777.18,1360.0151,4.9112,2.6753,NA,NA,32,64,128,128,64,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T09-24-46Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B820393640>",0.100000001490116,"nn_exp.R",2022-10-13 09:24:47,2022-10-13 09:25:53,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-24-46Z/tfruns.d/source.tar.gz","local","training"
"246","runs/2022-10-13T04-36-26Z",777.4027,1036.7533,3.7484,2.6393,NA,NA,32,32,32,64,16,0.1,NA,NA,NA,NA,NA,35,25,"runs/2022-10-13T04-36-26Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8134D9E50>",0.100000001490116,"nn_exp.R",2022-10-13 04:36:26,2022-10-13 04:38:13,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-36-26Z/tfruns.d/source.tar.gz","local","training"
"273","runs/2022-10-13T03-38-23Z",777.4327,1027.4783,3.7113,2.8009,NA,NA,32,32,32,32,16,0.1,NA,NA,NA,NA,NA,35,27,"runs/2022-10-13T03-38-23Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B04953D0>",0.100000001490116,"nn_exp.R",2022-10-13 03:38:23,2022-10-13 03:39:54,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T03-38-23Z/tfruns.d/source.tar.gz","local","training"
"239","runs/2022-10-13T04-51-41Z",779.2065,1033.2399,3.7347,2.6953,NA,NA,64,128,32,64,16,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T04-51-41Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8203A38B0>",0.100000001490116,"nn_exp.R",2022-10-13 04:51:41,2022-10-13 04:53:34,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-51-41Z/tfruns.d/source.tar.gz","local","training"
"282","runs/2022-10-13T01-29-55Z",779.8922,1171.0027,NA,NA,NA,NA,16,16,NA,NA,1,0.1,0,0,NA,NA,10,40,40,"runs/2022-10-13T01-29-55Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000002E5BE70CAC0>",0.100000001490116,"nn_exp.R",2022-10-13 01:29:55,2022-10-13 01:49:47,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_numeric(""dr ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   layer_dropout(rate = FLAGS$dropout1)  .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T01-29-55Z/tfruns.d/source.tar.gz","local","training"
"60","runs/2022-10-13T09-05-11Z",783.8005,976.3524,3.5162,2.6849,NA,NA,32,128,128,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-05-11Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0351B80>",0.100000001490116,"nn_exp.R",2022-10-13 09:05:12,2022-10-13 09:06:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-05-11Z/tfruns.d/source.tar.gz","local","training"
"175","runs/2022-10-13T06-55-51Z",790.3299,982.6049,3.559,2.7164,NA,NA,128,128,64,32,32,0.1,NA,NA,NA,NA,NA,35,30,"runs/2022-10-13T06-55-51Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FAD3820>",0.100000001490116,"nn_exp.R",2022-10-13 06:55:51,2022-10-13 06:56:47,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-55-51Z/tfruns.d/source.tar.gz","local","training"
"138","runs/2022-10-13T07-39-03Z",791.0208,1047.0531,3.782,2.7046,NA,NA,32,32,32,128,32,0.1,NA,NA,NA,NA,NA,35,29,"runs/2022-10-13T07-39-03Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B02D2B80>",0.100000001490116,"nn_exp.R",2022-10-13 07:39:04,2022-10-13 07:40:32,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-39-03Z/tfruns.d/source.tar.gz","local","training"
"267","runs/2022-10-13T03-47-49Z",791.991,1013.9828,3.659,2.6914,NA,NA,32,128,32,32,16,0.1,NA,NA,NA,NA,NA,35,34,"runs/2022-10-13T03-47-49Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81F1953D0>",0.100000001490116,"nn_exp.R",2022-10-13 03:47:49,2022-10-13 03:49:59,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T03-47-49Z/tfruns.d/source.tar.gz","local","training"
"91","runs/2022-10-13T08-40-21Z",795.8943,1005.6692,3.6168,2.6753,NA,NA,128,32,128,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-40-21Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E2BC760>",0.100000001490116,"nn_exp.R",2022-10-13 08:40:22,2022-10-13 08:41:32,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-40-21Z/tfruns.d/source.tar.gz","local","training"
"81","runs/2022-10-13T08-51-04Z",796.9973,885.3781,3.1803,2.6642,NA,NA,32,64,32,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-51-04Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829959940>",0.100000001490116,"nn_exp.R",2022-10-13 08:51:05,2022-10-13 08:51:37,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-51-04Z/tfruns.d/source.tar.gz","local","training"
"48","runs/2022-10-13T09-14-38Z",803.7413,1035.5541,3.7525,2.7106,NA,NA,32,32,64,128,64,0.1,NA,NA,NA,NA,NA,35,30,"runs/2022-10-13T09-14-38Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8149F5C70>",0.100000001490116,"nn_exp.R",2022-10-13 09:14:39,2022-10-13 09:15:15,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-14-38Z/tfruns.d/source.tar.gz","local","training"
"97","runs/2022-10-13T08-34-51Z",804.556,959.4099,3.4802,2.9085,NA,NA,128,64,64,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-34-51Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BC97190>",0.100000001490116,"nn_exp.R",2022-10-13 08:34:52,2022-10-13 08:35:52,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-34-51Z/tfruns.d/source.tar.gz","local","training"
"65","runs/2022-10-13T09-01-49Z",805.2579,1078.4153,3.8936,2.761,NA,NA,64,32,128,64,64,0.1,NA,NA,NA,NA,NA,35,21,"runs/2022-10-13T09-01-49Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81493F700>",0.100000001490116,"nn_exp.R",2022-10-13 09:01:49,2022-10-13 09:02:14,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-01-49Z/tfruns.d/source.tar.gz","local","training"
"100","runs/2022-10-13T08-32-16Z",806.086,951.4152,3.4281,2.6926,NA,NA,128,32,64,32,64,0.1,NA,NA,NA,NA,NA,35,33,"runs/2022-10-13T08-32-16Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8146F9670>",0.100000001490116,"nn_exp.R",2022-10-13 08:32:16,2022-10-13 08:33:04,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-32-16Z/tfruns.d/source.tar.gz","local","training"
"247","runs/2022-10-13T04-33-11Z",806.2631,1022.0961,3.688,2.7118,NA,NA,128,128,128,32,16,0.1,NA,NA,NA,NA,NA,35,34,"runs/2022-10-13T04-33-11Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E3E8A00>",0.100000001490116,"nn_exp.R",2022-10-13 04:33:11,2022-10-13 04:36:26,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-33-11Z/tfruns.d/source.tar.gz","local","training"
"80","runs/2022-10-13T08-51-37Z",808.1075,906.9078,3.2605,2.9453,NA,NA,64,64,32,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-51-37Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829980C70>",0.100000001490116,"nn_exp.R",2022-10-13 08:51:38,2022-10-13 08:52:09,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-51-37Z/tfruns.d/source.tar.gz","local","training"
"199","runs/2022-10-13T06-27-04Z",811.3246,1020.1242,3.6757,2.9254,NA,NA,128,32,128,128,16,0.1,NA,NA,NA,NA,NA,35,34,"runs/2022-10-13T06-27-04Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0AE2BE0>",0.100000001490116,"nn_exp.R",2022-10-13 06:27:05,2022-10-13 06:28:57,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-27-04Z/tfruns.d/source.tar.gz","local","training"
"182","runs/2022-10-13T06-49-43Z",811.5713,956.3127,3.442,2.8009,NA,NA,64,32,64,32,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-49-43Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81493FE80>",0.100000001490116,"nn_exp.R",2022-10-13 06:49:43,2022-10-13 06:50:37,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-49-43Z/tfruns.d/source.tar.gz","local","training"
"74","runs/2022-10-13T08-55-41Z",812.2194,930.6768,3.3593,2.7173,NA,NA,64,32,64,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-55-41Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC0C5AC0>",0.100000001490116,"nn_exp.R",2022-10-13 08:55:42,2022-10-13 08:56:15,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-55-41Z/tfruns.d/source.tar.gz","local","training"
"206","runs/2022-10-13T06-15-16Z",812.8424,1002.6391,3.6183,2.931,NA,NA,64,64,64,128,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-15-16Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8149F3310>",0.100000001490116,"nn_exp.R",2022-10-13 06:15:16,2022-10-13 06:16:47,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-15-16Z/tfruns.d/source.tar.gz","local","training"
"19","runs/2022-10-13T10-10-13Z",813.2297,847.1533,3.0604,2.7591,NA,NA,64,64,16,16,32,0.001,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T10-10-13Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0355C70>",0.00100000004749745,"nn_exp.R",2022-10-13 10:10:14,2022-10-13 10:11:18,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-10-13Z/tfruns.d/source.tar.gz","local","training"
"218","runs/2022-10-13T05-56-06Z",821.6153,996.8666,3.6017,2.7437,NA,NA,64,32,32,128,16,0.1,NA,NA,NA,NA,NA,35,29,"runs/2022-10-13T05-56-06Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC02F100>",0.100000001490116,"nn_exp.R",2022-10-13 05:56:06,2022-10-13 05:57:33,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-56-06Z/tfruns.d/source.tar.gz","local","training"
"37","runs/2022-10-13T09-23-57Z",825.8978,1119.5721,4.0515,2.8224,NA,NA,128,32,128,128,64,0.1,NA,NA,NA,NA,NA,35,25,"runs/2022-10-13T09-23-57Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B84107B220>",0.100000001490116,"nn_exp.R",2022-10-13 09:23:58,2022-10-13 09:24:46,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-23-57Z/tfruns.d/source.tar.gz","local","training"
"220","runs/2022-10-13T05-22-19Z",827.1652,1012.9291,3.6584,2.7972,NA,NA,128,128,128,64,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T05-22-19Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812429370>",0.100000001490116,"nn_exp.R",2022-10-13 05:22:19,2022-10-13 05:54:21,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-22-19Z/tfruns.d/source.tar.gz","local","training"
"52","runs/2022-10-13T09-11-33Z",827.3054,946.5643,3.4092,2.9942,NA,NA,128,64,32,128,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-11-33Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BC73BE0>",0.100000001490116,"nn_exp.R",2022-10-13 09:11:34,2022-10-13 09:12:13,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-11-33Z/tfruns.d/source.tar.gz","local","training"
"54","runs/2022-10-13T09-10-25Z",828.1655,989.0777,3.5776,2.7844,NA,NA,32,64,32,128,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-10-25Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7ADF1F7F0>",0.100000001490116,"nn_exp.R",2022-10-13 09:10:25,2022-10-13 09:10:59,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-10-25Z/tfruns.d/source.tar.gz","local","training"
"49","runs/2022-10-13T09-13-29Z",832.7817,978.7689,3.5267,2.9853,NA,NA,128,128,32,128,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-13-29Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B029DE20>",0.100000001490116,"nn_exp.R",2022-10-13 09:13:30,2022-10-13 09:14:38,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-13-29Z/tfruns.d/source.tar.gz","local","training"
"38","runs/2022-10-13T09-22-55Z",832.8457,1014.6603,3.6455,2.7763,NA,NA,64,32,128,128,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-22-55Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B031EFD0>",0.100000001490116,"nn_exp.R",2022-10-13 09:22:56,2022-10-13 09:23:57,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-22-55Z/tfruns.d/source.tar.gz","local","training"
"215","runs/2022-10-13T06-01-02Z",837.0997,1006.698,3.6361,3.0239,NA,NA,64,64,32,128,16,0.1,NA,NA,NA,NA,NA,35,32,"runs/2022-10-13T06-01-02Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FBD0430>",0.100000001490116,"nn_exp.R",2022-10-13 06:01:02,2022-10-13 06:02:36,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-01-02Z/tfruns.d/source.tar.gz","local","training"
"281","runs/2022-10-13T01-49-47Z",838.0759,1193.4175,NA,NA,NA,NA,32,16,NA,NA,1,0.1,0,0,NA,NA,10,40,31,"runs/2022-10-13T01-49-47Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000002E5B1D0F310>",0.100000001490116,"nn_exp.R",2022-10-13 01:49:47,2022-10-13 02:07:20,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_numeric(""dr ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   layer_dropout(rate = FLAGS$dropout1)  .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,"runs/2022-10-13T01-49-47Z/tfruns.d/source.tar.gz","local","training"
"82","runs/2022-10-13T08-50-31Z",840.4367,975.2511,3.5068,2.8136,NA,NA,128,32,32,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-50-31Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E30A670>",0.100000001490116,"nn_exp.R",2022-10-13 08:50:32,2022-10-13 08:51:04,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-50-31Z/tfruns.d/source.tar.gz","local","training"
"107","runs/2022-10-13T08-25-20Z",841.0805,961.5798,3.4679,2.8125,NA,NA,64,64,32,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-25-20Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829A4F910>",0.100000001490116,"nn_exp.R",2022-10-13 08:25:21,2022-10-13 08:26:16,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-25-20Z/tfruns.d/source.tar.gz","local","training"
"201","runs/2022-10-13T06-23-35Z",843.7504,1006.6983,3.6373,3.0486,NA,NA,32,32,128,128,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-23-35Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E404F70>",0.100000001490116,"nn_exp.R",2022-10-13 06:23:36,2022-10-13 06:25:32,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-23-35Z/tfruns.d/source.tar.gz","local","training"
"205","runs/2022-10-13T06-16-47Z",844.5129,986.7578,3.5653,3.0493,NA,NA,128,64,64,128,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-16-47Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC165700>",0.100000001490116,"nn_exp.R",2022-10-13 06:16:47,2022-10-13 06:18:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-16-47Z/tfruns.d/source.tar.gz","local","training"
"177","runs/2022-10-13T06-54-13Z",846.4653,1061.5596,3.8408,2.8844,NA,NA,32,128,64,32,32,0.1,NA,NA,NA,NA,NA,35,23,"runs/2022-10-13T06-54-13Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC14DD60>",0.100000001490116,"nn_exp.R",2022-10-13 06:54:13,2022-10-13 06:54:51,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-54-13Z/tfruns.d/source.tar.gz","local","training"
"78","runs/2022-10-13T08-52-47Z",849.265,995.735,3.5991,3.0733,NA,NA,32,128,32,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-52-47Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0B565B0>",0.100000001490116,"nn_exp.R",2022-10-13 08:52:47,2022-10-13 08:53:24,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-52-47Z/tfruns.d/source.tar.gz","local","training"
"106","runs/2022-10-13T08-26-16Z",849.7093,958.8494,3.4451,3.0145,NA,NA,128,64,32,32,64,0.1,NA,NA,NA,NA,NA,35,32,"runs/2022-10-13T08-26-16Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0B564C0>",0.100000001490116,"nn_exp.R",2022-10-13 08:26:17,2022-10-13 08:27:15,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-26-16Z/tfruns.d/source.tar.gz","local","training"
"223","runs/2022-10-13T05-18-32Z",850.2242,1036.3231,3.7408,3.0836,NA,NA,128,64,128,64,16,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T05-18-32Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B820371100>",0.100000001490116,"nn_exp.R",2022-10-13 05:18:32,2022-10-13 05:19:57,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-18-32Z/tfruns.d/source.tar.gz","local","training"
"200","runs/2022-10-13T06-25-32Z",851.3,1053.6884,3.8126,2.8541,NA,NA,64,32,128,128,16,0.1,NA,NA,NA,NA,NA,35,26,"runs/2022-10-13T06-25-32Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BE12FA0>",0.100000001490116,"nn_exp.R",2022-10-13 06:25:32,2022-10-13 06:27:04,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-25-32Z/tfruns.d/source.tar.gz","local","training"
"255","runs/2022-10-13T04-14-46Z",853.838,1036.8448,3.7504,3.0965,NA,NA,32,32,128,32,16,0.1,NA,NA,NA,NA,NA,35,28,"runs/2022-10-13T04-14-46Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829A68820>",0.100000001490116,"nn_exp.R",2022-10-13 04:14:46,2022-10-13 04:16:57,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-14-46Z/tfruns.d/source.tar.gz","local","training"
"139","runs/2022-10-13T07-36-15Z",859.4688,1024.0839,3.7021,3.1229,NA,NA,128,128,128,64,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-36-15Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0B20F40>",0.100000001490116,"nn_exp.R",2022-10-13 07:36:16,2022-10-13 07:39:03,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-36-15Z/tfruns.d/source.tar.gz","local","training"
"113","runs/2022-10-13T08-17-09Z",862.5484,1006.3796,3.6384,3.1252,NA,NA,64,128,128,128,32,0.1,NA,NA,NA,NA,NA,35,32,"runs/2022-10-13T08-17-09Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812431D90>",0.100000001490116,"nn_exp.R",2022-10-13 08:17:10,2022-10-13 08:19:21,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-17-09Z/tfruns.d/source.tar.gz","local","training"
"92","runs/2022-10-13T08-39-49Z",864.3041,1203.8066,4.3694,3.0986,NA,NA,64,32,128,32,64,0.1,NA,NA,NA,NA,NA,35,17,"runs/2022-10-13T08-39-49Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8149BE910>",0.100000001490116,"nn_exp.R",2022-10-13 08:39:50,2022-10-13 08:40:21,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-39-49Z/tfruns.d/source.tar.gz","local","training"
"261","runs/2022-10-13T04-00-06Z",867.4589,990.3648,3.5764,3.1447,NA,NA,32,64,64,32,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T04-00-06Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B82997C8E0>",0.100000001490116,"nn_exp.R",2022-10-13 04:00:06,2022-10-13 04:02:16,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-00-06Z/tfruns.d/source.tar.gz","local","training"
"9","runs/2022-10-13T10-24-33Z",881.0616,821.8014,2.9583,3.1763,NA,NA,128,64,16,32,32,0.01,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T10-24-33Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81488D130>",0.00999999977648258,"nn_exp.R",2022-10-13 10:24:34,2022-10-13 10:25:29,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-24-33Z/tfruns.d/source.tar.gz","local","training"
"76","runs/2022-10-13T08-53-56Z",882.7057,985.3989,3.5655,3.2164,NA,NA,128,128,32,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-53-56Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BE177C0>",0.100000001490116,"nn_exp.R",2022-10-13 08:53:57,2022-10-13 08:55:03,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-53-56Z/tfruns.d/source.tar.gz","local","training"
"207","runs/2022-10-13T06-13-42Z",886.5594,1015.3176,3.6651,3.2209,NA,NA,32,64,64,128,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-13-42Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A9113A0>",0.100000001490116,"nn_exp.R",2022-10-13 06:13:43,2022-10-13 06:15:16,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-13-42Z/tfruns.d/source.tar.gz","local","training"
"21","runs/2022-10-13T09-56-19Z",887.1378,1142.8221,4.1426,3.0879,NA,NA,64,64,16,16,16,0.01,NA,NA,NA,NA,NA,35,5,"runs/2022-10-13T09-56-19Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812412040>",0.00999999977648258,"nn_exp.R",2022-10-13 09:56:24,2022-10-13 09:56:42,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,"runs/2022-10-13T09-56-19Z/tfruns.d/source.tar.gz","local","training"
"163","runs/2022-10-13T07-06-46Z",887.6373,938.6382,3.378,2.948,NA,NA,128,32,32,64,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-06-46Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC2B9A00>",0.100000001490116,"nn_exp.R",2022-10-13 07:06:46,2022-10-13 07:07:40,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-06-46Z/tfruns.d/source.tar.gz","local","training"
"203","runs/2022-10-13T06-19-50Z",892.3072,1005.5813,3.6322,3.2195,NA,NA,64,128,64,128,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-19-50Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC2978B0>",0.100000001490116,"nn_exp.R",2022-10-13 06:19:50,2022-10-13 06:21:31,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-19-50Z/tfruns.d/source.tar.gz","local","training"
"79","runs/2022-10-13T08-52-09Z",894.9492,877.1719,3.1551,2.9819,NA,NA,128,64,32,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-52-09Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCA903A0>",0.100000001490116,"nn_exp.R",2022-10-13 08:52:10,2022-10-13 08:52:47,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-52-09Z/tfruns.d/source.tar.gz","local","training"
"202","runs/2022-10-13T06-21-31Z",898.3422,969.0165,3.4992,2.9841,NA,NA,128,128,64,128,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-21-31Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BE12340>",0.100000001490116,"nn_exp.R",2022-10-13 06:21:31,2022-10-13 06:23:35,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-21-31Z/tfruns.d/source.tar.gz","local","training"
"16","runs/2022-10-13T10-15-09Z",901.0681,1010.6567,3.6711,3.128,NA,NA,128,64,16,16,32,0.001,NA,NA,NA,NA,NA,35,14,"runs/2022-10-13T10-15-09Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829956E80>",0.00100000004749745,"nn_exp.R",2022-10-13 10:15:11,2022-10-13 10:16:13,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,"runs/2022-10-13T10-15-09Z/tfruns.d/source.tar.gz","local","training"
"96","runs/2022-10-13T08-35-52Z",902.6379,1143.5377,4.1358,3.2875,NA,NA,32,128,64,32,64,0.1,NA,NA,NA,NA,NA,35,21,"runs/2022-10-13T08-35-52Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B82553EE80>",0.100000001490116,"nn_exp.R",2022-10-13 08:35:53,2022-10-13 08:36:30,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-35-52Z/tfruns.d/source.tar.gz","local","training"
"187","runs/2022-10-13T06-45-12Z",905.461,988.5712,3.5773,3.2974,NA,NA,128,64,32,32,32,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T06-45-12Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FBB2E20>",0.100000001490116,"nn_exp.R",2022-10-13 06:45:12,2022-10-13 06:46:05,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-45-12Z/tfruns.d/source.tar.gz","local","training"
"191","runs/2022-10-13T06-41-35Z",905.8267,1014.9744,3.67,3.061,NA,NA,64,32,32,32,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-41-35Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FA51A90>",0.100000001490116,"nn_exp.R",2022-10-13 06:41:35,2022-10-13 06:42:30,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-41-35Z/tfruns.d/source.tar.gz","local","training"
"7","runs/2022-10-13T10-26-25Z",907.5557,783.8362,2.8279,3.2548,NA,NA,64,128,32,32,32,0.01,NA,NA,NA,NA,NA,35,32,"runs/2022-10-13T10-26-25Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BDDB8E0>",0.00999999977648258,"nn_exp.R",2022-10-13 10:26:26,2022-10-13 10:27:22,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-26-25Z/tfruns.d/source.tar.gz","local","training"
"167","runs/2022-10-13T07-02-33Z",910.7504,1038.1671,3.7329,3.381,NA,NA,64,128,128,32,32,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T07-02-33Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8298B2A30>",0.100000001490116,"nn_exp.R",2022-10-13 07:02:34,2022-10-13 07:03:43,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-02-33Z/tfruns.d/source.tar.gz","local","training"
"165","runs/2022-10-13T07-05-03Z",912.4183,1000.4395,3.618,3.0552,NA,NA,32,32,32,64,32,0.1,NA,NA,NA,NA,NA,35,34,"runs/2022-10-13T07-05-03Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC2C3E50>",0.100000001490116,"nn_exp.R",2022-10-13 07:05:04,2022-10-13 07:05:54,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-05-03Z/tfruns.d/source.tar.gz","local","training"
"229","runs/2022-10-13T05-09-53Z",912.5528,971.1873,3.5079,3.2975,NA,NA,128,128,64,64,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T05-09-53Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FB06700>",0.100000001490116,"nn_exp.R",2022-10-13 05:09:53,2022-10-13 05:11:35,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-09-53Z/tfruns.d/source.tar.gz","local","training"
"275","runs/2022-10-13T03-35-43Z",921.2556,1023.7064,3.7063,3.374,NA,NA,32,32,32,32,16,0.1,0,0,0,0,NA,35,20,"runs/2022-10-13T03-35-43Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E43C3A0>",0.100000001490116,"nn_exp.R",2022-10-13 03:35:43,2022-10-13 03:36:49,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   layer_dropout(rate = FLAGS$dropout1)  .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=8, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,"runs/2022-10-13T03-35-43Z/tfruns.d/source.tar.gz","local","training"
"212","runs/2022-10-13T06-05-36Z",925.3678,1042.1637,3.7653,3.3487,NA,NA,64,128,32,128,16,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T06-05-36Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812412EB0>",0.100000001490116,"nn_exp.R",2022-10-13 06:05:36,2022-10-13 06:07:06,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-05-36Z/tfruns.d/source.tar.gz","local","training"
"5","runs/2022-10-13T10-28-17Z",928.3111,880.879,3.1768,3.3356,NA,NA,128,64,16,32,32,0.001,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T10-28-17Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81483DF40>",0.00100000004749745,"nn_exp.R",2022-10-13 10:28:18,2022-10-13 10:29:13,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-28-17Z/tfruns.d/source.tar.gz","local","training"
"145","runs/2022-10-13T07-25-11Z",934.5162,925.3537,3.3534,3.3969,NA,NA,128,32,128,64,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-25-11Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81484C5B0>",0.100000001490116,"nn_exp.R",2022-10-13 07:25:12,2022-10-13 07:26:54,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-25-11Z/tfruns.d/source.tar.gz","local","training"
"103","runs/2022-10-13T08-28-56Z",934.5729,926.5427,3.3405,3.0858,NA,NA,128,128,32,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-28-56Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8134AFEE0>",0.100000001490116,"nn_exp.R",2022-10-13 08:28:56,2022-10-13 08:30:51,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-28-56Z/tfruns.d/source.tar.gz","local","training"
"260","runs/2022-10-13T04-02-16Z",936.2036,994.5126,3.5901,3.4002,NA,NA,64,64,64,32,16,0.1,NA,NA,NA,NA,NA,35,32,"runs/2022-10-13T04-02-16Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC0CBC70>",0.100000001490116,"nn_exp.R",2022-10-13 04:02:17,2022-10-13 04:04:53,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-02-16Z/tfruns.d/source.tar.gz","local","training"
"1","runs/2022-10-13T10-32-40Z",955.1935,849.48,3.0605,3.197,NA,NA,64,128,16,32,32,0.001,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T10-32-40Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B811E82400>",0.00100000004749745,"nn_exp.R",2022-10-13 10:32:41,2022-10-13 10:33:42,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-32-40Z/tfruns.d/source.tar.gz","local","training"
"58","runs/2022-10-13T09-07-29Z",956.6288,1025.5282,3.708,3.4869,NA,NA,128,128,128,64,64,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T09-07-29Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B825524340>",0.100000001490116,"nn_exp.R",2022-10-13 09:07:30,2022-10-13 09:08:46,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-07-29Z/tfruns.d/source.tar.gz","local","training"
"27","runs/2022-10-13T09-36-44Z",964.7985,833.3412,3.0023,3.2366,NA,NA,32,64,32,32,16,0.01,NA,NA,NA,NA,NA,35,23,"runs/2022-10-13T09-36-44Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8124203A0>",0.00999999977648258,"nn_exp.R",2022-10-13 09:36:45,2022-10-13 09:37:51,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-36-44Z/tfruns.d/source.tar.gz","local","training"
"134","runs/2022-10-13T07-44-48Z",967.3201,993.2993,3.5964,3.286,NA,NA,64,64,32,128,32,0.1,NA,NA,NA,NA,NA,35,28,"runs/2022-10-13T07-44-48Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B825493CA0>",0.100000001490116,"nn_exp.R",2022-10-13 07:44:49,2022-10-13 07:46:13,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-44-48Z/tfruns.d/source.tar.gz","local","training"
"272","runs/2022-10-13T03-39-54Z",974.2383,1004.9836,3.6312,3.2429,NA,NA,64,32,32,32,16,0.1,NA,NA,NA,NA,NA,35,29,"runs/2022-10-13T03-39-54Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8121C58B0>",0.100000001490116,"nn_exp.R",2022-10-13 03:39:54,2022-10-13 03:41:32,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T03-39-54Z/tfruns.d/source.tar.gz","local","training"
"171","runs/2022-10-13T06-59-12Z",974.9586,954.5758,3.4561,3.5513,NA,NA,32,64,128,32,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-59-12Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E233700>",0.100000001490116,"nn_exp.R",2022-10-13 06:59:12,2022-10-13 07:00:10,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-59-12Z/tfruns.d/source.tar.gz","local","training"
"71","runs/2022-10-13T08-57-24Z",976.3008,945.382,3.4071,3.5392,NA,NA,64,64,64,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-57-24Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81B91FE20>",0.100000001490116,"nn_exp.R",2022-10-13 08:57:25,2022-10-13 08:57:58,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-57-24Z/tfruns.d/source.tar.gz","local","training"
"269","runs/2022-10-13T03-44-18Z",981.7161,1011.0101,3.6489,3.5748,NA,NA,64,64,32,32,16,0.1,NA,NA,NA,NA,NA,35,29,"runs/2022-10-13T03-44-18Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B053BDC0>",0.100000001490116,"nn_exp.R",2022-10-13 03:44:18,2022-10-13 03:45:48,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T03-44-18Z/tfruns.d/source.tar.gz","local","training"
"159","runs/2022-10-13T07-10-11Z",984.0909,997.1893,3.6164,3.3375,NA,NA,32,128,32,64,32,0.1,NA,NA,NA,NA,NA,35,29,"runs/2022-10-13T07-10-11Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B03552E0>",0.100000001490116,"nn_exp.R",2022-10-13 07:10:11,2022-10-13 07:10:57,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-10-11Z/tfruns.d/source.tar.gz","local","training"
"185","runs/2022-10-13T06-47-05Z",984.1505,900.8336,3.2443,3.5804,NA,NA,64,128,32,32,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-47-05Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B820387370>",0.100000001490116,"nn_exp.R",2022-10-13 06:47:06,2022-10-13 06:48:11,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-47-05Z/tfruns.d/source.tar.gz","local","training"
"122","runs/2022-10-13T08-01-01Z",996.2013,996.7958,3.6046,3.2807,NA,NA,64,128,64,128,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-01-01Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E0E7D60>",0.100000001490116,"nn_exp.R",2022-10-13 08:01:01,2022-10-13 08:02:26,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-01-01Z/tfruns.d/source.tar.gz","local","training"
"221","runs/2022-10-13T05-21-10Z",1002.3497,1101.4695,3.9691,3.6357,NA,NA,64,128,128,64,16,0.1,NA,NA,NA,NA,NA,35,22,"runs/2022-10-13T05-21-10Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812463220>",0.100000001490116,"nn_exp.R",2022-10-13 05:21:10,2022-10-13 05:22:19,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-21-10Z/tfruns.d/source.tar.gz","local","training"
"265","runs/2022-10-13T03-51-51Z",1005.0295,994.0494,3.5929,3.6356,NA,NA,128,128,32,32,16,0.1,NA,NA,NA,NA,NA,35,34,"runs/2022-10-13T03-51-51Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B03AB7C0>",0.100000001490116,"nn_exp.R",2022-10-13 03:51:51,2022-10-13 03:53:47,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T03-51-51Z/tfruns.d/source.tar.gz","local","training"
"241","runs/2022-10-13T04-47-24Z",1005.4011,989.9404,3.5707,3.6516,NA,NA,128,64,32,64,16,0.1,NA,NA,NA,NA,NA,35,34,"runs/2022-10-13T04-47-24Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8203892E0>",0.100000001490116,"nn_exp.R",2022-10-13 04:47:25,2022-10-13 04:49:37,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-47-24Z/tfruns.d/source.tar.gz","local","training"
"70","runs/2022-10-13T08-57-58Z",1012.4263,987.3407,3.582,3.376,NA,NA,128,64,64,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-57-58Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B84118DF10>",0.100000001490116,"nn_exp.R",2022-10-13 08:57:59,2022-10-13 08:58:39,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-57-58Z/tfruns.d/source.tar.gz","local","training"
"118","runs/2022-10-13T08-07-42Z",1013.9594,1048.4156,3.7988,3.417,NA,NA,128,32,128,128,32,0.1,NA,NA,NA,NA,NA,35,26,"runs/2022-10-13T08-07-42Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8254DB1C0>",0.100000001490116,"nn_exp.R",2022-10-13 08:07:43,2022-10-13 08:09:01,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-07-42Z/tfruns.d/source.tar.gz","local","training"
"140","runs/2022-10-13T07-33-58Z",1018.798,942.5775,3.3947,3.3185,NA,NA,64,128,128,64,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-33-58Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E2A0250>",0.100000001490116,"nn_exp.R",2022-10-13 07:33:58,2022-10-13 07:36:15,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-33-58Z/tfruns.d/source.tar.gz","local","training"
"211","runs/2022-10-13T06-07-06Z",1019.1467,962.4302,3.4775,3.4249,NA,NA,128,128,32,128,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-07-06Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8146B5AF0>",0.100000001490116,"nn_exp.R",2022-10-13 06:07:07,2022-10-13 06:09:09,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-07-06Z/tfruns.d/source.tar.gz","local","training"
"162","runs/2022-10-13T07-07-40Z",1020.1136,996.9179,3.6075,3.7019,NA,NA,32,64,32,64,32,0.1,NA,NA,NA,NA,NA,35,28,"runs/2022-10-13T07-07-40Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC14D130>",0.100000001490116,"nn_exp.R",2022-10-13 07:07:40,2022-10-13 07:08:23,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-07-40Z/tfruns.d/source.tar.gz","local","training"
"231","runs/2022-10-13T05-07-14Z",1025.459,1007.7409,3.6412,3.4327,NA,NA,32,128,64,64,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T05-07-14Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E2D7400>",0.100000001490116,"nn_exp.R",2022-10-13 05:07:15,2022-10-13 05:08:43,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-07-14Z/tfruns.d/source.tar.gz","local","training"
"266","runs/2022-10-13T03-49-59Z",1027.8561,1028.3923,3.713,3.4588,NA,NA,64,128,32,32,16,0.1,NA,NA,NA,NA,NA,35,33,"runs/2022-10-13T03-49-59Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FA45FD0>",0.100000001490116,"nn_exp.R",2022-10-13 03:49:59,2022-10-13 03:51:51,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T03-49-59Z/tfruns.d/source.tar.gz","local","training"
"8","runs/2022-10-13T10-25-29Z",1036.7899,888.5781,3.2163,3.7745,NA,NA,128,64,32,32,32,0.001,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T10-25-29Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E0C6EE0>",0.00100000004749745,"nn_exp.R",2022-10-13 10:25:30,2022-10-13 10:26:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-25-29Z/tfruns.d/source.tar.gz","local","training"
"20","runs/2022-10-13T10-09-16Z",1037.4772,748.1241,2.6809,3.4378,NA,NA,64,64,16,16,32,0.01,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T10-09-16Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC123E80>",0.00999999977648258,"nn_exp.R",2022-10-13 10:09:17,2022-10-13 10:10:13,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-09-16Z/tfruns.d/source.tar.gz","local","training"
"257","runs/2022-10-13T04-09-22Z",1040.0607,1039.9666,3.7608,3.8133,NA,NA,64,128,64,32,16,0.1,NA,NA,NA,NA,NA,35,24,"runs/2022-10-13T04-09-22Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814978EB0>",0.100000001490116,"nn_exp.R",2022-10-13 04:09:22,2022-10-13 04:11:24,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-09-22Z/tfruns.d/source.tar.gz","local","training"
"268","runs/2022-10-13T03-45-48Z",1056.9753,993.2366,3.5939,3.8014,NA,NA,128,64,32,32,16,0.1,NA,NA,NA,NA,NA,35,34,"runs/2022-10-13T03-45-48Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0ACA520>",0.100000001490116,"nn_exp.R",2022-10-13 03:45:48,2022-10-13 03:47:49,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T03-45-48Z/tfruns.d/source.tar.gz","local","training"
"227","runs/2022-10-13T05-12-47Z",1058.9517,999.9555,3.6106,3.5301,NA,NA,64,32,128,64,16,0.1,NA,NA,NA,NA,NA,35,33,"runs/2022-10-13T05-12-47Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B03DE430>",0.100000001490116,"nn_exp.R",2022-10-13 05:12:47,2022-10-13 05:14:11,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-12-47Z/tfruns.d/source.tar.gz","local","training"
"56","runs/2022-10-13T09-09-19Z",1060.3972,962.2084,3.4681,3.5653,NA,NA,64,32,32,128,64,0.1,NA,NA,NA,NA,NA,35,33,"runs/2022-10-13T09-09-19Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8255D3C70>",0.100000001490116,"nn_exp.R",2022-10-13 09:09:19,2022-10-13 09:09:50,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-09-19Z/tfruns.d/source.tar.gz","local","training"
"264","runs/2022-10-13T03-53-47Z",1068.9457,989.8843,3.5716,3.6261,NA,NA,32,32,64,32,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T03-53-47Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8123D3580>",0.100000001490116,"nn_exp.R",2022-10-13 03:53:48,2022-10-13 03:55:57,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T03-53-47Z/tfruns.d/source.tar.gz","local","training"
"59","runs/2022-10-13T09-06-25Z",1071.6261,1028.483,3.7001,3.572,NA,NA,64,128,128,64,64,0.1,NA,NA,NA,NA,NA,35,32,"runs/2022-10-13T09-06-25Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A81F910>",0.100000001490116,"nn_exp.R",2022-10-13 09:06:26,2022-10-13 09:07:29,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-06-25Z/tfruns.d/source.tar.gz","local","training"
"67","runs/2022-10-13T09-00-03Z",1073.0122,950.3361,3.4423,3.9211,NA,NA,128,128,64,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-00-03Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B810E33340>",0.100000001490116,"nn_exp.R",2022-10-13 09:00:04,2022-10-13 09:01:10,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-00-03Z/tfruns.d/source.tar.gz","local","training"
"217","runs/2022-10-13T05-57-33Z",1074.1317,1252.5413,4.5448,3.6546,NA,NA,128,32,32,128,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T05-57-33Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E117CA0>",0.100000001490116,"nn_exp.R",2022-10-13 05:57:33,2022-10-13 05:59:21,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-57-33Z/tfruns.d/source.tar.gz","local","training"
"153","runs/2022-10-13T07-15-30Z",1076.2748,993.1613,3.5967,3.6018,NA,NA,32,64,64,64,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-15-30Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FB9DF70>",0.100000001490116,"nn_exp.R",2022-10-13 07:15:30,2022-10-13 07:16:27,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-15-30Z/tfruns.d/source.tar.gz","local","training"
"94","runs/2022-10-13T08-37-25Z",1077.0625,1015.696,3.6516,3.9684,NA,NA,128,128,64,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-37-25Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BBFE9E50>",0.100000001490116,"nn_exp.R",2022-10-13 08:37:26,2022-10-13 08:38:57,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-37-25Z/tfruns.d/source.tar.gz","local","training"
"183","runs/2022-10-13T06-48-54Z",1094.1666,990.4941,3.5799,4.0564,NA,NA,32,32,64,32,32,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T06-48-54Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81241C430>",0.100000001490116,"nn_exp.R",2022-10-13 06:48:54,2022-10-13 06:49:43,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-48-54Z/tfruns.d/source.tar.gz","local","training"
"235","runs/2022-10-13T04-59-14Z",1102.1112,1047.0219,3.777,3.9814,NA,NA,128,32,64,64,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T04-59-14Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BBFCEFD0>",0.100000001490116,"nn_exp.R",2022-10-13 04:59:15,2022-10-13 05:01:14,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-59-14Z/tfruns.d/source.tar.gz","local","training"
"104","runs/2022-10-13T08-28-03Z",1103.0951,939.0421,3.3833,3.9794,NA,NA,64,128,32,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-28-03Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCBA0460>",0.100000001490116,"nn_exp.R",2022-10-13 08:28:04,2022-10-13 08:28:56,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-28-03Z/tfruns.d/source.tar.gz","local","training"
"66","runs/2022-10-13T09-01-10Z",1103.3638,1013.9874,3.6517,3.9773,NA,NA,32,32,128,64,64,0.1,NA,NA,NA,NA,NA,35,33,"runs/2022-10-13T09-01-10Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829AB1310>",0.100000001490116,"nn_exp.R",2022-10-13 09:01:11,2022-10-13 09:01:49,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-01-10Z/tfruns.d/source.tar.gz","local","training"
"179","runs/2022-10-13T06-52-26Z",1112.0538,971.4443,3.5133,3.7491,NA,NA,64,64,64,32,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-52-26Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812455A90>",0.100000001490116,"nn_exp.R",2022-10-13 06:52:26,2022-10-13 06:53:21,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-52-26Z/tfruns.d/source.tar.gz","local","training"
"121","runs/2022-10-13T08-02-26Z",1124.3403,987.207,3.5753,3.7981,NA,NA,128,128,64,128,32,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T08-02-26Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8149AF910>",0.100000001490116,"nn_exp.R",2022-10-13 08:02:27,2022-10-13 08:03:59,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-02-26Z/tfruns.d/source.tar.gz","local","training"
"131","runs/2022-10-13T07-48-49Z",1126.4031,963.3139,3.4691,4.0887,NA,NA,64,128,32,128,32,0.1,NA,NA,NA,NA,NA,35,33,"runs/2022-10-13T07-48-49Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8149D5D60>",0.100000001490116,"nn_exp.R",2022-10-13 07:48:50,2022-10-13 07:50:23,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-48-49Z/tfruns.d/source.tar.gz","local","training"
"114","runs/2022-10-13T08-14-57Z",1136.4598,963.076,3.461,3.7493,NA,NA,32,128,128,128,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-14-57Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BCEB8E0>",0.100000001490116,"nn_exp.R",2022-10-13 08:14:57,2022-10-13 08:17:09,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-14-57Z/tfruns.d/source.tar.gz","local","training"
"166","runs/2022-10-13T07-03-43Z",1136.5266,933.3997,3.3803,4.1306,NA,NA,128,128,128,32,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-03-43Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0B6DDF0>",0.100000001490116,"nn_exp.R",2022-10-13 07:03:44,2022-10-13 07:05:03,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-03-43Z/tfruns.d/source.tar.gz","local","training"
"105","runs/2022-10-13T08-27-15Z",1147.6346,966.9114,3.4814,3.7948,NA,NA,32,128,32,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-27-15Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8255A7FA0>",0.100000001490116,"nn_exp.R",2022-10-13 08:27:16,2022-10-13 08:28:03,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-27-15Z/tfruns.d/source.tar.gz","local","training"
"98","runs/2022-10-13T08-33-57Z",1149.3365,977.5995,3.5399,3.8531,NA,NA,64,64,64,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-33-57Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8147DED00>",0.100000001490116,"nn_exp.R",2022-10-13 08:33:58,2022-10-13 08:34:51,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-33-57Z/tfruns.d/source.tar.gz","local","training"
"109","runs/2022-10-13T08-23-32Z",1152.4528,972.7756,3.501,4.3203,NA,NA,128,32,32,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-23-32Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814895D00>",0.100000001490116,"nn_exp.R",2022-10-13 08:23:32,2022-10-13 08:24:31,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-23-32Z/tfruns.d/source.tar.gz","local","training"
"262","runs/2022-10-13T03-58-00Z",1153.5172,978.9028,3.5379,3.8813,NA,NA,128,32,64,32,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T03-58-00Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A844DF0>",0.100000001490116,"nn_exp.R",2022-10-13 03:58:00,2022-10-13 04:00:06,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T03-58-00Z/tfruns.d/source.tar.gz","local","training"
"160","runs/2022-10-13T07-09-15Z",1153.5327,958.0444,3.4615,4.2159,NA,NA,128,64,32,64,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-09-15Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0544190>",0.100000001490116,"nn_exp.R",2022-10-13 07:09:15,2022-10-13 07:10:11,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-09-15Z/tfruns.d/source.tar.gz","local","training"
"51","runs/2022-10-13T09-12-13Z",1153.8892,1033.8179,3.7288,3.8355,NA,NA,32,128,32,128,64,0.1,NA,NA,NA,NA,NA,35,30,"runs/2022-10-13T09-12-13Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC08DBB0>",0.100000001490116,"nn_exp.R",2022-10-13 09:12:14,2022-10-13 09:12:46,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-12-13Z/tfruns.d/source.tar.gz","local","training"
"240","runs/2022-10-13T04-49-37Z",1156.495,1008.9667,3.6482,4.2039,NA,NA,32,128,32,64,16,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T04-49-37Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B04CD370>",0.100000001490116,"nn_exp.R",2022-10-13 04:49:38,2022-10-13 04:51:41,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-49-37Z/tfruns.d/source.tar.gz","local","training"
"15","runs/2022-10-13T10-16-29Z",1160.9761,1214.5958,4.4372,4.063,NA,NA,128,128,16,32,32,1e-04,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T10-16-29Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BD864C0>",9.99999974737875e-05,"nn_exp.R",2022-10-13 10:16:31,2022-10-13 10:19:02,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-16-29Z/tfruns.d/source.tar.gz","local","training"
"32","runs/2022-10-13T09-29-16Z",1163.2611,961.7108,3.4608,4.2766,NA,NA,64,128,128,128,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-29-16Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8147B7610>",0.100000001490116,"nn_exp.R",2022-10-13 09:29:16,2022-10-13 09:30:48,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-29-16Z/tfruns.d/source.tar.gz","local","training"
"209","runs/2022-10-13T06-10-37Z",1163.379,1007.1464,3.6392,3.9279,NA,NA,64,32,64,128,16,0.1,NA,NA,NA,NA,NA,35,32,"runs/2022-10-13T06-10-37Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FAC7BB0>",0.100000001490116,"nn_exp.R",2022-10-13 06:10:37,2022-10-13 06:12:09,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-10-37Z/tfruns.d/source.tar.gz","local","training"
"252","runs/2022-10-13T04-21-14Z",1169.0171,980.3063,3.5401,3.8917,NA,NA,32,64,128,32,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T04-21-14Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC306CA0>",0.100000001490116,"nn_exp.R",2022-10-13 04:21:14,2022-10-13 04:23:39,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-21-14Z/tfruns.d/source.tar.gz","local","training"
"102","runs/2022-10-13T08-30-51Z",1176.2267,952.0683,3.4201,3.9779,NA,NA,32,32,64,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-30-51Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812420D60>",0.100000001490116,"nn_exp.R",2022-10-13 08:30:52,2022-10-13 08:31:39,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-30-51Z/tfruns.d/source.tar.gz","local","training"
"43","runs/2022-10-13T09-18-01Z",1184.0806,1015.9115,3.6598,4.2861,NA,NA,128,64,64,128,64,0.1,NA,NA,NA,NA,NA,35,33,"runs/2022-10-13T09-18-01Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8204C5E50>",0.100000001490116,"nn_exp.R",2022-10-13 09:18:02,2022-10-13 09:18:55,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-18-01Z/tfruns.d/source.tar.gz","local","training"
"225","runs/2022-10-13T05-15-43Z",1192.5499,1005.1627,3.6296,3.8838,NA,NA,32,64,128,64,16,0.1,NA,NA,NA,NA,NA,35,33,"runs/2022-10-13T05-15-43Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B02933D0>",0.100000001490116,"nn_exp.R",2022-10-13 05:15:43,2022-10-13 05:17:12,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-15-43Z/tfruns.d/source.tar.gz","local","training"
"33","runs/2022-10-13T09-27-55Z",1193.2365,1082.632,3.9209,4.3759,NA,NA,32,128,128,128,64,0.1,NA,NA,NA,NA,NA,35,32,"runs/2022-10-13T09-27-55Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814711190>",0.100000001490116,"nn_exp.R",2022-10-13 09:27:55,2022-10-13 09:29:16,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-27-55Z/tfruns.d/source.tar.gz","local","training"
"46","runs/2022-10-13T09-15-56Z",1194.1021,1091.5535,3.9552,4.4067,NA,NA,128,32,64,128,64,0.1,NA,NA,NA,NA,NA,35,22,"runs/2022-10-13T09-15-56Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8411C9910>",0.100000001490116,"nn_exp.R",2022-10-13 09:15:57,2022-10-13 09:16:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-15-56Z/tfruns.d/source.tar.gz","local","training"
"69","runs/2022-10-13T08-58-39Z",1194.4586,1007.8854,3.6514,3.9727,NA,NA,32,128,64,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-58-39Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E3EE730>",0.100000001490116,"nn_exp.R",2022-10-13 08:58:40,2022-10-13 08:59:19,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-58-39Z/tfruns.d/source.tar.gz","local","training"
"184","runs/2022-10-13T06-48-11Z",1195.5125,1070.6895,3.8619,3.897,NA,NA,128,128,32,32,32,0.1,NA,NA,NA,NA,NA,35,21,"runs/2022-10-13T06-48-11Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC23FC40>",0.100000001490116,"nn_exp.R",2022-10-13 06:48:11,2022-10-13 06:48:54,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-48-11Z/tfruns.d/source.tar.gz","local","training"
"128","runs/2022-10-13T07-53-23Z",1197.0718,964.0203,3.4697,4.4798,NA,NA,64,32,64,128,32,0.1,NA,NA,NA,NA,NA,35,32,"runs/2022-10-13T07-53-23Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BD512B0>",0.100000001490116,"nn_exp.R",2022-10-13 07:53:24,2022-10-13 07:54:34,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-53-23Z/tfruns.d/source.tar.gz","local","training"
"180","runs/2022-10-13T06-51-32Z",1198.3274,995.1461,3.6008,4.0429,NA,NA,32,64,64,32,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-51-32Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A841BE0>",0.100000001490116,"nn_exp.R",2022-10-13 06:51:32,2022-10-13 06:52:26,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-51-32Z/tfruns.d/source.tar.gz","local","training"
"83","runs/2022-10-13T08-49-59Z",1201.6692,971.5535,3.5227,4.3586,NA,NA,64,32,32,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-49-59Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8149AE0D0>",0.100000001490116,"nn_exp.R",2022-10-13 08:50:00,2022-10-13 08:50:31,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-49-59Z/tfruns.d/source.tar.gz","local","training"
"228","runs/2022-10-13T05-11-35Z",1203.2909,1046.9213,3.7792,4.0256,NA,NA,32,32,128,64,16,0.1,NA,NA,NA,NA,NA,35,27,"runs/2022-10-13T05-11-35Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B05C8880>",0.100000001490116,"nn_exp.R",2022-10-13 05:11:35,2022-10-13 05:12:47,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-11-35Z/tfruns.d/source.tar.gz","local","training"
"85","runs/2022-10-13T08-48-07Z",1207.051,980.4387,3.5338,4.4402,NA,NA,128,128,128,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-48-07Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A811940>",0.100000001490116,"nn_exp.R",2022-10-13 08:48:08,2022-10-13 08:49:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-48-07Z/tfruns.d/source.tar.gz","local","training"
"41","runs/2022-10-13T09-19-48Z",1208.0292,980.6392,3.5496,4.0615,NA,NA,64,128,64,128,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-19-48Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8299C8280>",0.100000001490116,"nn_exp.R",2022-10-13 09:19:49,2022-10-13 09:20:42,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-19-48Z/tfruns.d/source.tar.gz","local","training"
"11","runs/2022-10-13T10-22-18Z",1221.972,1270.0258,4.6414,4.2459,NA,NA,128,64,16,16,32,1e-04,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T10-22-18Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A8F7AF0>",9.99999974737875e-05,"nn_exp.R",2022-10-13 10:22:19,2022-10-13 10:23:15,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-22-18Z/tfruns.d/source.tar.gz","local","training"
"173","runs/2022-10-13T06-57-37Z",1228.8214,989.4431,3.5852,4.5202,NA,NA,64,32,128,32,32,0.1,NA,NA,NA,NA,NA,35,26,"runs/2022-10-13T06-57-37Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCB60580>",0.100000001490116,"nn_exp.R",2022-10-13 06:57:38,2022-10-13 06:58:19,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-57-37Z/tfruns.d/source.tar.gz","local","training"
"256","runs/2022-10-13T04-11-24Z",1233.9713,1001.6599,3.6154,4.5094,NA,NA,128,128,64,32,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T04-11-24Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A818DF0>",0.100000001490116,"nn_exp.R",2022-10-13 04:11:24,2022-10-13 04:14:46,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-11-24Z/tfruns.d/source.tar.gz","local","training"
"133","runs/2022-10-13T07-46-13Z",1236.4423,1010.6617,3.6607,4.5001,NA,NA,128,64,32,128,32,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T07-46-13Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B84105CB80>",0.100000001490116,"nn_exp.R",2022-10-13 07:46:13,2022-10-13 07:47:31,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-46-13Z/tfruns.d/source.tar.gz","local","training"
"141","runs/2022-10-13T07-31-42Z",1241.4045,955.6705,3.4615,4.1797,NA,NA,32,128,128,64,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-31-42Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B035A3A0>",0.100000001490116,"nn_exp.R",2022-10-13 07:31:43,2022-10-13 07:33:58,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-31-42Z/tfruns.d/source.tar.gz","local","training"
"127","runs/2022-10-13T07-54-34Z",1252.3851,941.1305,3.3859,4.1422,NA,NA,128,32,64,128,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-54-34Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC219310>",0.100000001490116,"nn_exp.R",2022-10-13 07:54:35,2022-10-13 07:55:55,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-54-34Z/tfruns.d/source.tar.gz","local","training"
"222","runs/2022-10-13T05-19-57Z",1255.2155,1048.6259,3.7893,4.2017,NA,NA,32,128,128,64,16,0.1,NA,NA,NA,NA,NA,35,24,"runs/2022-10-13T05-19-57Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B814824640>",0.100000001490116,"nn_exp.R",2022-10-13 05:19:57,2022-10-13 05:21:09,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-19-57Z/tfruns.d/source.tar.gz","local","training"
"129","runs/2022-10-13T07-52-09Z",1260.0668,1048.7966,3.7826,4.6118,NA,NA,32,32,64,128,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-52-09Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B841061760>",0.100000001490116,"nn_exp.R",2022-10-13 07:52:09,2022-10-13 07:53:23,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-52-09Z/tfruns.d/source.tar.gz","local","training"
"119","runs/2022-10-13T08-05-49Z",1267.1277,904.7509,3.2594,4.5219,NA,NA,64,32,128,128,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-05-49Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC1234C0>",0.100000001490116,"nn_exp.R",2022-10-13 08:05:49,2022-10-13 08:07:42,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-05-49Z/tfruns.d/source.tar.gz","local","training"
"149","runs/2022-10-13T07-19-32Z",1289.6759,952.5585,3.4476,4.2748,NA,NA,64,128,64,64,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-19-32Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0B6DF10>",0.100000001490116,"nn_exp.R",2022-10-13 07:19:32,2022-10-13 07:20:57,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-19-32Z/tfruns.d/source.tar.gz","local","training"
"259","runs/2022-10-13T04-04-53Z",1292.7147,1048.3816,3.785,4.7555,NA,NA,128,64,64,32,16,0.1,NA,NA,NA,NA,NA,35,30,"runs/2022-10-13T04-04-53Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B820440F70>",0.100000001490116,"nn_exp.R",2022-10-13 04:04:53,2022-10-13 04:07:04,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-04-53Z/tfruns.d/source.tar.gz","local","training"
"144","runs/2022-10-13T07-26-54Z",1312.8118,968.2813,3.499,4.784,NA,NA,32,64,128,64,32,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T07-26-54Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BD1D2E0>",0.100000001490116,"nn_exp.R",2022-10-13 07:26:55,2022-10-13 07:28:22,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-26-54Z/tfruns.d/source.tar.gz","local","training"
"68","runs/2022-10-13T08-59-19Z",1315.655,904.1025,3.2566,4.3752,NA,NA,64,128,64,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-59-19Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BE1EBE0>",0.100000001490116,"nn_exp.R",2022-10-13 08:59:19,2022-10-13 09:00:03,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-59-19Z/tfruns.d/source.tar.gz","local","training"
"198","runs/2022-10-13T06-28-57Z",1320.6223,989.0985,3.571,4.4225,NA,NA,32,64,128,128,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-28-57Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A941D90>",0.100000001490116,"nn_exp.R",2022-10-13 06:28:57,2022-10-13 06:30:52,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-28-57Z/tfruns.d/source.tar.gz","local","training"
"148","runs/2022-10-13T07-20-57Z",1330.6743,978.2769,3.542,4.5165,NA,NA,128,128,64,64,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-20-57Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BDD91F0>",0.100000001490116,"nn_exp.R",2022-10-13 07:20:58,2022-10-13 07:22:57,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-20-57Z/tfruns.d/source.tar.gz","local","training"
"18","runs/2022-10-13T10-11-18Z",1333.1532,1388.191,5.0776,4.6579,NA,NA,64,64,16,16,32,1e-04,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T10-11-18Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829A13B50>",9.99999974737875e-05,"nn_exp.R",2022-10-13 10:11:19,2022-10-13 10:12:38,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T10-11-18Z/tfruns.d/source.tar.gz","local","training"
"34","runs/2022-10-13T09-26-43Z",1342.7174,1012.478,3.6691,4.4759,NA,NA,128,64,128,128,64,0.1,NA,NA,NA,NA,NA,35,33,"runs/2022-10-13T09-26-43Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A8DC490>",0.100000001490116,"nn_exp.R",2022-10-13 09:26:44,2022-10-13 09:27:54,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-26-43Z/tfruns.d/source.tar.gz","local","training"
"77","runs/2022-10-13T08-53-24Z",1356.2473,1021.6733,3.7095,4.5769,NA,NA,64,128,32,64,64,0.1,NA,NA,NA,NA,NA,35,27,"runs/2022-10-13T08-53-24Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A8DCF70>",0.100000001490116,"nn_exp.R",2022-10-13 08:53:25,2022-10-13 08:53:56,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-53-24Z/tfruns.d/source.tar.gz","local","training"
"274","runs/2022-10-13T03-37-46Z",1362.0369,1743.424,6.317,5.0298,NA,NA,32,32,32,32,16,0.1,NA,NA,NA,NA,NA,35,2,"runs/2022-10-13T03-37-46Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FB12FA0>",0.100000001490116,"nn_exp.R",2022-10-13 03:37:46,2022-10-13 03:37:57,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,"runs/2022-10-13T03-37-46Z/tfruns.d/source.tar.gz","local","training"
"142","runs/2022-10-13T07-29-49Z",1367.4835,974.9393,3.5252,4.975,NA,NA,128,64,128,64,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-29-49Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8299E7760>",0.100000001490116,"nn_exp.R",2022-10-13 07:29:50,2022-10-13 07:31:42,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-29-49Z/tfruns.d/source.tar.gz","local","training"
"156","runs/2022-10-13T07-12-54Z",1368.8341,1006.2159,3.6392,4.5593,NA,NA,32,32,64,64,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-12-54Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E42BB50>",0.100000001490116,"nn_exp.R",2022-10-13 07:12:55,2022-10-13 07:13:46,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-12-54Z/tfruns.d/source.tar.gz","local","training"
"61","runs/2022-10-13T09-04-20Z",1374.95,973.8774,3.5353,4.6606,NA,NA,128,64,128,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-04-20Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B08F5610>",0.100000001490116,"nn_exp.R",2022-10-13 09:04:21,2022-10-13 09:05:11,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-04-20Z/tfruns.d/source.tar.gz","local","training"
"88","runs/2022-10-13T08-44-25Z",1378.679,970.457,3.5218,4.9808,NA,NA,128,64,128,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-44-25Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8297CD640>",0.100000001490116,"nn_exp.R",2022-10-13 08:44:26,2022-10-13 08:45:55,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-44-25Z/tfruns.d/source.tar.gz","local","training"
"45","runs/2022-10-13T09-16-25Z",1421.8154,1026.1096,3.7155,4.7814,NA,NA,32,64,64,128,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-16-25Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B8146E8130>",0.100000001490116,"nn_exp.R",2022-10-13 09:16:26,2022-10-13 09:17:12,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-16-25Z/tfruns.d/source.tar.gz","local","training"
"234","runs/2022-10-13T05-01-14Z",1430.3751,1004.9393,3.6308,4.8559,NA,NA,32,64,64,64,16,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T05-01-14Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E39E2E0>",0.100000001490116,"nn_exp.R",2022-10-13 05:01:14,2022-10-13 05:03:14,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-01-14Z/tfruns.d/source.tar.gz","local","training"
"84","runs/2022-10-13T08-49-25Z",1431.9827,977.4875,3.5234,5.2371,NA,NA,32,32,32,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-49-25Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BBFDA580>",0.100000001490116,"nn_exp.R",2022-10-13 08:49:25,2022-10-13 08:49:59,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-49-25Z/tfruns.d/source.tar.gz","local","training"
"233","runs/2022-10-13T05-03-14Z",1452.6279,999.3696,3.6111,4.8942,NA,NA,64,64,64,64,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T05-03-14Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E0A1460>",0.100000001490116,"nn_exp.R",2022-10-13 05:03:14,2022-10-13 05:05:26,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-03-14Z/tfruns.d/source.tar.gz","local","training"
"151","runs/2022-10-13T07-17-25Z",1482.9464,989.8863,3.5784,5.4365,NA,NA,128,64,64,64,32,0.1,NA,NA,NA,NA,NA,35,30,"runs/2022-10-13T07-17-25Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BD59490>",0.100000001490116,"nn_exp.R",2022-10-13 07:17:25,2022-10-13 07:18:38,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-17-25Z/tfruns.d/source.tar.gz","local","training"
"53","runs/2022-10-13T09-10-59Z",1504.309,1047.8455,3.7719,4.8236,NA,NA,64,64,32,128,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-10-59Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B812424BE0>",0.100000001490116,"nn_exp.R",2022-10-13 09:10:59,2022-10-13 09:11:33,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-10-59Z/tfruns.d/source.tar.gz","local","training"
"152","runs/2022-10-13T07-16-28Z",1547.9032,969.1349,3.4915,5.2861,NA,NA,64,64,64,64,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-16-28Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B82558E610>",0.100000001490116,"nn_exp.R",2022-10-13 07:16:28,2022-10-13 07:17:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-16-28Z/tfruns.d/source.tar.gz","local","training"
"251","runs/2022-10-13T04-23-39Z",1550.5358,1022.5385,3.7011,5.243,NA,NA,64,64,128,32,16,0.1,NA,NA,NA,NA,NA,35,24,"runs/2022-10-13T04-23-39Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E4A4F70>",0.100000001490116,"nn_exp.R",2022-10-13 04:23:39,2022-10-13 04:25:38,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-23-39Z/tfruns.d/source.tar.gz","local","training"
"271","runs/2022-10-13T03-41-32Z",1554.1295,993.4129,3.5866,5.6699,NA,NA,128,32,32,32,16,0.1,NA,NA,NA,NA,NA,35,28,"runs/2022-10-13T03-41-32Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FB0D8E0>",0.100000001490116,"nn_exp.R",2022-10-13 03:41:32,2022-10-13 03:42:56,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T03-41-32Z/tfruns.d/source.tar.gz","local","training"
"237","runs/2022-10-13T04-55-35Z",1567.2192,993.4254,3.583,5.2927,NA,NA,32,32,64,64,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T04-55-35Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC274EB0>",0.100000001490116,"nn_exp.R",2022-10-13 04:55:35,2022-10-13 04:57:27,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-55-35Z/tfruns.d/source.tar.gz","local","training"
"31","runs/2022-10-13T09-30-48Z",1594.468,1017.7444,3.6863,5.3458,NA,NA,128,128,128,128,64,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T09-30-48Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B825444670>",0.100000001490116,"nn_exp.R",2022-10-13 09:30:49,2022-10-13 09:32:19,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-30-48Z/tfruns.d/source.tar.gz","local","training"
"47","runs/2022-10-13T09-15-15Z",1596.2595,1003.9138,3.6176,5.3429,NA,NA,64,32,64,128,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-15-15Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81E103610>",0.100000001490116,"nn_exp.R",2022-10-13 09:15:15,2022-10-13 09:15:56,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-15-15Z/tfruns.d/source.tar.gz","local","training"
"146","runs/2022-10-13T07-23-48Z",1614.6024,987.4085,3.5788,5.8964,NA,NA,64,32,128,64,32,0.1,NA,NA,NA,NA,NA,35,32,"runs/2022-10-13T07-23-48Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BCB75B0>",0.100000001490116,"nn_exp.R",2022-10-13 07:23:49,2022-10-13 07:25:11,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-23-48Z/tfruns.d/source.tar.gz","local","training"
"89","runs/2022-10-13T08-42-56Z",1663.2373,1005.2368,3.6402,5.6057,NA,NA,64,64,128,32,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T08-42-56Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC2F99A0>",0.100000001490116,"nn_exp.R",2022-10-13 08:42:57,2022-10-13 08:44:25,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-42-56Z/tfruns.d/source.tar.gz","local","training"
"155","runs/2022-10-13T07-13-46Z",1747.6436,913.9027,3.2908,5.7958,NA,NA,64,32,64,64,32,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T07-13-46Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BCB71CD0>",0.100000001490116,"nn_exp.R",2022-10-13 07:13:47,2022-10-13 07:14:40,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-13-46Z/tfruns.d/source.tar.gz","local","training"
"258","runs/2022-10-13T04-07-05Z",1754.396,999.0527,3.6059,5.8939,NA,NA,32,128,64,32,16,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T04-07-05Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B829A6C8E0>",0.100000001490116,"nn_exp.R",2022-10-13 04:07:05,2022-10-13 04:09:22,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-07-05Z/tfruns.d/source.tar.gz","local","training"
"40","runs/2022-10-13T09-20-42Z",1771.552,966.5164,3.5097,5.9508,NA,NA,128,128,64,128,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-20-42Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B81BE0B3A0>",0.100000001490116,"nn_exp.R",2022-10-13 09:20:43,2022-10-13 09:21:53,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-20-42Z/tfruns.d/source.tar.gz","local","training"
"158","runs/2022-10-13T07-10-57Z",1812.2209,981.0834,3.5505,6.119,NA,NA,64,128,32,64,32,0.1,NA,NA,NA,NA,NA,35,32,"runs/2022-10-13T07-10-57Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B77E3BD190>",0.100000001490116,"nn_exp.R",2022-10-13 07:10:57,2022-10-13 07:11:49,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-10-57Z/tfruns.d/source.tar.gz","local","training"
"64","runs/2022-10-13T09-02-14Z",2088.874,982.4146,3.533,7.6934,NA,NA,128,32,128,64,64,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T09-02-14Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B825621BE0>",0.100000001490116,"nn_exp.R",2022-10-13 09:02:15,2022-10-13 09:02:54,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-02-14Z/tfruns.d/source.tar.gz","local","training"
"232","runs/2022-10-13T05-05-26Z",2248.8782,1009.3513,3.6474,8.1096,NA,NA,128,64,64,64,16,0.1,NA,NA,NA,NA,NA,35,33,"runs/2022-10-13T05-05-26Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80A86A310>",0.100000001490116,"nn_exp.R",2022-10-13 05:05:26,2022-10-13 05:07:14,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T05-05-26Z/tfruns.d/source.tar.gz","local","training"
"238","runs/2022-10-13T04-53-34Z",2253.2153,1011.3956,3.6503,7.6224,NA,NA,128,128,32,64,16,0.1,NA,NA,NA,NA,NA,35,29,"runs/2022-10-13T04-53-34Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FA8DD90>",0.100000001490116,"nn_exp.R",2022-10-13 04:53:34,2022-10-13 04:55:35,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T04-53-34Z/tfruns.d/source.tar.gz","local","training"
"276","runs/2022-10-13T02-57-46Z",2532.3203,1371.5194,4.9522,9.1258,1370.8242,2531.6221,32,32,NA,NA,1,0.1,0,0,NA,NA,10,35,9,"runs/2022-10-13T02-57-46Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7A1A1E850>",0.100000001490116,"nn_exp.R",2022-10-13 02:57:46,2022-10-13 03:04:17,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_numeric(""dr ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   layer_dropout(rate = FLAGS$dropout1)  .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,"runs/2022-10-13T02-57-46Z/tfruns.d/source.tar.gz","local","training"
"101","runs/2022-10-13T08-31-39Z",2770.2532,976.1826,3.5127,9.9967,NA,NA,64,32,64,32,64,0.1,NA,NA,NA,NA,NA,35,31,"runs/2022-10-13T08-31-39Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7B0AA1DF0>",0.100000001490116,"nn_exp.R",2022-10-13 08:31:39,2022-10-13 08:32:16,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T08-31-39Z/tfruns.d/source.tar.gz","local","training"
"195","runs/2022-10-13T06-34-24Z",2866.5496,999.6578,3.6156,10.2334,NA,NA,32,128,128,128,16,0.1,NA,NA,NA,NA,NA,35,35,"runs/2022-10-13T06-34-24Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B7BC0E2F10>",0.100000001490116,"nn_exp.R",2022-10-13 06:34:24,2022-10-13 06:36:34,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T06-34-24Z/tfruns.d/source.tar.gz","local","training"
"168","runs/2022-10-13T07-01-53Z",4720.7988,4701.2544,17.4242,17.1106,NA,NA,32,128,128,32,32,0.1,NA,NA,NA,NA,NA,35,19,"runs/2022-10-13T07-01-53Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B84107BD60>",0.100000001490116,"nn_exp.R",2022-10-13 07:01:53,2022-10-13 07:02:33,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T07-01-53Z/tfruns.d/source.tar.gz","local","training"
"35","runs/2022-10-13T09-25-53Z",4728.5649,4689.7778,17.3916,17.4136,NA,NA,64,64,128,128,64,0.1,NA,NA,NA,NA,NA,35,22,"runs/2022-10-13T09-25-53Z/tfruns.d/metrics.json","Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001B80FAC5F40>",0.100000001490116,"nn_exp.R",2022-10-13 09:25:54,2022-10-13 09:26:43,TRUE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_integer(""de ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   #layer_dropout(rate = FLAGS$dropout1) .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Early stopping
> early_stopping <- callback_early_stopping(monitor=""loss"", min_delta = 10, patience=6, mode=""min"")

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] 

> plot(history)",NA,NA,"runs/2022-10-13T09-25-53Z/tfruns.d/source.tar.gz","local","training"
"277","runs/2022-10-13T02-55-46Z",NA,NA,NA,NA,NA,NA,32,32,NA,NA,1,0.1,0,0,NA,NA,10,35,0,NA,"Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001D52DD9F940>",0.100000001490116,"nn_exp.R",2022-10-13 02:55:46,2022-10-13 02:55:46,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_numeric(""dr ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   layer_dropout(rate = FLAGS$dropout1)  .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ","RuntimeError: reentrant call inside <_io.BufferedWriter name='<stdout>'>
","py_call_impl(callable, dots$args, dots$keywords)
(structure(function (...) 
{
    dots <- py_resolve_dots(list(...))
    result <- py_call_impl(callable, dots$args, dots$keywords)
    if (convert) 
        result <- py_to_r(result)
    if (is.null(result)) 
        invisible(result)
    else result
}, class = c(""python.builtin.method"", ""python.builtin.object""
), py_object = <environment>))(batch_size = 1L, epochs = 35L, 
    verbose = ""auto"", validation_split = 0.2, shuffle = TRUE, 
    class_weight = NULL, sample_weight = NULL, initial_epoch = 0L, 
    x = <environment>, y = <environment>, callbacks = list(<environment>, 
        <environment>))
do.call(object$fit, args)
fit.keras.engine.training.Model(., x = as.matrix(X_train), y = as.matrix(y_train), 
    validation_split = 0.2, batch_size = FLAGS$batch_size, epochs = 35)
fit(., x = as.matrix(X_train), y = as.matrix(y_train), validation_split = 0.2, 
    batch_size = FLAGS$batch_size, epochs = 35)
model %>% fit(x = as.matrix(X_train), y = as.matrix(y_train), 
    validation_split = 0.2, batch_size = FLAGS$batch_size, epochs = 35)
eval(ei, envir)
eval(ei, envir)
withVisible(eval(ei, envir))
tuning_run(""nn_exp.R"", flags = list(dense_units1 = c(32, 64, 
    128), dense_units2 = c(32, 64, 128), dropout1 = c(0, 0.25, 
    0.5), dropout2 = c(0, 0.25, 0.5), batch_size = c(1, 16, 32), 
    learning_rate = c(0.1, 0.01, 0.001, 1e-04)))","runs/2022-10-13T02-55-46Z/tfruns.d/source.tar.gz","local","training"
"278","runs/2022-10-13T02-54-40Z",NA,NA,NA,NA,NA,NA,32,32,NA,NA,1,0.1,0,0,NA,NA,10,35,0,NA,"Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001D52DDE1A60>",0.100000001490116,"nn_exp.R",2022-10-13 02:54:40,2022-10-13 02:54:40,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_numeric(""dr ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   layer_dropout(rate = FLAGS$dropout1)  .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ","RuntimeError: reentrant call inside <_io.BufferedWriter name='<stdout>'>
","py_call_impl(callable, dots$args, dots$keywords)
(structure(function (...) 
{
    dots <- py_resolve_dots(list(...))
    result <- py_call_impl(callable, dots$args, dots$keywords)
    if (convert) 
        result <- py_to_r(result)
    if (is.null(result)) 
        invisible(result)
    else result
}, class = c(""python.builtin.method"", ""python.builtin.object""
), py_object = <environment>))(batch_size = 1L, epochs = 35L, 
    verbose = ""auto"", validation_split = 0.2, shuffle = TRUE, 
    class_weight = NULL, sample_weight = NULL, initial_epoch = 0L, 
    x = <environment>, y = <environment>, callbacks = list(<environment>, 
        <environment>))
do.call(object$fit, args)
fit.keras.engine.training.Model(., x = as.matrix(X_train), y = as.matrix(y_train), 
    validation_split = 0.2, batch_size = FLAGS$batch_size, epochs = 35)
fit(., x = as.matrix(X_train), y = as.matrix(y_train), validation_split = 0.2, 
    batch_size = FLAGS$batch_size, epochs = 35)
model %>% fit(x = as.matrix(X_train), y = as.matrix(y_train), 
    validation_split = 0.2, batch_size = FLAGS$batch_size, epochs = 35)
eval(ei, envir)
eval(ei, envir)
withVisible(eval(ei, envir))
tuning_run(""nn_exp.R"", flags = list(dense_units1 = c(32, 64, 
    128), dense_units2 = c(32, 64, 128), dropout1 = c(0, 0.25, 
    0.5), dropout2 = c(0, 0.25, 0.5), batch_size = c(1, 16, 32), 
    learning_rate = c(0.1, 0.01, 0.001, 1e-04)))","runs/2022-10-13T02-54-40Z/tfruns.d/source.tar.gz","local","training"
"279","runs/2022-10-13T02-54-28Z",NA,NA,NA,NA,NA,NA,32,32,NA,NA,1,0.1,0,0,NA,NA,10,35,0,NA,"Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001D52DD611F0>",0.100000001490116,"nn_exp.R",2022-10-13 02:54:28,2022-10-13 02:54:28,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_numeric(""dr ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   layer_dropout(rate = FLAGS$dropout1)  .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ","RuntimeError: reentrant call inside <_io.BufferedWriter name='<stdout>'>
","py_call_impl(callable, dots$args, dots$keywords)
(structure(function (...) 
{
    dots <- py_resolve_dots(list(...))
    result <- py_call_impl(callable, dots$args, dots$keywords)
    if (convert) 
        result <- py_to_r(result)
    if (is.null(result)) 
        invisible(result)
    else result
}, class = c(""python.builtin.method"", ""python.builtin.object""
), py_object = <environment>))(batch_size = 1L, epochs = 35L, 
    verbose = ""auto"", validation_split = 0.2, shuffle = TRUE, 
    class_weight = NULL, sample_weight = NULL, initial_epoch = 0L, 
    x = <environment>, y = <environment>, callbacks = list(<environment>, 
        <environment>))
do.call(object$fit, args)
fit.keras.engine.training.Model(., x = as.matrix(X_train), y = as.matrix(y_train), 
    validation_split = 0.2, batch_size = FLAGS$batch_size, epochs = 35)
fit(., x = as.matrix(X_train), y = as.matrix(y_train), validation_split = 0.2, 
    batch_size = FLAGS$batch_size, epochs = 35)
model %>% fit(x = as.matrix(X_train), y = as.matrix(y_train), 
    validation_split = 0.2, batch_size = FLAGS$batch_size, epochs = 35)
eval(ei, envir)
eval(ei, envir)
withVisible(eval(ei, envir))
tuning_run(""nn_exp.R"", flags = list(dense_units1 = c(32, 64, 
    128), dense_units2 = c(32, 64, 128), dropout1 = c(0, 0.25, 
    0.5), dropout2 = c(0, 0.25, 0.5), batch_size = c(1, 16, 32), 
    learning_rate = c(0.1, 0.01, 0.001, 1e-04)))","runs/2022-10-13T02-54-28Z/tfruns.d/source.tar.gz","local","training"
"280","runs/2022-10-13T02-53-14Z",NA,NA,NA,NA,NA,NA,32,32,NA,NA,1,0.1,0,0,NA,NA,10,35,0,NA,"Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001D5020228B0>",0.100000001490116,"nn_exp.R",2022-10-13 02:53:14,2022-10-13 02:53:39,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_numeric(""dr ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   layer_dropout(rate = FLAGS$dropout1)  .... [TRUNCATED] 

> # LR scheduler
> #lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)
> 
 .... [TRUNCATED] 

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,"runs/2022-10-13T02-53-14Z/tfruns.d/source.tar.gz","local","training"
"283","runs/2022-10-13T01-08-01Z",NA,NA,NA,NA,NA,NA,16,16,NA,NA,1,0.1,0,0,NA,NA,5,40,0,NA,"Model: <no summary available, model was not built>","mean_absolute_error","<keras.optimizers.optimizer_v2.adam.Adam object at 0x000002E582054E80>",NA,"nn_exp.R",2022-10-13 01:08:01,2022-10-13 01:08:11,FALSE,"
> # Flags for hyperparameter tuning
> FLAGS <- flags(
+   flag_integer(""dense_units1"", 32),
+   flag_integer(""dense_units2"", 32),
+   flag_numeric(""dr ..."" ... [TRUNCATED] 

> # Model
> set.seed(2022)

> model <- keras_model_sequential() %>%
+   layer_dense(units = FLAGS$dense_units1, activation = ""relu"") %>%
+   layer_dropout(rate = FLAGS$dropout1)  .... [TRUNCATED] 

> # LR scheduler
> lr_schedule <- learning_rate_schedule_cosine_decay(initial_learning_rate=FLAGS$learning_rate, decay_steps = FLAGS$decay_steps)

> # Compile w/ optimiser and loss
> model %>% compile(optimizer = optimizer_adam(learning_rate = lr_schedule), loss = 'mean_absolute_error')

> # Fit 
> set.seed(2022)

> history <- model %>% 
+   fit(
+     x = as.matrix(X_train), 
+     y = as.matrix(y_train), 
+     validation_split = 0.2, 
+     batch_size = FLAGS .... [TRUNCATED] ",NA,NA,"runs/2022-10-13T01-08-01Z/tfruns.d/source.tar.gz","local","training"
